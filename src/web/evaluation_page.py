"""
è¯„æµ‹æ§åˆ¶å°é¡µé¢ - æ”¯æŒæ¶ˆèå®éªŒå¯¹æ¯”

åŠŸèƒ½ï¼š
1. å•æ¨¡å¼è¯„æµ‹
2. å¤šæ¨¡å¼å¯¹æ¯”ï¼ˆæ¶ˆèå®éªŒï¼‰
3. å¯è§†åŒ–è¯„æµ‹ç»“æœ
4. å†å²ç»“æœæŸ¥çœ‹
"""

import streamlit as st
import asyncio
import sys
import json
import os
from pathlib import Path
from datetime import datetime
import pandas as pd

# Add project root to sys.path
project_root = Path(__file__).resolve().parents[2]
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

# Import ablation benchmark
try:
    from evaluation.ablation_benchmark import (
        run_ablation_benchmark,
        run_full_ablation_study,
        EvalMode
    )
    ABLATION_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Could not import ablation_benchmark: {e}")
    ABLATION_AVAILABLE = False

# Import legacy benchmark for backward compatibility
try:
    from evaluation.run_benchmark import run_benchmark
    LEGACY_AVAILABLE = True
except ImportError:
    LEGACY_AVAILABLE = False


def render_evaluation_page():
    """æ¸²æŸ“è¯„æµ‹æ§åˆ¶å°é¡µé¢"""
    
    st.markdown("## ğŸ“Š æ¶ˆèå®éªŒè¯„æµ‹æ§åˆ¶å°")
    st.markdown("> å¯¹æ¯”ä¸åŒé…ç½®ä¸‹çš„æ¨¡å‹è¡¨ç°ï¼ŒéªŒè¯å„ç»„ä»¶çš„è´¡çŒ®åº¦")
    
    # é€‰é¡¹å¡
    tab1, tab2, tab3 = st.tabs(["ğŸ§ª æ¶ˆèå®éªŒ", "ğŸ“ˆ å•æ¨¡å¼è¯„æµ‹", "ğŸ“‚ å†å²ç»“æœ"])
    
    with tab1:
        render_ablation_study_tab()
    
    with tab2:
        render_single_mode_tab()
    
    with tab3:
        render_history_tab()


def render_ablation_study_tab():
    """æ¶ˆèå®éªŒæ ‡ç­¾é¡µ"""
    
    if not ABLATION_AVAILABLE:
        st.error("âŒ æ¶ˆèå®éªŒæ¨¡å—ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ `evaluation/ablation_benchmark.py` æ˜¯å¦å­˜åœ¨")
        return
    
    st.markdown("### ğŸ”¬ å¤šæ¨¡å¼å¯¹æ¯”å®éªŒ")
    
    # æ¨¡å¼è¯´æ˜
    with st.expander("ğŸ“– æ¨¡å¼è¯´æ˜", expanded=False):
        st.markdown("""
        | æ¨¡å¼ | è¯´æ˜ | ç»„ä»¶ |
        |:----:|------|------|
        | **æ¨¡å¼1** | çº¯LLM | æ— Promptæ¨¡æ¿ï¼Œç›´æ¥è¾“å…¥æ¡æ¬¾ |
        | **æ¨¡å¼2** | åŸºç¡€Prompt | æœ‰æ ¼å¼åŒ–Promptï¼Œæ— è§„åˆ™å¼•æ“ |
        | **æ¨¡å¼3** | å½“å‰å·¥ä½œæµ | Prompt + è§„åˆ™å¼•æ“ |
        | **æ¨¡å¼4** | ä¼˜åŒ–å·¥ä½œæµ | æ”¹è¿›Prompt(CoT) + è§„åˆ™å¼•æ“ |
        """)
    
    # é…ç½®
    col1, col2, col3 = st.columns([2, 1, 1])
    
    # å¯ç”¨æ•°æ®é›†
    available_datasets = [
        "evaluation/llm_benchmark_dataset.json",
    ]
    
    with col1:
        data_path = st.selectbox(
            "æµ‹è¯•æ•°æ®é›†", 
            options=available_datasets,
            index=0,
            key="ablation_data_path"
        )
    
    with col2:
        limit = st.number_input(
            "æ ·æœ¬æ•°é‡é™åˆ¶", 
            min_value=1, 
            max_value=100, 
            value=5,
            key="ablation_limit"
        )
    
    with col3:
        source = st.selectbox(
            "LLMæ¥æº",
            options=["local", "cloud"],
            index=0,
            key="ablation_source"
        )
    
    # æ¨¡å¼é€‰æ‹©
    st.markdown("**é€‰æ‹©è¯„æµ‹æ¨¡å¼**ï¼ˆå¯å¤šé€‰ï¼‰ï¼š")
    
    mode_cols = st.columns(4)
    modes_selected = []
    
    with mode_cols[0]:
        if st.checkbox("æ¨¡å¼1: çº¯LLM", value=True, key="mode1"):
            modes_selected.append(1)
    with mode_cols[1]:
        if st.checkbox("æ¨¡å¼2: åŸºç¡€Prompt", value=True, key="mode2"):
            modes_selected.append(2)
    with mode_cols[2]:
        if st.checkbox("æ¨¡å¼3: å½“å‰å·¥ä½œæµ", value=True, key="mode3"):
            modes_selected.append(3)
    with mode_cols[3]:
        if st.checkbox("æ¨¡å¼4: ä¼˜åŒ–å·¥ä½œæµ", value=True, key="mode4"):
            modes_selected.append(4)
    
    # å¼€å§‹æŒ‰é’®
    if st.button("ğŸš€ å¼€å§‹æ¶ˆèå®éªŒ", type="primary", use_container_width=True):
        if not modes_selected:
            st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªè¯„æµ‹æ¨¡å¼")
            return
        
        if not Path(data_path).exists():
            st.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            return
        
        run_ablation_experiment(data_path, modes_selected, limit, source)
    
    # æ˜¾ç¤ºç»“æœ
    if "ablation_results" in st.session_state:
        display_ablation_results(st.session_state.ablation_results)


def run_ablation_experiment(data_path: str, modes: list, limit: int, source: str):
    """è¿è¡Œæ¶ˆèå®éªŒ"""
    
    st.markdown("### â³ è¯„æµ‹è¿›åº¦")
    
    progress_placeholder = st.empty()
    log_placeholder = st.empty()
    
    logs = []
    def log_callback(msg):
        logs.append(msg)
        log_placeholder.code("\n".join(logs[-10:]), language="text")
    
    all_results = {}
    total_modes = len(modes)
    
    for idx, mode in enumerate(modes):
        progress_placeholder.progress(
            (idx) / total_modes, 
            text=f"æ­£åœ¨è¯„æµ‹æ¨¡å¼ {mode}: {EvalMode.name(mode)}..."
        )
        
        try:
            result = asyncio.run(
                run_ablation_benchmark(
                    data_path=data_path,
                    mode=mode,
                    limit=limit,
                    source=source,
                    log_callback=log_callback
                )
            )
            
            if result:
                all_results[f"mode_{mode}"] = result
                
        except Exception as e:
            st.error(f"æ¨¡å¼ {mode} è¯„æµ‹å¤±è´¥: {e}")
    
    progress_placeholder.progress(1.0, text="è¯„æµ‹å®Œæˆï¼")
    
    if all_results:
        st.session_state.ablation_results = all_results
        st.success(f"âœ… æ¶ˆèå®éªŒå®Œæˆï¼å…±è¯„æµ‹ {len(all_results)} ç§æ¨¡å¼")
        
        # ä¿å­˜ç»“æœ
        save_ablation_results(all_results)
    else:
        st.error("âŒ æ‰€æœ‰æ¨¡å¼è¯„æµ‹å¤±è´¥")


def display_ablation_results(results: dict):
    """æ˜¾ç¤ºæ¶ˆèå®éªŒç»“æœ"""
    
    st.markdown("### ğŸ“Š è¯„æµ‹ç»“æœå¯¹æ¯”")
    
    # æ„å»ºå¯¹æ¯”æ•°æ®
    metrics_data = []
    
    for mode_key, mode_result in results.items():
        mode_num = int(mode_key.split("_")[1])
        metrics = mode_result.get("metrics", {})
        
        metrics_data.append({
            "æ¨¡å¼": EvalMode.name(mode_num),
            "å‡†ç¡®ç‡": metrics.get("accuracy", 0),
            "F1åˆ†æ•°": metrics.get("f1", 0),
            "ç²¾ç¡®ç‡": metrics.get("precision", 0),
            "å¬å›ç‡": metrics.get("recall", 0),
            "è§£ææˆåŠŸç‡": metrics.get("parse_rate", 0),
            "å¹»è§‰ç‡": metrics.get("hallucination_rate", 0),
        })
    
    df = pd.DataFrame(metrics_data)
    
    # æ ¸å¿ƒæŒ‡æ ‡å¡ç‰‡
    st.markdown("#### ğŸ¯ æ ¸å¿ƒæŒ‡æ ‡")
    
    cols = st.columns(len(results))
    for idx, (mode_key, mode_result) in enumerate(results.items()):
        mode_num = int(mode_key.split("_")[1])
        metrics = mode_result.get("metrics", {})
        
        with cols[idx]:
            st.markdown(f"**{EvalMode.name(mode_num)}**")
            st.metric("å‡†ç¡®ç‡", f"{metrics.get('accuracy', 0):.1%}")
            st.metric("F1åˆ†æ•°", f"{metrics.get('f1', 0):.1%}")
            st.metric("å¹»è§‰ç‡", f"{metrics.get('hallucination_rate', 0):.1%}", 
                     delta_color="inverse")
    
    # å¯¹æ¯”è¡¨æ ¼
    st.markdown("#### ğŸ“‹ è¯¦ç»†æŒ‡æ ‡å¯¹æ¯”")
    
    # æ ¼å¼åŒ–ç™¾åˆ†æ¯”
    df_display = df.copy()
    for col in df_display.columns[1:]:
        df_display[col] = df_display[col].apply(lambda x: f"{x:.1%}")
    
    st.dataframe(df_display, use_container_width=True, hide_index=True)
    
    # å¯è§†åŒ–å›¾è¡¨
    st.markdown("#### ğŸ“ˆ å¯è§†åŒ–å¯¹æ¯”")
    
    chart_col1, chart_col2 = st.columns(2)
    
    with chart_col1:
        # å‡†ç¡®ç‡ & F1 å¯¹æ¯”
        chart_data = df[["æ¨¡å¼", "å‡†ç¡®ç‡", "F1åˆ†æ•°"]].set_index("æ¨¡å¼")
        st.bar_chart(chart_data)
        st.caption("å‡†ç¡®ç‡ & F1åˆ†æ•°å¯¹æ¯”")
    
    with chart_col2:
        # ç²¾ç¡®ç‡ & å¬å›ç‡ å¯¹æ¯”
        chart_data2 = df[["æ¨¡å¼", "ç²¾ç¡®ç‡", "å¬å›ç‡"]].set_index("æ¨¡å¼")
        st.bar_chart(chart_data2)
        st.caption("ç²¾ç¡®ç‡ & å¬å›ç‡å¯¹æ¯”")
    
    # è¯¦ç»†ç»“æœå±•å¼€
    st.markdown("#### ğŸ“ è¯¦ç»†è¯„æµ‹è®°å½•")
    
    for mode_key, mode_result in results.items():
        mode_num = int(mode_key.split("_")[1])
        mode_name = EvalMode.name(mode_num)
        
        with st.expander(f"ğŸ“‹ {mode_name} - è¯¦ç»†ç»“æœ"):
            display_single_mode_results(mode_result)


def display_single_mode_results(result: dict):
    """æ˜¾ç¤ºå•ä¸ªæ¨¡å¼çš„è¯¦ç»†ç»“æœ"""
    
    items = result.get("results", [])
    
    if not items:
        st.info("æ— è¯„æµ‹è®°å½•")
        return
    
    # ç»Ÿè®¡
    correct_count = sum(1 for item in items if item.get("correct_risk", False))
    st.markdown(f"**æ­£ç¡®/æ€»æ•°**: {correct_count}/{len(items)}")
    
    # é€æ¡æ˜¾ç¤º
    for item in items:
        is_correct = item.get("correct_risk", False)
        icon = "âœ…" if is_correct else "âŒ"
        
        pred = item.get("prediction", {})
        gt = item.get("ground_truth", {})
        
        with st.expander(f"{icon} Case {item.get('id', 'N/A')}: é¢„æµ‹={pred.get('risk_level', '?')} / å®é™…={gt.get('risk_level', '?')}"):
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**ğŸ¤– æ¨¡å‹é¢„æµ‹**")
                st.markdown(f"- é£é™©ç­‰çº§: `{pred.get('risk_level', 'N/A')}`")
                st.markdown(f"- è¯æ®: {pred.get('evidence', 'N/A')[:100] if pred.get('evidence') else 'N/A'}...")
                st.markdown(f"- è§£ææˆåŠŸ: {'âœ…' if pred.get('parse_success') else 'âŒ'}")
                if pred.get('latency'):
                    st.markdown(f"- å“åº”æ—¶é—´: `{pred.get('latency', 0):.2f}s`")
            
            with col2:
                st.markdown("**ğŸ“ æ ‡å‡†ç­”æ¡ˆ**")
                st.markdown(f"- é£é™©ç­‰çº§: `{gt.get('risk_level', 'N/A')}`")
                keywords = gt.get('reason_keywords', [])
                if keywords:
                    st.markdown(f"- å…³é”®è¯: `{', '.join(keywords[:3])}`")
            
            st.markdown(f"**è¯æ®éªŒè¯**: {'âœ… æœ‰æ•ˆ' if item.get('evidence_valid', True) else 'âŒ å¹»è§‰'}")


def render_single_mode_tab():
    """å•æ¨¡å¼è¯„æµ‹æ ‡ç­¾é¡µ"""
    
    st.markdown("### ğŸ¯ å•æ¨¡å¼è¯„æµ‹")
    
    if not ABLATION_AVAILABLE:
        st.error("âŒ è¯„æµ‹æ¨¡å—ä¸å¯ç”¨")
        return
    
    # é…ç½®
    col1, col2, col3 = st.columns([2, 1, 1])
    
    # å¯ç”¨æ•°æ®é›†
    available_datasets = [
        "evaluation/llm_benchmark_dataset.json",
    ]
    
    with col1:
        data_path = st.selectbox(
            "æµ‹è¯•æ•°æ®é›†", 
            options=available_datasets,
            index=0,
            key="single_data_path"
        )
    
    with col2:
        mode = st.selectbox(
            "è¯„æµ‹æ¨¡å¼",
            options=[1, 2, 3, 4],
            format_func=lambda x: f"æ¨¡å¼{x}: {EvalMode.name(x)}",
            key="single_mode"
        )
    
    with col3:
        limit = st.number_input(
            "æ ·æœ¬é™åˆ¶", 
            min_value=1, 
            max_value=100, 
            value=10,
            key="single_limit"
        )
    
    source = st.radio(
        "LLMæ¥æº",
        options=["local", "cloud"],
        horizontal=True,
        key="single_source"
    )
    
    if st.button("â–¶ï¸ å¼€å§‹è¯„æµ‹", type="primary", key="single_start"):
        if not Path(data_path).exists():
            st.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            return
        
        run_single_mode_evaluation(data_path, mode, limit, source)
    
    # æ˜¾ç¤ºç»“æœ
    if "single_mode_result" in st.session_state:
        result = st.session_state.single_mode_result
        st.markdown(f"### ğŸ“Š {EvalMode.name(mode)} è¯„æµ‹ç»“æœ")
        display_single_mode_results(result)


def run_single_mode_evaluation(data_path: str, mode: int, limit: int, source: str):
    """è¿è¡Œå•æ¨¡å¼è¯„æµ‹"""
    
    log_placeholder = st.empty()
    
    logs = []
    def log_callback(msg):
        logs.append(msg)
        log_placeholder.code("\n".join(logs[-8:]), language="text")
    
    with st.spinner(f"æ­£åœ¨è¯„æµ‹æ¨¡å¼ {mode}..."):
        try:
            result = asyncio.run(
                run_ablation_benchmark(
                    data_path=data_path,
                    mode=mode,
                    limit=limit,
                    source=source,
                    log_callback=log_callback
                )
            )
            
            if result:
                st.session_state.single_mode_result = result
                st.success("âœ… è¯„æµ‹å®Œæˆï¼")
            else:
                st.error("âŒ è¯„æµ‹å¤±è´¥")
                
        except Exception as e:
            st.error(f"âŒ è¿è¡Œå‡ºé”™: {e}")


def render_history_tab():
    """å†å²ç»“æœæ ‡ç­¾é¡µ"""
    
    st.markdown("### ğŸ“‚ å†å²è¯„æµ‹ç»“æœ")
    
    # æŸ¥æ‰¾å†å²ç»“æœæ–‡ä»¶
    eval_dir = Path("evaluation")
    if not eval_dir.exists():
        st.info("æš‚æ— å†å²è¯„æµ‹è®°å½•")
        return
    
    result_files = list(eval_dir.glob("ablation_results_*.json"))
    result_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
    
    if not result_files:
        st.info("æš‚æ— æ¶ˆèå®éªŒå†å²è®°å½•")
        return
    
    # æ–‡ä»¶åˆ—è¡¨
    st.markdown(f"**æ‰¾åˆ° {len(result_files)} ä¸ªå†å²è®°å½•**")
    
    selected_file = st.selectbox(
        "é€‰æ‹©ç»“æœæ–‡ä»¶",
        options=result_files,
        format_func=lambda x: x.name,
        key="history_file"
    )
    
    if selected_file:
        try:
            with open(selected_file, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            st.markdown(f"**æ–‡ä»¶**: `{selected_file.name}`")
            st.markdown(f"**åˆ›å»ºæ—¶é—´**: {datetime.fromtimestamp(selected_file.stat().st_mtime)}")
            
            if st.button("ğŸ“Š åŠ è½½å¹¶æ˜¾ç¤º", key="load_history"):
                st.session_state.ablation_results = data
                st.success("å·²åŠ è½½å†å²ç»“æœ")
                st.rerun()
            
            with st.expander("ğŸ“„ åŸå§‹JSONæ•°æ®"):
                st.json(data)
                
        except Exception as e:
            st.error(f"åŠ è½½å¤±è´¥: {e}")


def save_ablation_results(results: dict):
    """ä¿å­˜æ¶ˆèå®éªŒç»“æœ"""
    
    eval_dir = Path("evaluation")
    eval_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = eval_dir / f"ablation_results_{timestamp}.json"
    
    # æ¸…ç†ä¸å¯åºåˆ—åŒ–çš„å†…å®¹
    clean_results = {}
    for mode_key, mode_data in results.items():
        clean_results[mode_key] = {
            "mode": mode_data.get("mode"),
            "mode_name": mode_data.get("mode_name"),
            "metrics": mode_data.get("metrics", {}),
            "results": [
                {
                    "id": r.get("id"),
                    "correct_risk": r.get("correct_risk"),
                    "evidence_valid": r.get("evidence_valid"),
                    "prediction": r.get("prediction", {}),
                    "ground_truth": r.get("ground_truth", {}),
                }
                for r in mode_data.get("results", [])
            ]
        }
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(clean_results, f, ensure_ascii=False, indent=2)
    
    st.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {output_path}")
