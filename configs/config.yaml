llm_config:
  provider: "ollama"
  base_url: "http://localhost:11434"
  model_name: "qwen3:4b-instruct"
  temperature: 0

llm_cloud_config:
  provider: "ollama"
  # 注意：端口变成了 11435 (对应 SSH 隧道)
  base_url: "http://localhost:11435" 
  # 注意：这里不需要 https，也不需要 ngrok header，因为 SSH 也是本地协议
  # 确保名字和云端下载的一致 (如 qwen3:4b 或 qwen2.5:7b)
  model_name: "deepseek-chat"
  temperature: 0.1
  max_tokens: 2000

embedding_config:
  model_path: "C:\\Users\\Lenovo\\.cache\\huggingface\\hub\\models--BAAI--bge-small-zh-v1.5\\snapshots\\7999e1d3359715c523056ef9478215996d62a620"

system_config:
  max_concurrency: 5 # Limit parallel LLM calls

# 混合检索配置
hybrid_search_config:
  alpha: 0.65  # Dense Score 权重 (1-alpha 为 Sparse Score 权重)
  threshold: 0.75  # 相似度阈值，低于此值的匹配将被过滤
  top_k: 3  # Top-K 规则数量：传递给 LLM 的候选规则数量（仅超过阈值的规则）

# Reranker 重排序配置
reranker_config:
  enabled: true  # 是否启用 Reranker
  model_name: "BAAI/bge-reranker-v2-m3"  # Reranker 模型名称
  threshold: 0.5  # Rerank 分数阈值，低于此分数的结果将被过滤
  use_fp16: true  # 是否使用 FP16 加速（推荐）

# ============= 消融实验评测配置 =============
ablation_benchmark_config:
  # 数据集配置
  dataset_path: "evaluation/llm_benchmark_dataset.json"  # 评测数据集路径
  
  # 证据幻觉检测配置
  # 注意：threshold 复用 reranker_config.threshold，top_k 复用 hybrid_search_config.top_k
  hallucination_detection:
    use_two_stage: true  # 是否使用两阶段检测（BGE-M3 召回 + Reranker 精排）
  
  # 评测模式配置
  eval_modes:
    raw_llm: 1  # 原始 LLM
    basic_prompt: 2  # 基础 Prompt
    current_workflow: 3  # 当前工作流（带 RAG）
    optimized_workflow: 4  # 优化后工作流
  
  # 默认设置
  default_mode: 3  # 默认评测模式
  default_source: "local"  # 默认 LLM 来源（local/cloud）