"""
LLM è¯Šæ–­æµ‹è¯•è„šæœ¬
æµ‹è¯• Ollama å’Œ ChatOllama çš„è¿æ¥
"""
import requests
import sys

# é…ç½®
BASE_URL = "http://localhost:11434"
MODEL_NAME = "qwen3:4b-instruct"

def test_ollama_direct():
    """æµ‹è¯• 1: ç›´æ¥ HTTP è°ƒç”¨ Ollama API"""
    print("=" * 50)
    print("æµ‹è¯• 1: ç›´æ¥ HTTP è°ƒç”¨ Ollama API")
    print("=" * 50)
    
    try:
        # æ£€æŸ¥ Ollama æœåŠ¡
        print(f"â†’ æ£€æŸ¥ Ollama æœåŠ¡: {BASE_URL}/api/tags")
        resp = requests.get(f"{BASE_URL}/api/tags", timeout=5)
        print(f"  çŠ¶æ€: {resp.status_code}")
        
        if resp.ok:
            models = [m['name'] for m in resp.json().get('models', [])]
            print(f"  å¯ç”¨æ¨¡å‹: {models}")
            
            if MODEL_NAME not in models:
                print(f"  âš ï¸ è­¦å‘Š: ç›®æ ‡æ¨¡å‹ '{MODEL_NAME}' ä¸åœ¨åˆ—è¡¨ä¸­!")
                # å°è¯•æ‰¾åˆ°åŒ¹é…çš„æ¨¡å‹
                for m in models:
                    if 'qwen' in m.lower():
                        print(f"  ğŸ’¡ å»ºè®®ä½¿ç”¨: {m}")
        
        # æµ‹è¯•ç”Ÿæˆ
        print(f"\nâ†’ æµ‹è¯•ç”Ÿæˆè¯·æ±‚: {BASE_URL}/api/generate")
        resp = requests.post(
            f"{BASE_URL}/api/generate",
            json={
                "model": MODEL_NAME,
                "prompt": "ä½ å¥½",
                "stream": False
            },
            timeout=120
        )
        print(f"  çŠ¶æ€: {resp.status_code}")
        
        if resp.ok:
            data = resp.json()
            response_text = data.get('response', '')[:100]
            print(f"  å“åº” (å‰100å­—ç¬¦): {response_text}")
            print("  âœ… ç›´æ¥ HTTP è°ƒç”¨æˆåŠŸ!")
            return True
        else:
            print(f"  âŒ é”™è¯¯: {resp.text}")
            return False
            
    except Exception as e:
        print(f"  âŒ å¼‚å¸¸: {e}")
        return False

def test_chat_ollama():
    """æµ‹è¯• 2: é€šè¿‡ ChatOllama è°ƒç”¨"""
    print("\n" + "=" * 50)
    print("æµ‹è¯• 2: é€šè¿‡ ChatOllama (LangChain) è°ƒç”¨")
    print("=" * 50)
    
    try:
        from langchain_ollama import ChatOllama
        from langchain_core.messages import HumanMessage
        
        print(f"â†’ åˆå§‹åŒ– ChatOllama: model={MODEL_NAME}")
        llm = ChatOllama(
            base_url=BASE_URL,
            model=MODEL_NAME,
            temperature=0.1,
            timeout=120,
        )
        
        print("â†’ å‘é€æµ‹è¯•è¯·æ±‚...")
        response = llm.invoke([HumanMessage(content="ä½ å¥½")])
        
        content = getattr(response, 'content', str(response))[:100]
        print(f"  å“åº” (å‰100å­—ç¬¦): {content}")
        print("  âœ… ChatOllama è°ƒç”¨æˆåŠŸ!")
        return True
        
    except Exception as e:
        print(f"  âŒ å¼‚å¸¸: {type(e).__name__}: {e}")
        
        # å°è¯•è·å–æ›´å¤šé”™è¯¯ä¿¡æ¯
        import traceback
        print("\nè¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()
        return False

def test_chat_ollama_sync():
    """æµ‹è¯• 3: ä½¿ç”¨åŒæ­¥æ–¹å¼è°ƒç”¨ ChatOllama"""
    print("\n" + "=" * 50)
    print("æµ‹è¯• 3: ChatOllama ä½¿ç”¨ httpx å®¢æˆ·ç«¯")
    print("=" * 50)
    
    try:
        from langchain_ollama import ChatOllama
        from langchain_core.messages import HumanMessage
        import httpx
        
        print(f"â†’ åˆå§‹åŒ– ChatOllama (ä½¿ç”¨ httpx)")
        
        # åˆ›å»ºè‡ªå®šä¹‰ httpx å®¢æˆ·ç«¯
        client = httpx.Client(timeout=120.0)
        
        llm = ChatOllama(
            base_url=BASE_URL,
            model=MODEL_NAME,
            temperature=0.1,
            client=client,
        )
        
        print("â†’ å‘é€æµ‹è¯•è¯·æ±‚...")
        response = llm.invoke([HumanMessage(content="ä½ å¥½")])
        
        content = getattr(response, 'content', str(response))[:100]
        print(f"  å“åº” (å‰100å­—ç¬¦): {content}")
        print("  âœ… ChatOllama (httpx) è°ƒç”¨æˆåŠŸ!")
        return True
        
    except Exception as e:
        print(f"  âŒ å¼‚å¸¸: {type(e).__name__}: {e}")
        return False

if __name__ == "__main__":
    print("\nğŸ” LLM è¯Šæ–­æµ‹è¯•\n")
    
    results = []
    
    # æµ‹è¯• 1
    results.append(("ç›´æ¥ HTTP", test_ollama_direct()))
    
    # æµ‹è¯• 2
    results.append(("ChatOllama", test_chat_ollama()))
    
    # æµ‹è¯• 3
    results.append(("ChatOllama (httpx)", test_chat_ollama_sync()))
    
    # æ±‡æ€»
    print("\n" + "=" * 50)
    print("æµ‹è¯•æ±‡æ€»")
    print("=" * 50)
    for name, passed in results:
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"  {name}: {status}")
