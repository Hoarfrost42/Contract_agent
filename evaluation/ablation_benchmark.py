"""
æ¶ˆèå®éªŒè¯„æµ‹è„šæœ¬ (Ablation Study Benchmark)

æ”¯æŒ4ç§è¯„æµ‹æ¨¡å¼å¯¹æ¯”ï¼š
- Mode 1: çº¯LLMï¼ˆæ— Promptæ¨¡æ¿ï¼Œç›´æ¥è¾“å…¥æ¡æ¬¾ï¼‰
- Mode 2: åŸºç¡€Promptï¼ˆæœ‰æ ¼å¼åŒ–Promptï¼Œæ— è§„åˆ™å¼•æ“ï¼‰
- Mode 3: å½“å‰å·¥ä½œæµï¼ˆPrompt + è§„åˆ™å¼•æ“ï¼‰
- Mode 4: ä¼˜åŒ–å·¥ä½œæµï¼ˆæ”¹è¿›Prompt + è§„åˆ™å¼•æ“ï¼‰

è¯„æµ‹æŒ‡æ ‡ï¼š
- é£é™©ç­‰çº§å‡†ç¡®ç‡ (Accuracy)
- F1 åˆ†æ•° (Precision/Recall å¹³è¡¡)
- Jaccard ç›¸ä¼¼åº¦
- å¹»è§‰ç‡ (å¼•ç”¨éªŒè¯å¤±è´¥æ¯”ä¾‹)
- è§£ææˆåŠŸç‡
"""

import argparse
import asyncio
import json
import os
import sys
import re
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field

# Add project root to sys.path
sys.path.append(str(Path(__file__).resolve().parents[1]))

from src.utils.config_loader import load_config

# å¯¼å…¥ç»Ÿä¸€æ¨¡å—
from src.core.ollama_client import OllamaClient
from src.core.output_parser import parse_markdown_output, ParsedResult
from src.core.prompts import (
    RAW_LLM_PROMPT,
    BASIC_PROMPT, 
    CURRENT_WORKFLOW_PROMPT,
    OPTIMIZED_WORKFLOW_PROMPT,
    SELF_REFLECTION_PROMPT,
    get_prompt_by_mode,
)
from src.core.reference_retriever import retrieve_reference



# ============================================================================
# OllamaClient å·²ç§»è‡³ src/core/ollama_client.pyï¼Œé€šè¿‡å¯¼å…¥ä½¿ç”¨
# ============================================================================


# ============================================================================
# è¯„æµ‹æ¨¡å¼å®šä¹‰
# ============================================================================

class EvalMode:
    """è¯„æµ‹æ¨¡å¼æšä¸¾"""
    RAW_LLM = 1           # çº¯LLMï¼Œæ— Promptæ¨¡æ¿
    BASIC_PROMPT = 2      # åŸºç¡€Promptï¼Œæ— è§„åˆ™å¼•æ“
    CURRENT_WORKFLOW = 3  # å½“å‰å·¥ä½œæµï¼ˆPrompt + è§„åˆ™å¼•æ“ï¼‰
    OPTIMIZED_WORKFLOW = 4  # ä¼˜åŒ–å·¥ä½œæµï¼ˆæ”¹è¿›Prompt + è§„åˆ™å¼•æ“ï¼‰
    
    @staticmethod
    def name(mode: int) -> str:
        names = {
            1: "çº¯LLM (Raw)",
            2: "åŸºç¡€Prompt",
            3: "å½“å‰å·¥ä½œæµ",
            4: "ä¼˜åŒ–å·¥ä½œæµ"
        }
        return names.get(mode, "æœªçŸ¥")




# ============================================================================
# Prompt æ¨¡æ¿å·²ç§»è‡³ src/core/prompts.pyï¼Œé€šè¿‡å¯¼å…¥ä½¿ç”¨
# ============================================================================




# ============================================================================
# ParsedResult å’Œ parse_markdown_output å·²ç§»è‡³ src/core/output_parser.py
# ============================================================================


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def stratified_sample(dataset: List[dict], limit: int, seed: int = 42) -> List[dict]:
    """åˆ†å±‚é‡‡æ ·ï¼šç¡®ä¿ High/Medium/Low æ¯”ä¾‹å°½é‡ä¸º 1:1:1
    
    Args:
        dataset: å®Œæ•´æ•°æ®é›†
        limit: é‡‡æ ·æ•°é‡
        seed: éšæœºç§å­ï¼ˆå›ºå®šç§å­ç¡®ä¿å¯é‡å¤æ€§ï¼‰
    """
    import random
    random.seed(seed)  # å›ºå®šç§å­ï¼Œç¡®ä¿æ¯æ¬¡è¿è¡Œç›¸åŒ
    
    high = [d for d in dataset if d.get("ground_truth", {}).get("risk_level") == "é«˜"]
    medium = [d for d in dataset if d.get("ground_truth", {}).get("risk_level") == "ä¸­"]
    low = [d for d in dataset if d.get("ground_truth", {}).get("risk_level") == "ä½"]
    
    per_class = limit // 3
    remainder = limit % 3  # å¤„ç†é™¤ä¸å°½çš„æƒ…å†µ
    
    sampled = []
    
    # 1. æ ¸å¿ƒé‡‡æ ·ï¼šæ¯ç±»æŠ½å– limit/3
    sampled.extend(random.sample(high, min(len(high), per_class)))
    sampled.extend(random.sample(medium, min(len(medium), per_class)))
    sampled.extend(random.sample(low, min(len(low), per_class)))
    
    # 2. è¡¥é½å‰©ä½™ï¼ˆè½®æµä»å„ç±»è¡¥å……ï¼Œç¡®ä¿å¹³è¡¡ï¼‰
    current_count = len(sampled)
    if current_count < limit:
        sampled_ids = {item.get("id") for item in sampled}
        remaining_high = [d for d in high if d.get("id") not in sampled_ids]
        remaining_medium = [d for d in medium if d.get("id") not in sampled_ids]
        remaining_low = [d for d in low if d.get("id") not in sampled_ids]
        
        # è½®æµä»å„ç±»è¡¥å……ï¼Œç¡®ä¿å¹³è¡¡
        pools = [remaining_high, remaining_medium, remaining_low]
        pool_idx = 0
        needed = limit - current_count
        while needed > 0 and any(pools):
            if pools[pool_idx]:
                sampled.append(pools[pool_idx].pop(0))
                needed -= 1
            pool_idx = (pool_idx + 1) % 3
    
    random.shuffle(sampled)
    return sampled


def parse_reflection_output(content: str) -> dict:
    """è§£æè‡ªåæ€è¾“å‡ºï¼ˆé€‚é…æ–°æ ¼å¼ï¼‰
    
    æœŸæœ›æ ¼å¼ï¼š
    å®¡æŸ¥ç»“è®ºï¼š[ç»´æŒ / è°ƒçº§]
    æœ€ç»ˆé£é™©ç­‰çº§ï¼š[é«˜é£é™© / ä¸­é£é™© / ä½é£é™©]
    ä¿®æ­£ç†ç”±ï¼š[ç†ç”±]
    
    Returns:
        dict: {
            "conclusion": "ç»´æŒ" / "è°ƒçº§",
            "final_level": "é«˜" / "ä¸­" / "ä½" / None,
            "reason": "..."
        }
    """
    result = {
        "conclusion": "ç»´æŒ",
        "final_level": None,
        "reason": ""
    }
    
    # è§£æå®¡æŸ¥ç»“è®º
    conclusion_match = re.search(r'å®¡æŸ¥ç»“è®º[ï¼š:]\s*\[?\s*(ç»´æŒ|è°ƒçº§)\s*\]?', content)
    if conclusion_match:
        result["conclusion"] = conclusion_match.group(1)
    
    # è§£ææœ€ç»ˆé£é™©ç­‰çº§ï¼ˆæ ¸å¿ƒå­—æ®µï¼‰
    # åŒ¹é… "æœ€ç»ˆé£é™©ç­‰çº§ï¼šé«˜é£é™©" æˆ– "æœ€ç»ˆé£é™©ç­‰çº§ï¼š[é«˜é£é™©]"
    level_match = re.search(r'æœ€ç»ˆé£é™©ç­‰çº§[ï¼š:]\s*\[?\s*([é«˜ä¸­ä½])é£é™©\s*\]?', content)
    if level_match:
        result["final_level"] = level_match.group(1)
    
    # è§£æä¿®æ­£ç†ç”±
    reason_match = re.search(r'ä¿®æ­£ç†ç”±[ï¼š:]\s*\[?\s*(.+?)\s*\]?(?:\n|$)', content, re.DOTALL)
    if reason_match:
        result["reason"] = reason_match.group(1).strip()[:100]
    
    return result


# ============================================================================
# è¯„ä¼°å™¨
# ============================================================================

@dataclass
class EvalMetrics:
    """è¯„ä¼°æŒ‡æ ‡"""
    # åŸºç¡€æŒ‡æ ‡
    total: int = 0
    correct_risk: int = 0
    correct_reason: int = 0
    parse_success: int = 0
    
    # åŠ æƒè¯„åˆ†ï¼ˆç²¾ç¡®åŒ¹é…1åˆ†ï¼Œå·®ä¸€çº§0.5åˆ†ï¼Œå·®ä¸¤çº§0åˆ†ï¼‰
    total_weighted_score: float = 0.0
    
    # ä¸‰åˆ†ç±»æ··æ·†çŸ©é˜µ (High=0, Medium=1, Low=2)
    # confusion_matrix[actual][predicted]
    conf_matrix: List[List[int]] = field(default_factory=lambda: [[0, 0, 0], [0, 0, 0], [0, 0, 0]])
    
    # æ··æ·†çŸ©é˜µï¼ˆæ—§äºŒåˆ†ç±»å…¼å®¹ï¼Œç”¨äº Precision/Recall/F1ï¼‰
    true_positive: int = 0   
    false_positive: int = 0  
    false_negative: int = 0  
    true_negative: int = 0   
    
    # ===== æ–¹æ³•ä¸€ï¼šè¯æ®ä¸€è‡´æ€§è¯„ä¼°ï¼ˆç»†åˆ†å¹»è§‰ç±»å‹ï¼‰=====
    clause_evidence_valid: int = 0    # åˆåŒæ¡æ¬¾è¯æ®æœ‰æ•ˆ
    clause_evidence_invalid: int = 0  # åˆåŒæ¡æ¬¾è¯æ®å¹»è§‰
    law_citation_valid: int = 0       # æ³•å¾‹å¼•ç”¨æœ‰æ•ˆ
    law_citation_invalid: int = 0     # æ³•å¾‹å¼•ç”¨å¹»è§‰ï¼ˆä¸å­˜åœ¨çš„æ³•æ¡ï¼‰
    
    # æ—§å­—æ®µä¿æŒå…¼å®¹
    evidence_valid: int = 0
    evidence_invalid: int = 0
    
    # ===== æ–¹æ³•äºŒï¼šè§„åˆ™è§¦å‘ä¸€è‡´æ€§ =====
    rule_trigger_count: int = 0   # å®é™…è§¦å‘çš„è§„åˆ™æ•°
    rule_target_count: int = 0    # åº”è¯¥è§¦å‘çš„è§„åˆ™æ•°
    rule_correct_count: int = 0   # æ­£ç¡®è§¦å‘çš„è§„åˆ™æ•°
    
    # ===== Risk ID åŒ¹é…ï¼ˆå¤šæ ‡ç­¾åœºæ™¯ï¼ŒPrecision/Recall/F1ï¼‰=====
    risk_id_precision_sum: float = 0.0  # Precision ç´¯åŠ 
    risk_id_recall_sum: float = 0.0     # Recall ç´¯åŠ 
    risk_id_f1_sum: float = 0.0         # F1 ç´¯åŠ 
    risk_id_count: int = 0              # æœ‰æ•ˆæ ·æœ¬æ•°ï¼ˆç”¨äºè®¡ç®—å¹³å‡å€¼ï¼‰
    
    # ===== æ–¹æ³•ä¸‰ï¼šä»»åŠ¡æˆåŠŸç‡ =====
    task_success_count: int = 0  # ä»»åŠ¡å®Œå…¨æˆåŠŸçš„æ ·æœ¬æ•°
    
    # ===== è‡ªåæ€æœºåˆ¶ç»Ÿè®¡ =====
    reflection_calls: int = 0           # è‡ªåæ€è°ƒç”¨æ¬¡æ•°
    reflection_adjustments: int = 0     # åæ€åè°ƒçº§æ¬¡æ•°
    reflection_maintain: int = 0        # åæ€åç»´æŒåŸåˆ¤æ¬¡æ•°
    # è¯¦ç»†è°ƒçº§ç»Ÿè®¡ (æ ¼å¼: "åˆå§‹â†’æœ€ç»ˆ": æ¬¡æ•°)
    reflection_transitions: Dict[str, int] = None  # å°†åœ¨ __post_init__ åˆå§‹åŒ–
    # æ¡ä»¶æ€§åæ€ç»Ÿè®¡
    reflection_skipped_high_conf: int = 0      # å› é«˜ç½®ä¿¡åº¦(>=0.7)è·³è¿‡åæ€
    reflection_triggered_medium_conf: int = 0  # å› ä¸­ç­‰ç½®ä¿¡åº¦(0.5-0.7)è§¦å‘åæ€
    reflection_triggered_low_conf: int = 0     # å› ä½ç½®ä¿¡åº¦(<0.5ï¼Œç©ºè§„åˆ™)è§¦å‘åæ€
    
    # å“åº”æ—¶é—´ç»Ÿè®¡
    total_latency: float = 0.0
    
    def __post_init__(self):
        """åˆå§‹åŒ–å¯å˜é»˜è®¤å€¼"""
        if self.reflection_transitions is None:
            self.reflection_transitions = {}
    @staticmethod
    def calculate_weighted_score(gt_risk: str, pred_risk: str) -> float:
        """
        è®¡ç®—éå¯¹ç§°åŠ æƒè¯„åˆ† (Asymmetric Weighted Accuracy)ï¼š
        - ç²¾ç¡®åŒ¹é…ï¼š1.0åˆ†
        - é˜²å¾¡æ€§è¯¯åˆ¤ï¼ˆä½â†’ä¸­, ä¸­â†’é«˜ï¼‰ï¼š0.8åˆ†ï¼ˆå®å¯é”™æ€ï¼‰
        - é£é™©é™çº§ï¼ˆé«˜â†’ä¸­ï¼‰ï¼š0.4åˆ†ï¼ˆæ¼æŠ¥æ‰£åˆ†æ›´é‡ï¼‰
        - å·®ä¸¤çº§ï¼ˆé«˜â†”ä½ï¼‰ï¼š0.0åˆ†ï¼ˆè‡´å‘½æ¼åˆ¤é›¶å®¹å¿ï¼‰
        """
        if gt_risk == pred_risk:
            return 1.0
        
        # éå¯¹ç§°æƒé‡çŸ©é˜µ: weight_matrix[gt][pred]
        # gt: é«˜=0, ä¸­=1, ä½=2
        asymmetric_weights = {
            ("é«˜", "ä¸­"): 0.4,  # é«˜é£é™©é™ä¸ºä¸­ï¼šå±é™©ï¼Œæ‰£åˆ†é‡
            ("é«˜", "ä½"): 0.0,  # é«˜é£é™©é™ä¸ºä½ï¼šè‡´å‘½æ¼åˆ¤
            ("ä¸­", "é«˜"): 0.8,  # ä¸­é£é™©å‡ä¸ºé«˜ï¼šè¿‡åº¦è°¨æ…ï¼Œå¯æ¥å—
            ("ä¸­", "ä½"): 0.4,  # ä¸­é£é™©é™ä¸ºä½ï¼šæœ‰ä¸€å®šé£é™©
            ("ä½", "ä¸­"): 0.8,  # ä½é£é™©å‡ä¸ºä¸­ï¼šé˜²å¾¡æ€§è¯¯åˆ¤
            ("ä½", "é«˜"): 0.5,  # ä½é£é™©å‡ä¸ºé«˜ï¼šè¿‡åº¦æŠ¥è­¦
        }
        
        return asymmetric_weights.get((gt_risk, pred_risk), 0.0)
            
    def update_confusion_matrix(self, gt_risk: str, pred_risk: str):
        """æ›´æ–°ä¸‰åˆ†ç±»æ··æ·†çŸ©é˜µ"""
        level_map = {"é«˜": 0, "ä¸­": 1, "ä½": 2}
        gt_idx = level_map.get(gt_risk, 2)   # é»˜è®¤ä¸ºä½é£é™©
        pred_idx = level_map.get(pred_risk, 2)
        self.conf_matrix[gt_idx][pred_idx] += 1

    def calculate_kappa(self, use_linear: bool = True) -> float:
        """
        è®¡ç®—åŠ æƒ Kappa
        
        Args:
            use_linear: True=çº¿æ€§æƒé‡(LWK), False=äºŒæ¬¡æ–¹æƒé‡(QWK)
        
        Linear Weighted Kappa å…¬å¼: w_ij = 1 - |i-j| / (N-1)
        å¯¹äºæœ‰åºåˆ†ç±»æ›´ç¨³å¥ï¼Œä¸ä¼šè¿‡åº¦æƒ©ç½š"ç¦»ç¾¤"é”™è¯¯
        """
        n_classes = 3
        weights = [[0.0] * n_classes for _ in range(n_classes)]
        
        for i in range(n_classes):
            for j in range(n_classes):
                if use_linear:
                    # Linear Weighted Kappa
                    weights[i][j] = abs(i - j) / (n_classes - 1)
                else:
                    # Quadratic Weighted Kappa (åŸç‰ˆ)
                    weights[i][j] = ((i - j) / (n_classes - 1)) ** 2
                
        # è§‚å¯ŸçŸ©é˜µ O (å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ)
        total = self.total
        if total == 0: return 0.0
        
        observed = [[self.conf_matrix[i][j] / total for j in range(n_classes)] for i in range(n_classes)]
        
        # æœŸæœ›çŸ©é˜µ E (è¾¹ç¼˜åˆ†å¸ƒå¤–ç§¯)
        row_sums = [sum(self.conf_matrix[i]) / total for i in range(n_classes)]
        col_sums = [sum(self.conf_matrix[i][j] for i in range(n_classes)) / total for j in range(n_classes)]
        expected = [[row_sums[i] * col_sums[j] for j in range(n_classes)] for i in range(n_classes)]
        
        # è®¡ç®— Kappa = 1 - (sum(W*O) / sum(W*E))
        numerator = sum(weights[i][j] * observed[i][j] for i in range(n_classes) for j in range(n_classes))
        denominator = sum(weights[i][j] * expected[i][j] for i in range(n_classes) for j in range(n_classes))
        
        if denominator == 0: return 1.0  # å®Œå…¨ä¸€è‡´
        return 1.0 - (numerator / denominator)

    def calculate_macro_f1(self) -> dict:
        """è®¡ç®—å®å¹³å‡ F1"""
        f1_scores = []
        precisions = []
        recalls = []
        
        for k in range(3):  # 0:é«˜, 1:ä¸­, 2:ä½
            # TP = conf[k][k]
            tp = self.conf_matrix[k][k]
            # FP = sum(col[k]) - TP
            fp = sum(self.conf_matrix[i][k] for i in range(3)) - tp
            # FN = sum(row[k]) - TP
            fn = sum(self.conf_matrix[k]) - tp
            
            p = tp / (tp + fp) if (tp + fp) > 0 else 0
            r = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0
            
            precisions.append(p)
            recalls.append(r)
            f1_scores.append(f1)
            
        return {
            "macro_precision": sum(precisions) / 3,
            "macro_recall": sum(recalls) / 3,
            "macro_f1": sum(f1_scores) / 3,
            "class_f1": {"High": f1_scores[0], "Medium": f1_scores[1], "Low": f1_scores[2]},
            "class_precision": {"High": precisions[0], "Medium": precisions[1], "Low": precisions[2]},
            "class_recall": {"High": recalls[0], "Medium": recalls[1], "Low": recalls[2]}
        }
    
    def calculate_high_risk_f2(self) -> float:
        """
        è®¡ç®—é«˜é£é™©ç±»åˆ«çš„ F2-Score (Recall-Oriented)
        
        F2 = (1 + Î²Â²) Ã— (P Ã— R) / (Î²Â² Ã— P + R)ï¼Œå…¶ä¸­ Î² = 2
        F2 åˆ†æ•°ä¸­ Recall æƒé‡æ˜¯ Precision çš„ 2 å€ï¼Œé€‚åˆ"å®å¯é”™æ€"çš„é£æ§åœºæ™¯
        """
        # High = index 0 in confusion matrix
        tp = self.conf_matrix[0][0]
        fp = sum(self.conf_matrix[i][0] for i in range(3)) - tp
        fn = sum(self.conf_matrix[0]) - tp
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        
        beta = 2  # F2-Score
        if (precision + recall) == 0:
            return 0.0
        
        f2 = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall)
        return f2
    
    def weighted_accuracy(self) -> float:
        """åŠ æƒå‡†ç¡®ç‡ï¼ˆè€ƒè™‘éƒ¨åˆ†åŒ¹é…ï¼‰"""
        return self.total_weighted_score / self.total if self.total > 0 else 0
    
    def accuracy(self) -> float:
        return self.correct_risk / self.total if self.total > 0 else 0
    
    def precision(self) -> float:
        denom = self.true_positive + self.false_positive
        return self.true_positive / denom if denom > 0 else 0
    
    def recall(self) -> float:
        denom = self.true_positive + self.false_negative
        return self.true_positive / denom if denom > 0 else 0
    
    def f1(self) -> float:
        p, r = self.precision(), self.recall()
        return 2 * p * r / (p + r) if (p + r) > 0 else 0
    
    def parse_rate(self) -> float:
        return self.parse_success / self.total if self.total > 0 else 0
    
    def hallucination_rate(self) -> float:
        total_evidence = self.evidence_valid + self.evidence_invalid
        return self.evidence_invalid / total_evidence if total_evidence > 0 else 0
    
    # ===== æ–°å¢ï¼šç»†åˆ†å¹»è§‰ç‡ =====
    def clause_hallucination_rate(self) -> float:
        """åˆåŒæ¡æ¬¾è¯æ®å¹»è§‰ç‡"""
        total = self.clause_evidence_valid + self.clause_evidence_invalid
        return self.clause_evidence_invalid / total if total > 0 else 0
    
    def law_hallucination_rate(self) -> float:
        """æ³•å¾‹å¼•ç”¨å¹»è§‰ç‡"""
        total = self.law_citation_valid + self.law_citation_invalid
        return self.law_citation_invalid / total if total > 0 else 0
    
    # ===== æ–°å¢ï¼šè§„åˆ™è§¦å‘ä¸€è‡´æ€§ =====
    def rule_recall(self) -> float:
        """è§„åˆ™å¬å›ç‡ï¼šæ­£ç¡®è§¦å‘ / åº”è§¦å‘"""
        return self.rule_correct_count / self.rule_target_count if self.rule_target_count > 0 else 0
    
    def rule_precision(self) -> float:
        """è§„åˆ™ç²¾ç¡®ç‡ï¼šæ­£ç¡®è§¦å‘ / å®é™…è§¦å‘"""
        return self.rule_correct_count / self.rule_trigger_count if self.rule_trigger_count > 0 else 0
    
    # ===== æ–°å¢ï¼šä»»åŠ¡æˆåŠŸç‡ =====
    def task_success_rate(self) -> float:
        """ä»»åŠ¡æˆåŠŸç‡ï¼šå®Œå…¨æˆåŠŸçš„æ ·æœ¬ / æ€»æ ·æœ¬"""
        return self.task_success_count / self.total if self.total > 0 else 0
    
    def risk_id_accuracy(self) -> float:
        return self.risk_id_match / self.risk_id_total if self.risk_id_total > 0 else 0
    
    def avg_latency(self) -> float:
        return self.total_latency / self.total if self.total > 0 else 0
    
    def to_dict(self) -> dict:
        macro = self.calculate_macro_f1()
        return {
            "total": self.total,
            # åŸºç¡€æŒ‡æ ‡
            "accuracy": round(self.correct_risk / self.total, 4) if self.total > 0 else 0,
            "weighted_accuracy": round(self.total_weighted_score / self.total, 4) if self.total > 0 else 0,
            
            # Kappa æŒ‡æ ‡ (æ–°å¢ LWK å’Œ QWK å¯¹æ¯”)
            "kappa_linear": round(self.calculate_kappa(use_linear=True), 4),  # çº¿æ€§åŠ æƒ Kappa (æ¨è)
            "kappa_quadratic": round(self.calculate_kappa(use_linear=False), 4),  # äºŒæ¬¡æ–¹åŠ æƒ Kappa (å¯¹æ¯”)
            "kappa": round(self.calculate_kappa(use_linear=True), 4),  # é»˜è®¤ä½¿ç”¨ LWKï¼Œä¿æŒå…¼å®¹
            
            # ä¸‰åˆ†ç±»æŒ‡æ ‡
            "macro_precision": round(macro["macro_precision"], 4),
            "macro_recall": round(macro["macro_recall"], 4),
            "macro_f1": round(macro["macro_f1"], 4),
            
            # é«˜é£é™© F2-Score (æ–°å¢ - Recall ä¼˜å…ˆ)
            "high_risk_f2": round(self.calculate_high_risk_f2(), 4),
            
            # åˆ†ç±»åˆ«æŒ‡æ ‡
            "class_f1": {
                "High": round(macro["class_f1"]["High"], 4),
                "Medium": round(macro["class_f1"]["Medium"], 4),
                "Low": round(macro["class_f1"]["Low"], 4)
            },
            "class_precision": {
                "High": round(macro["class_precision"]["High"], 4),
                "Medium": round(macro["class_precision"]["Medium"], 4),
                "Low": round(macro["class_precision"]["Low"], 4)
            },
            "class_recall": {
                "High": round(macro["class_recall"]["High"], 4),
                "Medium": round(macro["class_recall"]["Medium"], 4),
                "Low": round(macro["class_recall"]["Low"], 4)
            },
            
            # æ··æ·†çŸ©é˜µ (ç”¨äºç»˜å›¾)
            "conf_matrix": self.conf_matrix,
            
            # è¿‡ç¨‹æŒ‡æ ‡
            "parse_rate": round(self.parse_success / self.total, 4) if self.total > 0 else 0,
            
            # å¹»è§‰æ£€æµ‹
            "hallucination_rate": round(self.evidence_invalid / (self.evidence_valid + self.evidence_invalid), 4) if (self.evidence_valid + self.evidence_invalid) > 0 else 0,
            "clause_hallucination_rate": round(self.clause_evidence_invalid / (self.clause_evidence_valid + self.clause_evidence_invalid), 4) if (self.clause_evidence_valid + self.clause_evidence_invalid) > 0 else 0,
            "law_hallucination_rate": round(self.law_citation_invalid / (self.law_citation_valid + self.law_citation_invalid), 4) if (self.law_citation_valid + self.law_citation_invalid) > 0 else 0,
            
            # è§„åˆ™è§¦å‘
            "rule_recall": round(self.rule_correct_count / self.rule_target_count, 4) if self.rule_target_count > 0 else 0,
            "rule_precision": round(self.rule_correct_count / self.rule_trigger_count, 4) if self.rule_trigger_count > 0 else 0,
            
            # ä»»åŠ¡æˆåŠŸç‡
            "task_success_rate": round(self.task_success_count / self.total, 4) if self.total > 0 else 0,
            
            # Risk ID åŒ¹é…æŒ‡æ ‡ (Precision/Recall/F1)
            "risk_id_precision": round(self.risk_id_precision_sum / self.risk_id_count, 4) if self.risk_id_count > 0 else 0,
            "risk_id_recall": round(self.risk_id_recall_sum / self.risk_id_count, 4) if self.risk_id_count > 0 else 0,
            "risk_id_f1": round(self.risk_id_f1_sum / self.risk_id_count, 4) if self.risk_id_count > 0 else 0,
            "risk_id_accuracy": round(self.risk_id_precision_sum / self.risk_id_count, 4) if self.risk_id_count > 0 else 0,  # å…¼å®¹æ—§å­—æ®µ
            
            # æ€§èƒ½
            "avg_latency_sec": round(self.total_latency / self.total, 3) if self.total > 0 else 0,
            "reason_quality": round(self.correct_reason / self.total, 4) if self.total > 0 else 0,
            
            # è‡ªåæ€æœºåˆ¶ç»Ÿè®¡
            "reflection_calls": self.reflection_calls,
            "reflection_adjustments": self.reflection_adjustments,
            "reflection_maintain": self.reflection_maintain,
            "reflection_adjustment_rate": round(self.reflection_adjustments / self.reflection_calls, 4) if self.reflection_calls > 0 else 0,
            "reflection_transitions": self.reflection_transitions,  # è¯¦ç»†è°ƒçº§æ–¹å‘ç»Ÿè®¡
            # æ¡ä»¶æ€§åæ€ç»Ÿè®¡
            "reflection_skipped_high_conf": self.reflection_skipped_high_conf,
            "reflection_triggered_medium_conf": self.reflection_triggered_medium_conf,
            "reflection_triggered_low_conf": self.reflection_triggered_low_conf,
        }


def verify_evidence(
    evidence: str, 
    clause: str, 
    embedding_model=None, 
    reranker=None,
    threshold: float = 0.5
) -> tuple:
    """éªŒè¯è¯æ®æ˜¯å¦å­˜åœ¨äºåŸæ–‡ä¸­ï¼ˆä¸¤é˜¶æ®µæ£€ç´¢ + ç²¾æ’ï¼‰
    
    æµç¨‹ï¼š
    1. ç²¾ç¡®å­ä¸²åŒ¹é…ï¼ˆå¿«é€Ÿè·¯å¾„ï¼‰
    2. Stage 1: BGE-M3 Dense å¬å› Top-K å€™é€‰
    3. Stage 2: Reranker ç²¾æ’ï¼Œåˆ¤å®šç›¸ä¼¼åº¦æ˜¯å¦ >= threshold
    4. å›é€€ï¼šæ¨¡ç³ŠåŒ¹é…ï¼ˆå­—ç¬¦é‡å ï¼‰
    
    Args:
        evidence: LLM è¾“å‡ºçš„è¯æ®å­—æ®µ
        clause: åŸå§‹åˆåŒæ¡æ¬¾æ–‡æœ¬
        embedding_model: Sentence Transformer æ¨¡å‹ï¼ˆç”¨äºå¬å›ï¼‰
        reranker: BGE-Reranker æ¨¡å‹ï¼ˆç”¨äºç²¾æ’ï¼‰
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.7ï¼‰
    
    Returns:
        tuple: (is_valid: bool, similarity_score: float, match_type: str)
        - is_valid: è¯æ®æ˜¯å¦æœ‰æ•ˆ
        - similarity_score: ç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆ0-1ï¼‰
        - match_type: åŒ¹é…ç±»å‹ï¼ˆexact/reranker_match/semantic/fuzzy/hallucinationï¼‰
    """
    if not evidence or evidence in ["æ— ", "None", "", "ç•™ç©º"]:
        return (True, 1.0, "empty")  # æ— è¯æ®ä¸ç®—å¹»è§‰
    
    # æ¸…ç†è¯æ®æ–‡æœ¬
    clean_evidence = evidence.replace("ã€Œ", "").replace("ã€", "").replace("\"", "").strip()
    if len(clean_evidence) < 5:
        return (True, 1.0, "too_short")
    
    # 1. ç²¾ç¡®åŒ¹é…ï¼ˆå­ä¸²åŒ¹é…ï¼‰- å¿«é€Ÿè·¯å¾„
    if clean_evidence in clause:
        return (True, 1.0, "exact")
    
    # åˆ†å¥
    sentences = [s.strip() for s in clause.replace("ã€‚", "ã€‚\n").split("\n") if s.strip()]
    if not sentences:
        sentences = [clause]
    
    # 2. Stage 1: BGE-M3 Dense å¬å› Top-K å€™é€‰
    candidates = sentences[:3]  # é»˜è®¤å–å‰3å¥
    if embedding_model is not None:
        try:
            from sklearn.metrics.pairwise import cosine_similarity
            
            evidence_embedding = embedding_model.encode([clean_evidence])
            sentence_embeddings = embedding_model.encode(sentences)
            
            similarities = cosine_similarity(evidence_embedding, sentence_embeddings)[0]
            
            # å– Top-3 å€™é€‰è¿›å…¥ç²¾æ’
            top_k_indices = similarities.argsort()[-3:][::-1]
            candidates = [sentences[i] for i in top_k_indices]
            
            # å¦‚æœæ²¡æœ‰ rerankerï¼Œç›´æ¥ä½¿ç”¨ embedding ç›¸ä¼¼åº¦
            if reranker is None:
                max_similarity = float(max(similarities))
                if max_similarity >= threshold:
                    return (True, max_similarity, "semantic")
        except Exception as e:
            logging.warning(f"è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
    
    # 3. Stage 2: Reranker ç²¾æ’
    if reranker is not None and candidates:
        try:
            rerank_results = reranker.rerank(
                query=clean_evidence,
                candidates=[{"text": s} if isinstance(s, str) else s for s in candidates],
                top_k=1
            )
            if rerank_results:
                best_score = rerank_results[0][1]  # (doc, score)
                if best_score >= threshold:
                    return (True, best_score, "reranker_match")
                else:
                    return (False, best_score, "hallucination")
        except Exception as e:
            logging.warning(f"Reranker ç²¾æ’å¤±è´¥: {e}")
    
    # 4. å›é€€ï¼šæ¨¡ç³ŠåŒ¹é…ï¼ˆå­—ç¬¦é‡å ï¼‰
    evidence_words = set(clean_evidence)
    clause_words = set(clause)
    overlap = len(evidence_words & clause_words) / len(evidence_words) if evidence_words else 0
    
    if overlap >= 0.6:
        return (True, overlap, "fuzzy")
    
    return (False, overlap, "hallucination")



def verify_law_citation(
    law_reference: str, 
    reference_info: str = "",
    law_db_path: str = None
) -> tuple:
    """éªŒè¯æ³•å¾‹å¼•ç”¨æ˜¯å¦æœ‰æ•ˆï¼ˆåˆ†çº§éªŒè¯ï¼‰
    
    éªŒè¯é¡ºåº:
    1. æ£€æŸ¥æ˜¯å¦åœ¨ reference_info ä¸­æåŠ
    2. æ£€æŸ¥æ³•å¾‹åç§°æ˜¯å¦åœ¨æ•°æ®åº“/ç™½åå•ä¸­
    3. æ£€æŸ¥æ¡æ¬¾å·æ ¼å¼æ˜¯å¦æ­£ç¡®
    4. æ£€æŸ¥å¼•ç”¨å†…å®¹æ˜¯å¦ä¸æ•°æ®åº“ä¸€è‡´ï¼ˆå¦‚æœ‰ï¼‰
    
    Args:
        law_reference: LLM è¾“å‡ºçš„æ³•æ¡å¼•ç”¨
        reference_info: è§„åˆ™å¼•æ“æä¾›çš„å‚è€ƒä¿¡æ¯
        law_db_path: æ³•å¾‹æ•°æ®åº“è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        tuple: (is_valid: bool, validation_level: int, detail: str)
        - is_valid: æ³•æ¡æ˜¯å¦æœ‰æ•ˆ
        - validation_level: éªŒè¯é€šè¿‡çš„å±‚çº§ï¼ˆ1-4ï¼Œ0è¡¨ç¤ºå¤±è´¥ï¼‰
        - detail: éªŒè¯è¯¦æƒ…
    """
    import re
    
    if not law_reference or law_reference in ["æ— ", "None", "", "ç•™ç©º"]:
        return (True, 0, "empty")  # æ— å¼•ç”¨ä¸ç®—å¹»è§‰
    
    # æå–æ³•å¾‹åç§°ï¼ˆæ”¯æŒã€Šã€‹å’Œã€ã€‘ä¸¤ç§æ ¼å¼ï¼‰
    # æ ¼å¼1: ã€ŠåŠ³åŠ¨åˆåŒæ³•ã€‹ç¬¬Xæ¡
    # æ ¼å¼2: ã€æ°‘æ³•å…¸ ç¬¬Xæ¡ã€‘
    law_name_pattern1 = r"[ã€Š]([^ã€‹]+)[ã€‹]"  # ä¹¦åå·æ ¼å¼
    law_name_pattern2 = r"[ã€]([^ç¬¬]+?)[\sç¬¬]"  # æ–¹æ‹¬å·æ ¼å¼ï¼ˆæå–åˆ°"ç¬¬"å­—ä¹‹å‰ï¼‰
    law_name_pattern3 = r"(æ°‘æ³•å…¸|åŠ³åŠ¨åˆåŒæ³•|åŠ³åŠ¨æ³•|åˆåŒæ³•|ç‰©æƒæ³•|æ¶ˆè´¹è€…æƒç›Šä¿æŠ¤æ³•|ç¤¾ä¼šä¿é™©æ³•|ä¸ªäººä¿¡æ¯ä¿æŠ¤æ³•)"  # ç›´æ¥åŒ¹é…å¸¸è§æ³•å¾‹å
    
    law_names_found = re.findall(law_name_pattern1, law_reference)
    law_names_found.extend(re.findall(law_name_pattern2, law_reference))
    law_names_found.extend(re.findall(law_name_pattern3, law_reference))
    # å»é‡
    law_names_found = list(set([name.strip() for name in law_names_found if name.strip()]))
    
    # æå–æ¡æ¬¾å·
    article_pattern = r"ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒé›¶\d]+)æ¡"
    articles_found = re.findall(article_pattern, law_reference)
    
    # ========== å±‚çº§1ï¼šæ£€æŸ¥æ˜¯å¦åœ¨ reference_info ä¸­ ==========
    if reference_info:
        for law_name in law_names_found:
            if law_name in reference_info:
                return (True, 1, f"åœ¨ reference_info ä¸­æ‰¾åˆ°: {law_name}")
    
    # ========== å±‚çº§2ï¼šæ£€æŸ¥æ³•å¾‹åç§°æ˜¯å¦åœ¨ç™½åå•/æ•°æ®åº“ä¸­ ==========
    valid_laws = [
        # åŠ³åŠ¨æ³•ç›¸å…³
        "åŠ³åŠ¨åˆåŒæ³•", "åŠ³åŠ¨æ³•", "ç¤¾ä¼šä¿é™©æ³•", "å·¥ä¼¤ä¿é™©æ¡ä¾‹", 
        "åŠ³åŠ¨äº‰è®®è°ƒè§£ä»²è£æ³•", "å°±ä¸šä¿ƒè¿›æ³•", "èŒä¸šç—…é˜²æ²»æ³•",
        "å·¥èµ„æ”¯ä»˜æš‚è¡Œè§„å®š", "å¸¦è–ªå¹´ä¼‘å‡æ¡ä¾‹", "æœ€ä½å·¥èµ„è§„å®š",
        "å¥³èŒå·¥åŠ³åŠ¨ä¿æŠ¤ç‰¹åˆ«è§„å®š", "åŠ³åŠ¨ä¿éšœç›‘å¯Ÿæ¡ä¾‹",
        # æ°‘äº‹æ³•ç›¸å…³
        "æ°‘æ³•å…¸", "åˆåŒæ³•", "ç‰©æƒæ³•", "æ‹…ä¿æ³•", "ä¾µæƒè´£ä»»æ³•",
        # æ¶ˆè´¹è€…ä¿æŠ¤ç›¸å…³
        "æ¶ˆè´¹è€…æƒç›Šä¿æŠ¤æ³•", "äº§å“è´¨é‡æ³•", "ç”µå­å•†åŠ¡æ³•",
        # å…¶ä»–
        "ä¸ªäººä¿¡æ¯ä¿æŠ¤æ³•", "æ•°æ®å®‰å…¨æ³•", "åä¸æ­£å½“ç«äº‰æ³•",
        "å…¬å¸æ³•", "ä¿é™©æ³•", "æ‹›æ ‡æŠ•æ ‡æ³•", "æ”¿åºœé‡‡è´­æ³•",
    ]
    
    law_name_valid = False
    for law_name in law_names_found:
        # å»é™¤"ä¸­åäººæ°‘å…±å’Œå›½"å‰ç¼€
        clean_name = law_name.replace("ä¸­åäººæ°‘å…±å’Œå›½", "")
        if clean_name in valid_laws or any(v in clean_name for v in valid_laws):
            law_name_valid = True
            break
    
    if law_name_valid:
        # ========== å±‚çº§3ï¼šæ£€æŸ¥æ¡æ¬¾å·æ ¼å¼ ==========
        if articles_found:
            # éªŒè¯æ¡æ¬¾å·æ˜¯å¦åˆç†ï¼ˆ1-999æ¡ï¼‰
            for article in articles_found:
                try:
                    # è½¬æ¢ä¸­æ–‡æ•°å­—
                    cn_to_num = {
                        "ä¸€": 1, "äºŒ": 2, "ä¸‰": 3, "å››": 4, "äº”": 5,
                        "å…­": 6, "ä¸ƒ": 7, "å…«": 8, "ä¹": 9, "å": 10,
                        "åä¸€": 11, "åäºŒ": 12, "åä¸‰": 13, "åå››": 14, "åäº”": 15,
                        "äºŒå": 20, "ä¸‰å": 30, "å››å": 40, "äº”å": 50,
                        "å…­å": 60, "ä¸ƒå": 70, "å…«å": 80, "ä¹å": 90,
                        "ä¸€ç™¾": 100
                    }
                    if article in cn_to_num:
                        article_num = cn_to_num[article]
                    elif article.isdigit():
                        article_num = int(article)
                    else:
                        # å¤æ‚ä¸­æ–‡æ•°å­—ï¼ˆå¦‚"äºŒåä¸‰"ï¼‰æš‚æ—¶è·³è¿‡éªŒè¯
                        return (True, 3, f"æ¡æ¬¾å·æ ¼å¼æ­£ç¡®: ç¬¬{article}æ¡")
                    
                    if 1 <= article_num <= 999:
                        return (True, 3, f"æ¡æ¬¾å·æ ¼å¼æ­£ç¡®: ç¬¬{article}æ¡")
                except:
                    pass
            
            return (True, 3, "æ¡æ¬¾å·æ ¼å¼æ­£ç¡®ï¼ˆæœªéªŒè¯æ•°å€¼ï¼‰")
        else:
            return (True, 2, f"æ³•å¾‹åç§°æœ‰æ•ˆ: {law_names_found}")
    
    # ========== å±‚çº§4ï¼šå†…å®¹éªŒè¯ï¼ˆéœ€è¦æ³•å¾‹æ•°æ®åº“ï¼‰==========
    # æš‚æœªå®ç°ï¼Œé¢„ç•™æ¥å£
    
    # éªŒè¯å¤±è´¥
    if law_names_found:
        return (False, 0, f"æœªçŸ¥æ³•å¾‹åç§°: {law_names_found}")
    else:
        return (False, 0, f"æ— æ³•è¯†åˆ«æ³•å¾‹å¼•ç”¨: {law_reference[:50]}...")


# ============================================================================
# è¯„æµ‹æ‰§è¡Œå™¨
# ============================================================================

class AblationBenchmark:
    """æ¶ˆèå®éªŒè¯„æµ‹å™¨"""
    
    def __init__(self, mode: int, source: str = "local"):
        self.mode = mode
        self.config = load_config()
        
        # è¯»å–æ¶ˆèå®éªŒé…ç½®ï¼ˆå¤ç”¨æ··åˆæ£€ç´¢å’Œ Reranker çš„å…±äº«å‚æ•°ï¼‰
        ablation_cfg = self.config.get("ablation_benchmark_config", {})
        hallucination_cfg = ablation_cfg.get("hallucination_detection", {})
        reranker_cfg = self.config.get("reranker_config", {})
        hybrid_cfg = self.config.get("hybrid_search_config", {})
        
        # å¹»è§‰æ£€æµ‹é˜ˆå€¼ï¼šå¤ç”¨ reranker_config.threshold
        self.hallucination_threshold = reranker_cfg.get("threshold", 0.3)
        # å¬å› Top-Kï¼šå¤ç”¨ hybrid_search_config.top_k
        self.recall_top_k = hybrid_cfg.get("top_k", 3)
        # æ˜¯å¦ä½¿ç”¨ä¸¤é˜¶æ®µæ£€æµ‹
        self.use_two_stage = hallucination_cfg.get("use_two_stage", True)
        
        # ===== æ¡ä»¶æ€§è‡ªåæ€é˜ˆå€¼ï¼ˆä»é…ç½®è¯»å–ï¼‰=====
        # >= reflection_high_threshold: é«˜ç½®ä¿¡åº¦ï¼Œè·³è¿‡åæ€ï¼ˆè§„åˆ™åŒ¹é…å¯é ï¼‰
        # reflection_low_threshold ~ reflection_high_threshold: ä¸­ç­‰ç½®ä¿¡åº¦ï¼Œè§¦å‘åæ€å¹¶ä¼ é€’è§„åˆ™ä¸Šä¸‹æ–‡
        # < reflection_low_threshold: ä½ç½®ä¿¡åº¦ï¼Œè§„åˆ™å·²è¢«è¿‡æ»¤ï¼Œè§¦å‘åæ€éªŒè¯
        self.reflection_high_confidence_threshold = reranker_cfg.get("reflection_high_threshold", 0.7)
        self.reflection_low_confidence_threshold = reranker_cfg.get("reflection_low_threshold", 0.5)
        
        # åˆå§‹åŒ– LLM (ä½¿ç”¨ OllamaClient æ›¿ä»£ ChatOllama)
        if source == "local":
            llm_cfg = self.config.get("llm_config", {})
        else:
            llm_cfg = self.config.get("llm_cloud_config", {})
        
        self.llm = OllamaClient(
            base_url=llm_cfg.get("base_url", "http://localhost:11434"),
            model=llm_cfg.get("model_name", "qwen3:4b-instruct"),
            temperature=0,
        )
        
        # æ¨¡å¼3å’Œ4éœ€è¦è§„åˆ™å¼•æ“
        self.rule_engine = None
        if mode in [EvalMode.CURRENT_WORKFLOW, EvalMode.OPTIMIZED_WORKFLOW]:
            from src.core.rule_engine import RuleEngine
            self.rule_engine = RuleEngine()
        
        # åˆå§‹åŒ– Embedding æ¨¡å‹ï¼ˆç”¨äºè¯­ä¹‰ç›¸ä¼¼åº¦è¯„ä¼°ï¼Œæ›¿ä»£ LLM-as-a-Judgeï¼‰
        self.embedding_model = None
        try:
            from sentence_transformers import SentenceTransformer
            embedding_cfg = self.config.get("embedding_config", {})
            model_path = embedding_cfg.get("model_path", "BAAI/bge-small-zh-v1.5")
            self.embedding_model = SentenceTransformer(model_path)
            print(f"âœ… Embedding æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
        except Exception as e:
            print(f"âš ï¸ Embedding æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä»…ä½¿ç”¨å…³é”®è¯åŒ¹é…: {e}")
        
        # åˆå§‹åŒ– Reranker æ¨¡å‹ï¼ˆç”¨äºä¸¤é˜¶æ®µå¹»è§‰æ£€æµ‹ï¼‰
        self.reranker = None
        if self.use_two_stage:
            try:
                from src.core.reranker import get_reranker
                self.reranker = get_reranker()
                print(f"âœ… Reranker æ¨¡å‹åŠ è½½æˆåŠŸ (å¹»è§‰æ£€æµ‹é˜ˆå€¼: {self.hallucination_threshold})")
            except Exception as e:
                print(f"âš ï¸ Reranker æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä»…ä½¿ç”¨ Embedding ç›¸ä¼¼åº¦: {e}")
    
    def get_prompt(self, clause: str, reference_info: str = "") -> str:
        """æ ¹æ®æ¨¡å¼è·å–Prompt"""
        if self.mode == EvalMode.RAW_LLM:
            return RAW_LLM_PROMPT.format(clause=clause)
        elif self.mode == EvalMode.BASIC_PROMPT:
            return BASIC_PROMPT.format(clause=clause)
        elif self.mode == EvalMode.CURRENT_WORKFLOW:
            return CURRENT_WORKFLOW_PROMPT.format(clause=clause, reference_info=reference_info or "æ— ")
        elif self.mode == EvalMode.OPTIMIZED_WORKFLOW:
            return OPTIMIZED_WORKFLOW_PROMPT.format(clause=clause, reference_info=reference_info or "æ— ")
        else:
            return RAW_LLM_PROMPT.format(clause=clause)
    
    def get_reference_info(self, clause: str) -> tuple:
        """è·å–è§„åˆ™å¼•æ“çš„å‚è€ƒä¿¡æ¯ï¼ˆä½¿ç”¨ç»Ÿä¸€æ£€ç´¢å™¨ï¼‰
        
        Returns:
            tuple: (reference_info, law_contents, risk_ids, scores)
        """
        if self.rule_engine is None:
            return "", [], [], [], 0.0
        
        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„æ£€ç´¢å™¨æ¨¡å—
            from src.core.reference_retriever import retrieve_reference
            result = retrieve_reference(clause)
            # è¿”å› pre_filter_max_score ç”¨äºè°ƒè¯•ï¼ˆå½“ç»“æœè¢«è¿‡æ»¤æ—¶æ˜¾ç¤ºåŸå§‹æœ€é«˜åˆ†ï¼‰
            pre_filter_max = result.pre_filter_max_score if hasattr(result, 'pre_filter_max_score') else 0.0
            return result.reference_info, result.law_contents, result.risk_ids, result.scores, pre_filter_max
        except Exception as e:
            print(f"Reference retrieval error: {e}")
        
        return "", [], [], [], 0.0
    
    async def analyze_clause(self, clause: str) -> tuple:
        """åˆ†æå•ä¸ªæ¡æ¬¾
        
        Returns:
            tuple: (ParsedResult, reference_info, risk_ids, scores, reflection_info)
        """
        # è·å–å‚è€ƒä¿¡æ¯ï¼ˆæ¨¡å¼3å’Œ4ä½¿ç”¨ Top-K æ£€ç´¢ï¼‰
        reference_info, law_contents, risk_ids, scores, pre_filter_max_score = self.get_reference_info(clause)
        
        # æ„å»ºPrompt
        prompt = self.get_prompt(clause, reference_info)
        
        reflection_info = None  # è‡ªåæ€ç»“æœ
        
        try:
            # ä½¿ç”¨ OllamaClient çš„ achat æ–¹æ³•
            content = await self.llm.achat(prompt)
            
            # è§£æè¾“å‡º
            result = parse_markdown_output(content)
            
            # æ³¨å…¥æ³•æ¡å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
            if law_contents and result.law_reference == "":
                result.law_reference = law_contents[0] if law_contents[0] else ""
            
            # ========== æ¡ä»¶æ€§è‡ªåæ€æœºåˆ¶ï¼ˆåŸºäº Rerank ç½®ä¿¡åº¦ + é£é™©ç­‰çº§ï¼‰==========
            # é«˜ç½®ä¿¡åº¦(>=0.7): è·³è¿‡åæ€ï¼Œä¿¡ä»»è§„åˆ™åŒ¹é…ç»“æœï¼ˆä»…é™ä¸­/é«˜é£é™©ï¼‰
            # ä¸­ç­‰ç½®ä¿¡åº¦(0.5-0.7): è§¦å‘åæ€ï¼ˆä»…é™ä¸­/é«˜é£é™©ï¼‰ï¼Œä¼ é€’è§„åˆ™ä¸Šä¸‹æ–‡
            # ä½ç½®ä¿¡åº¦(<0.5): è§¦å‘åæ€ï¼ˆæ— è®ºé£é™©ç­‰çº§ï¼‰ï¼ŒéªŒè¯ LLM æ˜¯å¦æ­£ç¡®éµå¾ª"ç©ºä¸Šä¸‹æ–‡â†’ä½é£é™©"è§„åˆ™
            # ä½¿ç”¨ pre_filter_max_scoreï¼ˆå¦‚æœæœ‰ï¼‰ä»£æ›¿0ï¼Œç”¨äºè°ƒè¯•è¾“å‡º
            max_score = max(scores) if scores else pre_filter_max_score
            
            if self.mode in [EvalMode.CURRENT_WORKFLOW, EvalMode.OPTIMIZED_WORKFLOW]:
                should_reflect = False
                reflection_context = None
                
                if max_score >= self.reflection_high_confidence_threshold:
                    # é«˜ç½®ä¿¡åº¦ï¼šè§„åˆ™åŒ¹é…å¯é 
                    if result.risk_level in ["é«˜", "ä¸­"]:
                        # ä¸­/é«˜é£é™© + é«˜ç½®ä¿¡åº¦ = è·³è¿‡åæ€
                        should_reflect = False
                        reflection_info = {"skipped": True, "skip_reason": "high_confidence", "max_score": max_score}
                        print(f"  â­ï¸ è·³è¿‡åæ€ (é«˜ç½®ä¿¡åº¦: {max_score:.2f} >= 0.7)")
                    # ä½é£é™©æ— éœ€åæ€
                    
                elif max_score >= self.reflection_low_confidence_threshold:
                    # ä¸­ç­‰ç½®ä¿¡åº¦(0.5-0.7)
                    if result.risk_level in ["é«˜", "ä¸­"]:
                        # ä¸­/é«˜é£é™© + ä¸­ç­‰ç½®ä¿¡åº¦ = è§¦å‘åæ€
                        should_reflect = True
                        reflection_context = {
                            "matched_rules": list(zip(risk_ids, scores)) if risk_ids else [],
                            "confidence_level": "medium",
                            "max_score": max_score,
                        }
                        print(f"  ğŸ”„ è§¦å‘åæ€ (ä¸­ç­‰ç½®ä¿¡åº¦: {max_score:.2f})")
                    # ä½é£é™©æ— éœ€åæ€
                    
                else:
                    # ä½ç½®ä¿¡åº¦(<0.5): æ— è®ºé£é™©ç­‰çº§éƒ½è§¦å‘åæ€
                    # å› ä¸ºæ­¤æ—¶ LLM æ”¶åˆ°çš„å‚è€ƒä¿¡æ¯æ˜¯"æ— "ï¼Œåº”è¯¥è¾“å‡ºä½é£é™©
                    # å¦‚æœè¾“å‡ºäº†ä¸­/é«˜é£é™©ï¼Œéœ€è¦éªŒè¯æ˜¯å¦çœŸçš„è§¦å‘äº†"æç«¯è¿æ³•"ä¾‹å¤–
                    should_reflect = True
                    reflection_context = {
                        "matched_rules": [],  # æ— è§„åˆ™
                        "confidence_level": "low",
                        "max_score": max_score,
                        "empty_reference": True,  # æ ‡è®°ä¸ºç©ºå‚è€ƒä¿¡æ¯
                    }
                    print(f"  ğŸ”„ è§¦å‘åæ€ (ä½ç½®ä¿¡åº¦/ç©ºè§„åˆ™: {max_score:.2f} < 0.5, é£é™©={result.risk_level})")
                
                if should_reflect:
                    result, reflection_info = await self._apply_self_reflection(clause, result, reflection_context)
            
            return result, reference_info, risk_ids, scores, reflection_info
            
        except Exception as e:
            print(f"LLM error: {e}")
            return ParsedResult(), reference_info, risk_ids, scores, None
    
    async def _apply_self_reflection(self, clause: str, initial_result: ParsedResult, reflection_context: Dict = None) -> tuple:
        """åº”ç”¨è‡ªåæ€æœºåˆ¶
        
        Args:
            clause: æ¡æ¬¾åŸæ–‡
            initial_result: åˆæ¬¡åˆ†æç»“æœ
            reflection_context: è§„åˆ™ç½®ä¿¡åº¦ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
                - matched_rules: [(risk_id, score), ...]
                - confidence_level: "high" / "medium" / "low"
                - max_score: æœ€é«˜ç½®ä¿¡åº¦åˆ†æ•°
        
        Returns:
            tuple: (è°ƒæ•´åçš„ ParsedResult, reflection_info dict)
        """
        reflection_info = {
            "applied": True,
            "initial_level": initial_result.risk_level,
            "final_level": initial_result.risk_level,
            "adjusted": False,
            "reason": "",
            "confidence_context": reflection_context  # è®°å½•è§¦å‘åæ€æ—¶çš„ç½®ä¿¡åº¦ä¸Šä¸‹æ–‡
        }
        
        # æ ¹æ®ç½®ä¿¡åº¦ç¡®å®šç³»ç»Ÿä¿¡å·ï¼ˆä¸‰çº§ï¼šç¼ºå¤±/å­˜ç–‘/å……è¶³ï¼‰
        max_score = reflection_context.get("max_score", 0) if reflection_context else 0
        
        if max_score < 0.45:
            # è¯æ®ç¼ºå¤±
            system_signal = (
                "ã€è¯æ®çŠ¶æ€ï¼šç¼ºå¤±ã€‘\n"
                "æœªæ£€ç´¢åˆ°ä»»ä½•å¯ç”¨è§„åˆ™è¯æ®ã€‚\n"
                "ä»…å…è®¸åœ¨å‘½ä¸­æ˜ç¡®æ³•å¾‹ç¡¬ä¼¤æ—¶è¾“å‡ºé«˜é£é™©ï¼›\n"
                "è‹¥æœªå‘½ä¸­ç¡¬ä¼¤ï¼Œç¦æ­¢å‡çº§é£é™©ç­‰çº§ã€‚"
            )
            reference_info_text = ""
            
        elif max_score < 0.7:
            # è¯æ®å­˜ç–‘ (0.45-0.7)
            system_signal = (
                "ã€è¯æ®çŠ¶æ€ï¼šå­˜ç–‘ã€‘\n"
                "æ£€ç´¢åˆ°çš„è§„åˆ™ç›¸å…³æ€§è¾ƒå¼±ã€‚\n"
                "å…è®¸å¯¹åˆå®¡ç»“è®ºè¿›è¡Œå»å™ªä¿®æ­£ï¼Œ\n"
                "ä½†ç¦æ­¢ä»…å› ä¸ç¡®å®šæ€§å‡çº§ä¸ºé«˜é£é™©ã€‚"
            )
            # æ„å»ºè§„åˆ™åˆ—è¡¨
            if reflection_context and reflection_context.get("matched_rules"):
                rules_lines = []
                for rid, score in reflection_context.get("matched_rules", []):
                    rules_lines.append(f"  - è§„åˆ™ID: {rid}, ç½®ä¿¡åº¦: {score:.2f}")
                reference_info_text = "\n".join(rules_lines)
            else:
                reference_info_text = "æ— "
                
        else:
            # è¯æ®å……è¶³ (>= 0.7)
            system_signal = (
                "ã€è¯æ®çŠ¶æ€ï¼šå……è¶³ã€‘\n"
                "å·²æ£€ç´¢åˆ°é«˜åŒ¹é…åº¦è§„åˆ™è¯æ®ã€‚\n"
                "é™¤éå‘ç°æ˜æ˜¾é€‚ç”¨é”™è¯¯ï¼Œ\n"
                "å¦åˆ™åº”ç»´æŒåˆå®¡é£é™©ç­‰çº§ã€‚"
            )
            # æ„å»ºè§„åˆ™åˆ—è¡¨
            if reflection_context and reflection_context.get("matched_rules"):
                rules_lines = []
                for rid, score in reflection_context.get("matched_rules", []):
                    rules_lines.append(f"  - è§„åˆ™ID: {rid}, ç½®ä¿¡åº¦: {score:.2f}")
                reference_info_text = "\n".join(rules_lines)
            else:
                reference_info_text = "æ— "
        
        # æ„å»ºè‡ªåæ€ Promptï¼ˆé€‚é…æ–°æ¨¡æ¿ï¼‰
        reflection_prompt = SELF_REFLECTION_PROMPT.format(
            system_signal=system_signal,
            clause_text=clause,
            initial_risk_level=initial_result.risk_level or "æœªçŸ¥",
            analysis=initial_result.analysis or "æ— ",
            reference_info=reference_info_text
        )
        
        try:
            # è°ƒç”¨ LLM è¿›è¡Œåæ€
            reflection_content = await self.llm.achat(reflection_prompt)
            
            # è§£æåæ€ç»“æœ
            reflection_result = parse_reflection_output(reflection_content)
            reflection_info["reason"] = reflection_result.get("reason", "")
            
            # è·å–äºŒå®¡æœ€ç»ˆç­‰çº§
            final_level = reflection_result.get("final_level")
            initial_level = initial_result.risk_level
            
            # åˆ¤æ–­æ˜¯å¦çœŸæ­£è°ƒçº§ï¼ˆç­‰çº§ä¸åŒä¸”æœ‰æ˜ç¡®çš„æœ€ç»ˆç­‰çº§ï¼‰
            if final_level and final_level != initial_level:
                reflection_info["final_level"] = final_level
                reflection_info["adjusted"] = True
                
                # æ›´æ–° ParsedResult çš„é£é™©ç­‰çº§
                initial_result.risk_level = final_level
                
                # åœ¨åˆ†æä¸­æ·»åŠ è°ƒçº§è¯´æ˜
                adjustment_note = f"\n[äºŒå®¡è°ƒçº§: {initial_level}â†’{final_level}ï¼Œç†ç”±: {reflection_info['reason']}]"
                initial_result.analysis = (initial_result.analysis or "") + adjustment_note
                
                print(f"  ğŸ”„ è‡ªåæ€è°ƒçº§: {initial_level} â†’ {final_level}")
            else:
                # ç»´æŒåŸåˆ¤ï¼ˆåŒ…æ‹¬ï¼šç»“è®ºä¸º"ç»´æŒ"ã€æˆ–æœ€ç»ˆç­‰çº§ä¸åˆå§‹ç­‰çº§ç›¸åŒï¼‰
                print(f"  âœ“ è‡ªåæ€ç»´æŒ: {initial_level}")
                
        except Exception as e:
            print(f"  âš ï¸ è‡ªåæ€å¤±è´¥: {e}")
            reflection_info["applied"] = False
        
        return initial_result, reflection_info
    
    def evaluate_reason(self, clause: str, gt_keywords: List[str], ai_reason: str) -> float:
        """ä½¿ç”¨ç®—æ³•+è¯­ä¹‰åŒ¹é…è¯„ä¼°è®ºè¯è´¨é‡ï¼ˆæ›¿ä»£ LLM-as-a-Judgeï¼‰
        
        è¯„åˆ†æ–¹æ³•ï¼š
        1. å…³é”®è¯åŒ¹é…ï¼ˆ65%æƒé‡ï¼‰ï¼šæ£€æŸ¥ AI ç†ç”±æ˜¯å¦åŒ…å« ground_truth å…³é”®è¯
        2. è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆ35%æƒé‡ï¼‰ï¼šä½¿ç”¨ Embedding è®¡ç®—å‘é‡ä½™å¼¦ç›¸ä¼¼åº¦
        
        Returns:
            float: 0.0-1.0 çš„åŒ¹é…åˆ†æ•°
        """
        if not ai_reason or not gt_keywords:
            return 0.0
        
        ai_reason_lower = ai_reason.lower()
        
        # 1. å…³é”®è¯ç²¾ç¡®åŒ¹é…
        matched_keywords = sum(1 for kw in gt_keywords if kw.lower() in ai_reason_lower)
        keyword_ratio = matched_keywords / len(gt_keywords) if gt_keywords else 0
        
        # 2. è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆä½¿ç”¨ Embeddingï¼‰
        semantic_sim = 0.0
        if self.embedding_model:
            try:
                from sklearn.metrics.pairwise import cosine_similarity
                gt_text = " ".join(gt_keywords)
                embeddings = self.embedding_model.encode([gt_text, ai_reason])
                semantic_sim = float(cosine_similarity([embeddings[0]], [embeddings[1]])[0][0])
            except Exception as e:
                print(f"è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
                semantic_sim = 0.0
        
        # 3. ç»¼åˆè¯„åˆ†ï¼ˆåŠ æƒæ±‚å’Œï¼‰
        alpha = 0.65  # å…³é”®è¯åŒ¹é…æƒé‡
        score = alpha * keyword_ratio + (1 - alpha) * semantic_sim
        
        return score
    
    async def evaluate_single(self, item: Dict[str, Any], metrics: EvalMetrics) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªæ ·æœ¬"""
        import time
        
        text = item.get("text", "")
        gt = item.get("ground_truth", {})
        original_data = item.get("original_data", {})  # LLM æ•°æ®é›†çš„åŸå§‹æ•°æ®
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # åˆ†ææ¡æ¬¾ï¼ˆè¿”å›å…ƒç»„ï¼šresult, reference_info, risk_ids, scores, reflection_infoï¼‰
        result, reference_info, matched_risk_ids, matched_scores, reflection_info = await self.analyze_clause(text)
        
        # è®°å½•å“åº”æ—¶é—´
        latency = time.time() - start_time
        metrics.total_latency += latency
        
        # è‡ªåæ€ç»Ÿè®¡ï¼ˆåŒ…æ‹¬è·³è¿‡å’Œè§¦å‘çš„æƒ…å†µï¼‰
        if reflection_info:
            if reflection_info.get("skipped"):
                # åæ€è¢«è·³è¿‡
                skip_reason = reflection_info.get("skip_reason", "")
                if skip_reason == "high_confidence":
                    metrics.reflection_skipped_high_conf += 1
            elif reflection_info.get("applied"):
                # åæ€è¢«è§¦å‘
                metrics.reflection_calls += 1
                
                # æ ¹æ®ç½®ä¿¡åº¦ä¸Šä¸‹æ–‡åŒºåˆ†è§¦å‘ç±»å‹
                conf_context = reflection_info.get("confidence_context", {})
                conf_level = conf_context.get("confidence_level", "medium") if conf_context else "medium"
                if conf_level == "low" or conf_context.get("empty_reference"):
                    metrics.reflection_triggered_low_conf += 1
                else:
                    metrics.reflection_triggered_medium_conf += 1
                
                initial_level = reflection_info.get("initial_level", "")
                final_level = reflection_info.get("final_level", initial_level)
                
                if reflection_info.get("adjusted"):
                    metrics.reflection_adjustments += 1
                    # è®°å½•è¯¦ç»†è°ƒçº§æ–¹å‘
                    transition_key = f"{initial_level}â†’{final_level}"
                    metrics.reflection_transitions[transition_key] = metrics.reflection_transitions.get(transition_key, 0) + 1
                else:
                    metrics.reflection_maintain += 1
                    # è®°å½•ç»´æŒåŸåˆ¤
                    maintain_key = f"{initial_level}â†’{initial_level}(ç»´æŒ)"
                    metrics.reflection_transitions[maintain_key] = metrics.reflection_transitions.get(maintain_key, 0) + 1
        
        metrics.total += 1
        
        # è§£ææˆåŠŸç‡
        if result.parse_success:
            metrics.parse_success += 1
        
        # é£é™©ç­‰çº§è¯„ä¼°ï¼ˆæ”¯æŒé«˜/ä¸­/ä½ä¸‰çº§ï¼‰
        gt_risk = gt.get("risk_level", "")
        pred_risk = result.risk_level
        
        # ç²¾ç¡®åŒ¹é…æˆ–å…¼å®¹åŒ¹é…
        is_risk_correct = (gt_risk == pred_risk) or (gt_risk in pred_risk)
        
        if is_risk_correct:
            metrics.correct_risk += 1
        
        # åŠ æƒè¯„åˆ†ï¼ˆç²¾ç¡®åŒ¹é…1åˆ†ï¼Œå·®ä¸€çº§0.5åˆ†ï¼Œå·®ä¸¤çº§0åˆ†ï¼‰
        weighted_score = EvalMetrics.calculate_weighted_score(gt_risk, pred_risk)
        metrics.total_weighted_score += weighted_score
        
        # æ··æ·†çŸ©é˜µæ›´æ–°ï¼ˆåŸºäº"æœ‰é£é™©"(é«˜/ä¸­) vs "æ— é£é™©"(ä½)çš„äºŒåˆ†ç±»ï¼‰
        # é«˜/ä¸­é£é™©è§†ä¸ºæ­£ä¾‹(Positive)ï¼Œä½é£é™©è§†ä¸ºè´Ÿä¾‹(Negative)
        gt_is_risky = gt_risk in ["é«˜", "ä¸­"]
        pred_is_risky = pred_risk in ["é«˜", "ä¸­"]
        
        if pred_is_risky and gt_is_risky:
            metrics.true_positive += 1
        elif pred_is_risky and not gt_is_risky:
            metrics.false_positive += 1
        elif not pred_is_risky and gt_is_risky:
            metrics.false_negative += 1
        else:
            metrics.true_negative += 1
            
        # [æ–°å¢] æ›´æ–°ä¸‰åˆ†ç±»æ··æ·†çŸ©é˜µ
        metrics.update_confusion_matrix(gt_risk, pred_risk)
        
        # risk_id åŒ¹é…è¯„ä¼°ï¼ˆPrecision/Recall/F1ï¼‰
        # æ¯”è¾ƒç³»ç»Ÿæ£€ç´¢åˆ°çš„ risk_ids ä¸ Ground Truth ä¸­çš„ expected_risks
        expected_risks = original_data.get("expected_risks", [])
        # expected_risks å¯èƒ½æ˜¯ [{"risk_id": "LABOR_001", ...}] æˆ– ["LABOR_001", ...]
        if expected_risks and isinstance(expected_risks[0], dict):
            expected_set = set(r.get("risk_id", "") for r in expected_risks if r.get("risk_id"))
        else:
            expected_set = set(expected_risks) if expected_risks else set()
        matched_set = set(matched_risk_ids) if matched_risk_ids else set()
        
        # åªåœ¨æœ‰ GT æˆ–æœ‰é¢„æµ‹çš„æƒ…å†µä¸‹è®¡ç®—
        if expected_set or matched_set:
            # è®¡ç®—äº¤é›†
            intersection = expected_set & matched_set
            correct_count = len(intersection)
            
            # Precision (æŸ¥å‡†ç‡) - åˆ†æ¯æ˜¯"æ¨¡å‹é¢„æµ‹æ•°"
            if len(matched_set) > 0:
                risk_id_precision = correct_count / len(matched_set)
            else:
                risk_id_precision = 1.0 if len(expected_set) == 0 else 0.0
            
            # Recall (æŸ¥å…¨ç‡) - åˆ†æ¯æ˜¯"çœŸå®æ ‡ç­¾æ•°"
            if len(expected_set) > 0:
                risk_id_recall = correct_count / len(expected_set)
            else:
                risk_id_recall = 1.0 if len(matched_set) == 0 else 0.0
            
            # F1 Score
            if (risk_id_precision + risk_id_recall) > 0:
                risk_id_f1 = 2 * (risk_id_precision * risk_id_recall) / (risk_id_precision + risk_id_recall)
            else:
                risk_id_f1 = 0.0
            
            # ç´¯åŠ åˆ° metrics
            metrics.risk_id_precision_sum += risk_id_precision
            metrics.risk_id_recall_sum += risk_id_recall
            metrics.risk_id_f1_sum += risk_id_f1
            metrics.risk_id_count += 1
        
        # ===== æ–¹æ³•ä¸€ï¼šè¯æ®ä¸€è‡´æ€§è¯„ä¼°ï¼ˆä¸¤é˜¶æ®µï¼šBGE-M3 + Rerankerï¼‰=====
        # åˆåŒæ¡æ¬¾è¯æ®éªŒè¯
        evidence_result = verify_evidence(
            result.evidence, text, 
            embedding_model=self.embedding_model,
            reranker=self.reranker,
            threshold=self.hallucination_threshold
        )
        clause_evidence_valid = evidence_result[0]  # is_valid
        evidence_similarity = evidence_result[1]    # similarity_score
        evidence_match_type = evidence_result[2]    # match_type
        
        if clause_evidence_valid:
            metrics.clause_evidence_valid += 1
            metrics.evidence_valid += 1  # ä¿æŒå…¼å®¹
        else:
            metrics.clause_evidence_invalid += 1
            metrics.evidence_invalid += 1  # ä¿æŒå…¼å®¹
        
        # 1b. æ³•å¾‹å¼•ç”¨éªŒè¯ï¼ˆåˆ†çº§éªŒè¯ï¼šreference_info â†’ ç™½åå• â†’ æ¡æ¬¾å·ï¼‰
        # æ³¨æ„ï¼šä½é£é™©æ ·æœ¬ä¸éœ€è¦æ³•å¾‹å¼•ç”¨ï¼Œè·³è¿‡éªŒè¯
        if gt_risk == "ä½":
            # ä½é£é™©æ ·æœ¬æ³•å¾‹å¼•ç”¨é»˜è®¤æœ‰æ•ˆ
            law_citation_valid = True
            law_validation_level = 0
            law_validation_detail = "ä½é£é™©æ ·æœ¬æ— éœ€æ³•å¾‹å¼•ç”¨"
            metrics.law_citation_valid += 1
        else:
            # é«˜/ä¸­é£é™©æ ·æœ¬éœ€è¦éªŒè¯æ³•å¾‹å¼•ç”¨
            law_result = verify_law_citation(result.law_reference, reference_info=reference_info)
            law_citation_valid = law_result[0]       # is_valid
            law_validation_level = law_result[1]     # validation_level
            law_validation_detail = law_result[2]    # detail
            
            if law_citation_valid:
                metrics.law_citation_valid += 1
            else:
                metrics.law_citation_invalid += 1
        
        # ===== æ–¹æ³•äºŒï¼šè§„åˆ™è§¦å‘ä¸€è‡´æ€§ï¼ˆRAG æ£€ç´¢å™¨è¯„æµ‹ï¼‰=====
        # ä»æµ‹è¯•ç”¨ä¾‹è·å–åº”è§¦å‘çš„è§„åˆ™ ID
        expected_rule_ids = set()
        if expected_risks:
            for risk in expected_risks:
                if isinstance(risk, dict) and risk.get("risk_id"):
                    expected_rule_ids.add(risk.get("risk_id"))
        
        # å®é™…è§¦å‘çš„è§„åˆ™ IDï¼ˆæ¥è‡ªæ£€ç´¢å™¨ ReferenceResult.risk_idsï¼‰
        triggered_rule_ids = set(matched_risk_ids) if matched_risk_ids else set()
        
        # è®¡ç®—æŒ‡æ ‡ï¼ˆç²¾ç¡®å­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if expected_rule_ids:
            metrics.rule_target_count += len(expected_rule_ids)          # åº”è§¦å‘
            metrics.rule_trigger_count += len(triggered_rule_ids)        # å®é™…è§¦å‘
            correct_triggers = expected_rule_ids & triggered_rule_ids    # äº¤é›†
            metrics.rule_correct_count += len(correct_triggers)          # æ­£ç¡®è§¦å‘

        
        # ===== æ–¹æ³•ä¸‰ï¼šä»»åŠ¡æˆåŠŸç‡è¯„ä¼° (ä¼˜åŒ–ç‰ˆï¼šå®‰å…¨åˆè§„å¯¼å‘) =====
        # ä¿®æ”¹æ€è·¯ï¼š
        # 1. ç§»é™¤ clause_evidence_validï¼šMode 4 åœ¨ RAG å¤±è´¥æ—¶ä¼šç”¨é€»è¾‘æ¨ç†è¡¥å…¨ï¼Œè¿™ä¸åº”è¢«åˆ¤ä¸ºä»»åŠ¡å¤±è´¥ã€‚
        # 2. æ”¾å®½é£é™©åˆ¤å®šï¼šä¸å†å•çº¯çœ‹"Â±1çº§"ï¼Œè€Œæ˜¯çœ‹"æ˜¯å¦æ¼æ‰äº†é«˜é£é™©"ã€‚
        
        # A. åŸºç¡€é—¨æ§›ï¼šè§£ææˆåŠŸ + æœ‰å»ºè®®
        # æ³¨æ„ï¼šå¯¹äºä½é£é™©é¢„æµ‹ï¼ŒPrompt å…è®¸è¾“å‡º"æ— "ä½œä¸ºå»ºè®®ï¼Œè¿™æ˜¯åˆç†çš„ã€‚
        # å› æ­¤åªè¦æœ‰ä»»æ„éç©ºè¾“å‡ºï¼ˆåŒ…æ‹¬"æ— "ï¼‰éƒ½ç®—æœ‰å»ºè®®ã€‚
        has_suggestion_or_na = bool(result.suggestion) and result.suggestion.strip() != ""
        
        # B. é£é™©åˆè§„æ€§åˆ¤å®š (æ ¸å¿ƒä¿®æ”¹)
        # é€»è¾‘ï¼šåªè¦ä¸æ˜¯"è‡´å‘½æ¼åˆ¤ (High -> Low)"ï¼Œéƒ½ç®—ç³»ç»Ÿ"æˆåŠŸè¿ä½œ"ã€‚
        # è¿™æ„å‘³ç€ï¼š
        # - Exact Match (High->High): æˆåŠŸ
        # - Over-flagging (Medium->High, Low->High): æˆåŠŸ (é˜²å¾¡æ€§é¢„è­¦)
        # - Adjacent Miss (High->Medium): æˆåŠŸ (è™½ç„¶é™çº§ï¼Œä½†æœªå®Œå…¨æ”¾è¿‡ï¼Œåœ¨å®¹å¿èŒƒå›´å†…)
        # - Critical Miss (High->Low): å¤±è´¥ (è¿™æ˜¯å”¯ä¸€ä¸å¯æ¥å—çš„)
        
        is_safety_success = True
        if gt_risk == "é«˜" and pred_risk == "ä½":
            is_safety_success = False  # åªæœ‰è¿™ç§æƒ…å†µåˆ¤ä¸º"ä»»åŠ¡å¤±è´¥"
            
        # C. æœ€ç»ˆåˆ¤å®š (ç§»é™¤äº† clause_evidence_valid)
        # æ³¨æ„ï¼šè¿™é‡Œä¸å†è¦æ±‚ evidence_validã€‚åªè¦æ¨¡å‹ç»™å‡ºäº†å“åº”ï¼ˆhas_suggestion_or_naï¼‰ï¼Œ
        # ä¸”æ²¡æœ‰çŠ¯è‡´å‘½é”™è¯¯ï¼ˆis_safety_successï¼‰ï¼Œå°±ç®—ä»»åŠ¡æˆåŠŸã€‚
        if result.parse_success and is_safety_success and has_suggestion_or_na:
            metrics.task_success_count += 1
        
        # è®ºè¯è´¨é‡è¯„ä¼°ï¼ˆå¦‚æœæœ‰ reason_keywordsï¼‰
        reason_keywords = gt.get("reason_keywords", [])
        if reason_keywords:
            reason_score = self.evaluate_reason(text, reason_keywords, result.analysis)
            if reason_score >= 0.5:  # ä½¿ç”¨é˜ˆå€¼åˆ¤æ–­åŒ¹é…æˆåŠŸ
                metrics.correct_reason += 1
        
        return {
            "id": item.get("id"),
            "prediction": {
                "risk_level": result.risk_level,
                "evidence": result.evidence,
                "analysis": result.analysis,
                "parse_success": result.parse_success,
                "latency": round(latency, 3),
            },
            "ground_truth": gt,
            "correct_risk": is_risk_correct,
            "weighted_score": weighted_score,
            # è¯æ®éªŒè¯è¯¦æƒ…
            "evidence_valid": clause_evidence_valid,
            "evidence_similarity": round(evidence_similarity, 3),
            "evidence_match_type": evidence_match_type,
            # æ³•æ¡éªŒè¯è¯¦æƒ…
            "law_citation_valid": law_citation_valid,
            "law_validation_level": law_validation_level,
            "law_validation_detail": law_validation_detail,
            # ä»»åŠ¡æˆåŠŸ
            "task_success": result.parse_success and is_safety_success and has_suggestion_or_na,
        }


def convert_llm_dataset_item(item: Dict[str, Any]) -> Dict[str, Any]:
    """
    å°† LLM ç”Ÿæˆçš„æ•°æ®é›†æ ¼å¼è½¬æ¢ä¸º benchmark æœŸæœ›çš„æ ¼å¼
    
    æ–°æ ¼å¼ (399æ ·æœ¬):
    {
        "id": "LABOR_001_high_positive_1",
        "contract_text": "...",
        "expected_risks": [{"risk_id": "...", "risk_level": "high", ...}],
        "case_type": "high_positive/medium_positive/negative",
        "source_domain": "LABOR"
    }
    
    æ—§æ ¼å¼ (å…¼å®¹):
    {
        "id": "GENERAL_001_pos_1",
        "contract_text": "...",
        "expected_risks": [...],
        "case_type": "positive/negative/boundary"
    }
    
    Benchmark æ ¼å¼:
    {
        "id": "...",
        "text": "...",
        "ground_truth": {"risk_level": "é«˜/ä¸­/ä½", "reason_keywords": [...]},
        "original_data": {...}  # ä¿ç•™åŸå§‹æ•°æ®ç”¨äº risk_id åŒ¹é…
    }
    """
    # æ£€æµ‹æ˜¯å¦ä¸º LLM æ•°æ®é›†æ ¼å¼
    if "contract_text" in item:
        expected_risks = item.get("expected_risks", [])
        case_type = item.get("case_type", "")
        
        # æ ¹æ® case_type ç¡®å®šé£é™©ç­‰çº§
        if case_type == "high_positive":
            risk_level = "é«˜"
        elif case_type == "medium_positive":
            risk_level = "ä¸­"
        else:  # negative æˆ–å…¶ä»–
            risk_level = "ä½"
        
        # æå–å…³é”®è¯ä½œä¸º reason_keywords
        reason_keywords = []
        for risk in expected_risks:
            if isinstance(risk, dict) and risk.get("risk_name"):
                reason_keywords.append(risk["risk_name"])
        
        return {
            "id": item.get("id", ""),
            "text": item.get("contract_text", ""),
            "ground_truth": {
                "risk_level": risk_level,
                "reason_keywords": reason_keywords,
            },
            "original_data": item,  # ä¿ç•™åŸå§‹æ•°æ®ç”¨äº risk_id åŒ¹é…
            "source_domain": item.get("source_domain", ""),  # ä¿ç•™ source_domain
        }
    
    # å·²ç»æ˜¯æ—§æ ¼å¼ï¼Œç›´æ¥è¿”å›
    return item



async def run_ablation_benchmark(
    data_path: str = None,
    mode: int = 2,
    limit: int = None,
    source: str = "local",
    log_callback=None,
    dataset: List[Dict[str, Any]] = None  # æ”¯æŒä¼ å…¥é¢„åŠ è½½çš„æ•°æ®é›†
) -> Dict[str, Any]:
    """
    è¿è¡Œæ¶ˆèå®éªŒè¯„æµ‹
    
    Args:
        data_path: æ•°æ®é›†è·¯å¾„ (å½“ dataset ä¸º None æ—¶å¿…é¡»æä¾›)
        mode: è¯„æµ‹æ¨¡å¼ (1-4)
        limit: æ ·æœ¬æ•°é‡é™åˆ¶ (ä»…å½“ dataset ä¸º None æ—¶ç”Ÿæ•ˆ)
        source: LLM æ¥æº (local/cloud)
        log_callback: æ—¥å¿—å›è°ƒå‡½æ•°
        dataset: é¢„åŠ è½½çš„æ•°æ®é›† (ç”¨äºæ¶ˆèå®éªŒä¸­æ§åˆ¶å˜é‡)
    """
    
    def log(msg):
        print(msg)
        if log_callback:
            log_callback(msg)
    
    log(f"\n{'='*60}")
    log(f"ğŸ§ª æ¶ˆèå®éªŒè¯„æµ‹ - æ¨¡å¼ {mode}: {EvalMode.name(mode)}")
    log(f"{'='*60}")
    
    # å¦‚æœæ²¡æœ‰ä¼ å…¥æ•°æ®é›†ï¼Œåˆ™ä»æ–‡ä»¶åŠ è½½
    if dataset is None:
        if not data_path or not os.path.exists(data_path):
            log(f"Error: Data file not found at {data_path}")
            return None
        
        dataset = []
        with open(data_path, "r", encoding="utf-8") as f:
            content = f.read().strip()
            
            # æ£€æµ‹æ–‡ä»¶æ ¼å¼ï¼šJSON æ•°ç»„ vs JSONL
            if content.startswith("["):
                # JSON æ•°ç»„æ ¼å¼
                raw_items = json.loads(content)
                for raw_item in raw_items:
                    converted_item = convert_llm_dataset_item(raw_item)
                    dataset.append(converted_item)
            else:
                # JSONL æ ¼å¼ï¼ˆé€è¡Œè§£æï¼‰
                for line in content.split("\n"):
                    if line.strip():
                        raw_item = json.loads(line)
                        converted_item = convert_llm_dataset_item(raw_item)
                        dataset.append(converted_item)
        
        # ä»…åœ¨ç‹¬ç«‹è¿è¡Œæ—¶è¿›è¡Œéšæœºé‡‡æ ·
        total_samples = len(dataset)
        if limit and limit < total_samples:
            import random
            dataset = random.sample(dataset, limit)
            log(f"ğŸ“Š éšæœºé‡‡æ · {limit} æ¡ï¼ˆå…± {total_samples} æ¡å¯ç”¨ï¼‰")
    
    log(f"ğŸ“Š å®é™…è¯„æµ‹æ ·æœ¬æ•°: {len(dataset)}")
    
    # ç»Ÿè®¡æ•°æ®é›†ä¿¡æ¯ï¼ˆé«˜/ä¸­/ä½ä¸‰çº§é£é™©ï¼‰
    high_count = sum(1 for d in dataset if d.get("ground_truth", {}).get("risk_level") == "é«˜")
    medium_count = sum(1 for d in dataset if d.get("ground_truth", {}).get("risk_level") == "ä¸­")
    low_count = sum(1 for d in dataset if d.get("ground_truth", {}).get("risk_level") == "ä½")
    log(f"ğŸ“Š é«˜é£é™©: {high_count}, ä¸­é£é™©: {medium_count}, ä½é£é™©: {low_count}")
    
    # åˆå§‹åŒ–è¯„æµ‹å™¨
    benchmark = AblationBenchmark(mode=mode, source=source)
    metrics = EvalMetrics()
    results = []
    
    # é€ä¸ªè¯„æµ‹
    for i, item in enumerate(dataset):
        log(f"è¯„æµ‹è¿›åº¦: {i+1}/{len(dataset)} - {item.get('id', 'unknown')}")
        result = await benchmark.evaluate_single(item, metrics)
        results.append(result)
    
    # è¾“å‡ºæŠ¥å‘Š
    log(f"\n{'='*60}")
    log(f"ğŸ“ˆ è¯„æµ‹ç»“æœ - æ¨¡å¼ {mode}: {EvalMode.name(mode)}")
    log(f"{'='*60}")
    
    metrics_dict = metrics.to_dict()
    for key, value in metrics_dict.items():
        if isinstance(value, float):
            # latency ä½¿ç”¨ç§’æ•°æ ¼å¼ï¼Œå…¶ä»–ä½¿ç”¨ç™¾åˆ†æ¯”æ ¼å¼
            if "latency" in key:
                log(f"  {key}: {value:.3f}s")
            else:
                log(f"  {key}: {value:.2%}")
        else:
            log(f"  {key}: {value}")
    
    return {
        "mode": mode,
        "mode_name": EvalMode.name(mode),
        "metrics": metrics_dict,
        "results": results,
    }


async def run_full_ablation_study(
    data_path: str,
    modes: List[int] = None,
    limit: int = None,
    source: str = "local"
) -> Dict[str, Any]:
    """è¿è¡Œå®Œæ•´æ¶ˆèå®éªŒï¼ˆæ‰€æœ‰æ¨¡å¼å¯¹æ¯”ï¼Œä½¿ç”¨ç›¸åŒæ ·æœ¬æ§åˆ¶å˜é‡ï¼‰"""
    
    if modes is None:
        modes = [1, 2, 3, 4]  # é»˜è®¤è¿è¡Œæ‰€æœ‰æ¨¡å¼
    
    print("\n" + "="*70)
    print("ğŸ”¬ æ¶ˆèå®éªŒ (Ablation Study) - å¤šæ¨¡å¼å¯¹æ¯”è¯„æµ‹")
    print("="*70)
    
    # ========== ç»Ÿä¸€åŠ è½½å’Œé‡‡æ ·æ•°æ®ï¼ˆæ§åˆ¶å˜é‡ï¼‰ ==========
    if not os.path.exists(data_path):
        print(f"Error: Data file not found at {data_path}")
        return {}
    
    dataset = []
    with open(data_path, "r", encoding="utf-8") as f:
        content = f.read().strip()
        
        # æ£€æµ‹æ–‡ä»¶æ ¼å¼ï¼šJSON æ•°ç»„ vs JSONL
        if content.startswith("["):
            # JSON æ•°ç»„æ ¼å¼
            raw_items = json.loads(content)
        else:
            # JSONL æ ¼å¼ï¼ˆé€è¡Œè§£æï¼‰
            raw_items = [json.loads(line) for line in content.split("\n") if line.strip()]
        
        for raw_item in raw_items:
            converted_item = convert_llm_dataset_item(raw_item)
            dataset.append(converted_item)
    
    total_samples = len(dataset)
    if limit and limit < total_samples:
        # ä½¿ç”¨åˆ†å±‚é‡‡æ ·ï¼Œç¡®ä¿ High/Medium/Low å‡è¡¡ (1:1:1)
        dataset = stratified_sample(dataset, limit)
        print(f"ğŸ“Š ä½¿ç”¨åˆ†å±‚é‡‡æ · (Stratified Sampling) æŠ½å– {limit} æ¡ï¼ˆHigh/Medium/Low 1:1:1ï¼‰")
    
    print(f"ğŸ“Š æ‰€æœ‰æ¨¡å¼å°†ä½¿ç”¨ç›¸åŒçš„ {len(dataset)} æ¡æ ·æœ¬")
    
    # æ‰“å°æ ·æœ¬ ID ä»¥ä¾¿éªŒè¯
    sample_ids = [item.get("id", "unknown") for item in dataset[:5]]
    print(f"ğŸ“Š æ ·æœ¬ ID é¢„è§ˆ: {sample_ids}{'...' if len(dataset) > 5 else ''}")
    
    all_results = {}
    
    # ========== å¯¹æ¯ä¸ªæ¨¡å¼è¿è¡Œè¯„æµ‹ï¼ˆä¼ å…¥ç›¸åŒæ•°æ®é›†ï¼‰ ==========
    for mode in modes:
        result = await run_ablation_benchmark(
            mode=mode,
            source=source,
            dataset=dataset  # ä¼ å…¥é¢„åŠ è½½çš„æ•°æ®é›†
        )
        if result:
            all_results[f"mode_{mode}"] = result
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print("\n" + "="*70)
    print("ğŸ“Š æ¨¡å¼å¯¹æ¯”æ±‡æ€»")
    print("="*70)
    
    # è¡¨å¤´
    print(f"{'æŒ‡æ ‡':<20}", end="")
    for mode in modes:
        print(f"{EvalMode.name(mode):<18}", end="")
    print()
    print("-" * (20 + 18 * len(modes)))
    
    # æŒ‡æ ‡å¯¹æ¯” (åŒ…å«æ–°å¢çš„è¯„ä¼°æŒ‡æ ‡)
    metric_keys = [
        "accuracy", "weighted_accuracy", "f1", "precision", "recall", "parse_rate",
        "hallucination_rate", "clause_hallucination_rate", "law_hallucination_rate",
        "rule_recall", "rule_precision", "task_success_rate", "avg_latency_sec"
    ]
    for key in metric_keys:
        print(f"{key:<25}", end="")
        for mode in modes:
            mode_key = f"mode_{mode}"
            if mode_key in all_results:
                value = all_results[mode_key]["metrics"].get(key, 0)
                # latency ä½¿ç”¨ç§’æ•°æ ¼å¼
                if "latency" in key:
                    print(f"{value:.3f}s".ljust(15), end="")
                else:
                    print(f"{value:.2%}".ljust(15), end="")
            else:
                print(f"{'N/A':<15}", end="")
        print()
    
    # ========== åˆ›å»ºç‹¬ç«‹è¾“å‡ºç›®å½• ==========
    script_dir = Path(__file__).parent
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = script_dir / f"results_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜ç»“æœ JSON
    output_path = output_dir / "ablation_results.json"
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {output_dir}")
    
    # ========== ç”Ÿæˆä¸¤å¼ å›¾è¡¨ï¼ˆåŸºç¡€æŒ‡æ ‡ + é«˜çº§æŒ‡æ ‡ï¼‰==========    # ç”Ÿæˆå›¾è¡¨
    try:
        from evaluation.chart_generator import generate_report_charts
        
        # ç»“æœä¿å­˜ç›®å½•
        output_dir = Path(output_path).parent
        
        # ç”Ÿæˆå›¾è¡¨
        chart_paths = generate_report_charts(all_results, output_dir, timestamp)
        all_results["chart_paths"] = chart_paths
        all_results["timestamp"] = timestamp
        
        # æ›´æ–°ä¿å­˜çš„ JSONï¼ˆåŒ…å«å›¾è¡¨è·¯å¾„ï¼‰
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š å·²ç”Ÿæˆ {len(chart_paths)} å¼ å›¾è¡¨")
    except ImportError as e:
        print(f"âš ï¸ å›¾è¡¨ç”Ÿæˆå¤±è´¥ï¼ˆè¯·ç¡®ä¿å®‰è£… matplotlibï¼‰: {e}")
    except Exception as e:
        print(f"âš ï¸ å›¾è¡¨ç”Ÿæˆå¼‚å¸¸: {e}")
    
    return all_results


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="æ¶ˆèå®éªŒè¯„æµ‹è„šæœ¬")
    parser.add_argument("--data", type=str, default="evaluation/llm_benchmark_dataset.json",
                        help="è¯„æµ‹æ•°æ®é›†è·¯å¾„ (æ”¯æŒæ–°æ—§ä¸¤ç§æ ¼å¼)")
    parser.add_argument("--mode", type=int, choices=[1, 2, 3, 4], default=None,
                        help="è¯„æµ‹æ¨¡å¼ (1-4)ï¼Œä¸æŒ‡å®šåˆ™è¿è¡Œæ‰€æœ‰æ¨¡å¼")
    parser.add_argument("--limit", type=int, default=None,
                        help="é™åˆ¶è¯„æµ‹æ ·æœ¬æ•°é‡")
    parser.add_argument("--source", type=str, choices=["local", "cloud"], default="local",
                        help="LLMæ¥æº (local/cloud)")
    
    args = parser.parse_args()
    
    if args.mode:
        # è¿è¡Œå•ä¸ªæ¨¡å¼
        result = asyncio.run(run_ablation_benchmark(
            data_path=args.data,
            mode=args.mode,
            limit=args.limit,
            source=args.source
        ))
        
        # ä¿å­˜å•æ¨¡å¼ç»“æœåˆ°æ–‡ä»¶
        if result:
            from datetime import datetime
            script_dir = Path(__file__).parent
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = script_dir / f"results_mode{args.mode}_{timestamp}"
            output_dir.mkdir(parents=True, exist_ok=True)
            
            output_path = output_dir / "ablation_results.json"
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump({f"mode_{args.mode}": result}, f, ensure_ascii=False, indent=2)
            
            print(f"\nâœ… ç»“æœå·²ä¿å­˜: {output_path}")
    else:
        # è¿è¡Œå®Œæ•´æ¶ˆèå®éªŒ
        asyncio.run(run_full_ablation_study(
            data_path=args.data,
            limit=args.limit,
            source=args.source
        ))

