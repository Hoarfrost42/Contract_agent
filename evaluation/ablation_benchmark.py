"""
æ¶ˆèå®éªŒè¯„æµ‹è„šæœ¬ (Ablation Study Benchmark)

æ”¯æŒ4ç§è¯„æµ‹æ¨¡å¼å¯¹æ¯”ï¼š
- Mode 1: çº¯LLMï¼ˆæ— Promptæ¨¡æ¿ï¼Œç›´æ¥è¾“å…¥æ¡æ¬¾ï¼‰
- Mode 2: åŸºç¡€Promptï¼ˆæœ‰æ ¼å¼åŒ–Promptï¼Œæ— è§„åˆ™å¼•æ“ï¼‰
- Mode 3: å½“å‰å·¥ä½œæµï¼ˆPrompt + è§„åˆ™å¼•æ“ï¼‰
- Mode 4: ä¼˜åŒ–å·¥ä½œæµï¼ˆæ”¹è¿›Prompt + è§„åˆ™å¼•æ“ï¼‰

è¯„æµ‹æŒ‡æ ‡ï¼š
- é£é™©ç­‰çº§å‡†ç¡®ç‡ (Accuracy)
- F1 åˆ†æ•° (Precision/Recall å¹³è¡¡)
- Jaccard ç›¸ä¼¼åº¦
- å¹»è§‰ç‡ (å¼•ç”¨éªŒè¯å¤±è´¥æ¯”ä¾‹)
- è§£ææˆåŠŸç‡
"""

import argparse
import asyncio
import json
import os
import sys
import re
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field

# Add project root to sys.path
sys.path.append(str(Path(__file__).resolve().parents[1]))

from src.utils.config_loader import load_config

# å¯¼å…¥ç»Ÿä¸€æ¨¡å—
from src.core.ollama_client import OllamaClient
from src.core.output_parser import parse_markdown_output, ParsedResult
from src.core.prompts import (
    RAW_LLM_PROMPT,
    BASIC_PROMPT, 
    CURRENT_WORKFLOW_PROMPT,
    OPTIMIZED_WORKFLOW_PROMPT,
    SELF_REFLECTION_PROMPT,
    get_prompt_by_mode,
)
from src.core.reference_retriever import retrieve_reference



# ============================================================================
# OllamaClient å·²ç§»è‡³ src/core/ollama_client.pyï¼Œé€šè¿‡å¯¼å…¥ä½¿ç”¨
# ============================================================================


# ============================================================================
# è¯„æµ‹æ¨¡å¼å®šä¹‰
# ============================================================================

class EvalMode:
    """è¯„æµ‹æ¨¡å¼æšä¸¾"""
    RAW_LLM = 1           # çº¯LLMï¼Œæ— Promptæ¨¡æ¿
    BASIC_PROMPT = 2      # åŸºç¡€Promptï¼Œæ— è§„åˆ™å¼•æ“
    CURRENT_WORKFLOW = 3  # å½“å‰å·¥ä½œæµï¼ˆPrompt + è§„åˆ™å¼•æ“ï¼‰
    OPTIMIZED_WORKFLOW = 4  # ä¼˜åŒ–å·¥ä½œæµï¼ˆæ”¹è¿›Prompt + è§„åˆ™å¼•æ“ï¼‰
    
    @staticmethod
    def name(mode: int) -> str:
        names = {
            1: "çº¯LLM (Raw)",
            2: "åŸºç¡€Prompt",
            3: "å½“å‰å·¥ä½œæµ",
            4: "ä¼˜åŒ–å·¥ä½œæµ"
        }
        return names.get(mode, "æœªçŸ¥")




# ============================================================================
# Prompt æ¨¡æ¿å·²ç§»è‡³ src/core/prompts.pyï¼Œé€šè¿‡å¯¼å…¥ä½¿ç”¨
# ============================================================================




# ============================================================================
# ParsedResult å’Œ parse_markdown_output å·²ç§»è‡³ src/core/output_parser.py
# ============================================================================


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def stratified_sample(dataset: List[dict], limit: int) -> List[dict]:
    """åˆ†å±‚é‡‡æ ·ï¼šç¡®ä¿ High/Medium/Low æ¯”ä¾‹å°½é‡ä¸º 1:1:1"""
    high = [d for d in dataset if d.get("ground_truth", {}).get("risk_level") == "é«˜"]
    medium = [d for d in dataset if d.get("ground_truth", {}).get("risk_level") == "ä¸­"]
    low = [d for d in dataset if d.get("ground_truth", {}).get("risk_level") == "ä½"]
    
    per_class = limit // 3
    
    sampled = []
    import random
    
    # 1. æ ¸å¿ƒé‡‡æ ·ï¼šæ¯ç±»æŠ½å– limit/3
    sampled.extend(random.sample(high, min(len(high), per_class)))
    sampled.extend(random.sample(medium, min(len(medium), per_class)))
    sampled.extend(random.sample(low, min(len(low), per_class)))
    
    # 2. è¡¥é½å‰©ä½™ï¼šå¦‚æœæ€»æ•°ä¸è¶³ limitï¼ˆå› é™¤ä¸å°½æˆ–æŸç±»æ ·æœ¬ä¸è¶³ï¼‰
    current_count = len(sampled)
    if current_count < limit:
        # åˆ›å»ºå‰©ä½™æ± ï¼šæ‰€æœ‰æœªè¢«é€‰ä¸­çš„æ ·æœ¬
        # å¿…é¡»ç¡®ä¿ dataset ä¸­çš„å…ƒç´ ä¹Ÿæ˜¯å”¯ä¸€çš„æˆ–è€…é€šè¿‡ ID å»é‡
        sampled_ids = {item.get("id") for item in sampled}
        remaining_pool = [d for d in dataset if d.get("id") not in sampled_ids]
        
        needed = limit - current_count
        if remaining_pool:
            sampled.extend(random.sample(remaining_pool, min(len(remaining_pool), needed)))
    
    random.shuffle(sampled)
    return sampled


def parse_reflection_output(content: str) -> dict:
    """è§£æè‡ªåæ€è¾“å‡º
    
    æœŸæœ›æ ¼å¼ï¼š
    å®¡æŸ¥ç»“è®ºï¼š[ç»´æŒ / è°ƒçº§]
    ä¿®æ­£å»ºè®®ï¼š[è‹¥è°ƒçº§ï¼Œè¯·å†™å…·ä½“ç­‰çº§æµå‘ï¼Œå¦‚"ä¸­é£é™© -> ä½é£é™©"ï¼›è‹¥ç»´æŒï¼Œå¡«"æ— "]
    ç†ç”±ï¼š[åŸºäºå®¡æŸ¥åŸºå‡†ç®€è¿°ç†ç”±]
    
    Returns:
        dict: {
            "conclusion": "ç»´æŒ" / "è°ƒçº§",
            "adjustment": "ä¸­é£é™© -> ä½é£é™©" / "æ— ",
            "new_level": "é«˜" / "ä¸­" / "ä½" / None,
            "reason": "..."
        }
    """
    result = {
        "conclusion": "ç»´æŒ",
        "adjustment": "æ— ",
        "new_level": None,
        "reason": ""
    }
    
    # è§£æå®¡æŸ¥ç»“è®º
    conclusion_match = re.search(r'å®¡æŸ¥ç»“è®º[ï¼š:]\s*\[?\s*(ç»´æŒ|è°ƒçº§)\s*\]?', content)
    if conclusion_match:
        result["conclusion"] = conclusion_match.group(1)
    
    # è§£æä¿®æ­£å»ºè®®
    adjustment_match = re.search(r'ä¿®æ­£å»ºè®®[ï¼š:]\s*\[?\s*(.+?)\s*\]?(?:\n|$)', content)
    if adjustment_match:
        adj = adjustment_match.group(1).strip()
        result["adjustment"] = adj
        
        # æå–æ–°çš„é£é™©ç­‰çº§
        if "ä½é£é™©" in adj and "->" in adj:
            result["new_level"] = "ä½"
        elif "ä¸­é£é™©" in adj and "->" in adj:
            result["new_level"] = "ä¸­"
        elif "é«˜é£é™©" in adj and "->" in adj:
            result["new_level"] = "é«˜"
    
    # è§£æç†ç”±
    reason_match = re.search(r'ç†ç”±[ï¼š:]\s*\[?\s*(.+?)\s*\]?(?:\n|$)', content, re.DOTALL)
    if reason_match:
        result["reason"] = reason_match.group(1).strip()[:100]  # æˆªæ–­åˆ°100å­—
    
    return result


# ============================================================================
# è¯„ä¼°å™¨
# ============================================================================

@dataclass
class EvalMetrics:
    """è¯„ä¼°æŒ‡æ ‡"""
    # åŸºç¡€æŒ‡æ ‡
    total: int = 0
    correct_risk: int = 0
    correct_reason: int = 0
    parse_success: int = 0
    
    # åŠ æƒè¯„åˆ†ï¼ˆç²¾ç¡®åŒ¹é…1åˆ†ï¼Œå·®ä¸€çº§0.5åˆ†ï¼Œå·®ä¸¤çº§0åˆ†ï¼‰
    total_weighted_score: float = 0.0
    
    # ä¸‰åˆ†ç±»æ··æ·†çŸ©é˜µ (High=0, Medium=1, Low=2)
    # confusion_matrix[actual][predicted]
    conf_matrix: List[List[int]] = field(default_factory=lambda: [[0, 0, 0], [0, 0, 0], [0, 0, 0]])
    
    # æ··æ·†çŸ©é˜µï¼ˆæ—§äºŒåˆ†ç±»å…¼å®¹ï¼Œç”¨äº Precision/Recall/F1ï¼‰
    true_positive: int = 0   
    false_positive: int = 0  
    false_negative: int = 0  
    true_negative: int = 0   
    
    # ===== æ–¹æ³•ä¸€ï¼šè¯æ®ä¸€è‡´æ€§è¯„ä¼°ï¼ˆç»†åˆ†å¹»è§‰ç±»å‹ï¼‰=====
    clause_evidence_valid: int = 0    # åˆåŒæ¡æ¬¾è¯æ®æœ‰æ•ˆ
    clause_evidence_invalid: int = 0  # åˆåŒæ¡æ¬¾è¯æ®å¹»è§‰
    law_citation_valid: int = 0       # æ³•å¾‹å¼•ç”¨æœ‰æ•ˆ
    law_citation_invalid: int = 0     # æ³•å¾‹å¼•ç”¨å¹»è§‰ï¼ˆä¸å­˜åœ¨çš„æ³•æ¡ï¼‰
    
    # æ—§å­—æ®µä¿æŒå…¼å®¹
    evidence_valid: int = 0
    evidence_invalid: int = 0
    
    # ===== æ–¹æ³•äºŒï¼šè§„åˆ™è§¦å‘ä¸€è‡´æ€§ =====
    rule_trigger_count: int = 0   # å®é™…è§¦å‘çš„è§„åˆ™æ•°
    rule_target_count: int = 0    # åº”è¯¥è§¦å‘çš„è§„åˆ™æ•°
    rule_correct_count: int = 0   # æ­£ç¡®è§¦å‘çš„è§„åˆ™æ•°
    
    # æ–°å¢ï¼šrisk_id åŒ¹é…ï¼ˆå¤šæ ‡ç­¾åœºæ™¯ï¼‰
    risk_id_match: int = 0
    risk_id_total: int = 0
    
    # ===== æ–¹æ³•ä¸‰ï¼šä»»åŠ¡æˆåŠŸç‡ =====
    task_success_count: int = 0  # ä»»åŠ¡å®Œå…¨æˆåŠŸçš„æ ·æœ¬æ•°
    
    # ===== è‡ªåæ€æœºåˆ¶ç»Ÿè®¡ =====
    reflection_calls: int = 0      # è‡ªåæ€è°ƒç”¨æ¬¡æ•°
    reflection_adjustments: int = 0  # åæ€åè°ƒçº§æ¬¡æ•°
    reflection_maintain: int = 0   # åæ€åç»´æŒåŸåˆ¤æ¬¡æ•°
    
    # å“åº”æ—¶é—´ç»Ÿè®¡
    total_latency: float = 0.0
    
    @staticmethod
    def calculate_weighted_score(gt_risk: str, pred_risk: str) -> float:
        """
        è®¡ç®—éå¯¹ç§°åŠ æƒè¯„åˆ† (Asymmetric Weighted Accuracy)ï¼š
        - ç²¾ç¡®åŒ¹é…ï¼š1.0åˆ†
        - é˜²å¾¡æ€§è¯¯åˆ¤ï¼ˆä½â†’ä¸­, ä¸­â†’é«˜ï¼‰ï¼š0.8åˆ†ï¼ˆå®å¯é”™æ€ï¼‰
        - é£é™©é™çº§ï¼ˆé«˜â†’ä¸­ï¼‰ï¼š0.4åˆ†ï¼ˆæ¼æŠ¥æ‰£åˆ†æ›´é‡ï¼‰
        - å·®ä¸¤çº§ï¼ˆé«˜â†”ä½ï¼‰ï¼š0.0åˆ†ï¼ˆè‡´å‘½æ¼åˆ¤é›¶å®¹å¿ï¼‰
        """
        if gt_risk == pred_risk:
            return 1.0
        
        # éå¯¹ç§°æƒé‡çŸ©é˜µ: weight_matrix[gt][pred]
        # gt: é«˜=0, ä¸­=1, ä½=2
        asymmetric_weights = {
            ("é«˜", "ä¸­"): 0.4,  # é«˜é£é™©é™ä¸ºä¸­ï¼šå±é™©ï¼Œæ‰£åˆ†é‡
            ("é«˜", "ä½"): 0.0,  # é«˜é£é™©é™ä¸ºä½ï¼šè‡´å‘½æ¼åˆ¤
            ("ä¸­", "é«˜"): 0.8,  # ä¸­é£é™©å‡ä¸ºé«˜ï¼šè¿‡åº¦è°¨æ…ï¼Œå¯æ¥å—
            ("ä¸­", "ä½"): 0.4,  # ä¸­é£é™©é™ä¸ºä½ï¼šæœ‰ä¸€å®šé£é™©
            ("ä½", "ä¸­"): 0.8,  # ä½é£é™©å‡ä¸ºä¸­ï¼šé˜²å¾¡æ€§è¯¯åˆ¤
            ("ä½", "é«˜"): 0.5,  # ä½é£é™©å‡ä¸ºé«˜ï¼šè¿‡åº¦æŠ¥è­¦
        }
        
        return asymmetric_weights.get((gt_risk, pred_risk), 0.0)
            
    def update_confusion_matrix(self, gt_risk: str, pred_risk: str):
        """æ›´æ–°ä¸‰åˆ†ç±»æ··æ·†çŸ©é˜µ"""
        level_map = {"é«˜": 0, "ä¸­": 1, "ä½": 2}
        gt_idx = level_map.get(gt_risk, 2)   # é»˜è®¤ä¸ºä½é£é™©
        pred_idx = level_map.get(pred_risk, 2)
        self.conf_matrix[gt_idx][pred_idx] += 1

    def calculate_kappa(self, use_linear: bool = True) -> float:
        """
        è®¡ç®—åŠ æƒ Kappa
        
        Args:
            use_linear: True=çº¿æ€§æƒé‡(LWK), False=äºŒæ¬¡æ–¹æƒé‡(QWK)
        
        Linear Weighted Kappa å…¬å¼: w_ij = 1 - |i-j| / (N-1)
        å¯¹äºæœ‰åºåˆ†ç±»æ›´ç¨³å¥ï¼Œä¸ä¼šè¿‡åº¦æƒ©ç½š"ç¦»ç¾¤"é”™è¯¯
        """
        n_classes = 3
        weights = [[0.0] * n_classes for _ in range(n_classes)]
        
        for i in range(n_classes):
            for j in range(n_classes):
                if use_linear:
                    # Linear Weighted Kappa
                    weights[i][j] = abs(i - j) / (n_classes - 1)
                else:
                    # Quadratic Weighted Kappa (åŸç‰ˆ)
                    weights[i][j] = ((i - j) / (n_classes - 1)) ** 2
                
        # è§‚å¯ŸçŸ©é˜µ O (å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ)
        total = self.total
        if total == 0: return 0.0
        
        observed = [[self.conf_matrix[i][j] / total for j in range(n_classes)] for i in range(n_classes)]
        
        # æœŸæœ›çŸ©é˜µ E (è¾¹ç¼˜åˆ†å¸ƒå¤–ç§¯)
        row_sums = [sum(self.conf_matrix[i]) / total for i in range(n_classes)]
        col_sums = [sum(self.conf_matrix[i][j] for i in range(n_classes)) / total for j in range(n_classes)]
        expected = [[row_sums[i] * col_sums[j] for j in range(n_classes)] for i in range(n_classes)]
        
        # è®¡ç®— Kappa = 1 - (sum(W*O) / sum(W*E))
        numerator = sum(weights[i][j] * observed[i][j] for i in range(n_classes) for j in range(n_classes))
        denominator = sum(weights[i][j] * expected[i][j] for i in range(n_classes) for j in range(n_classes))
        
        if denominator == 0: return 1.0  # å®Œå…¨ä¸€è‡´
        return 1.0 - (numerator / denominator)

    def calculate_macro_f1(self) -> dict:
        """è®¡ç®—å®å¹³å‡ F1"""
        f1_scores = []
        precisions = []
        recalls = []
        
        for k in range(3):  # 0:é«˜, 1:ä¸­, 2:ä½
            # TP = conf[k][k]
            tp = self.conf_matrix[k][k]
            # FP = sum(col[k]) - TP
            fp = sum(self.conf_matrix[i][k] for i in range(3)) - tp
            # FN = sum(row[k]) - TP
            fn = sum(self.conf_matrix[k]) - tp
            
            p = tp / (tp + fp) if (tp + fp) > 0 else 0
            r = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0
            
            precisions.append(p)
            recalls.append(r)
            f1_scores.append(f1)
            
        return {
            "macro_precision": sum(precisions) / 3,
            "macro_recall": sum(recalls) / 3,
            "macro_f1": sum(f1_scores) / 3,
            "class_f1": {"High": f1_scores[0], "Medium": f1_scores[1], "Low": f1_scores[2]},
            "class_precision": {"High": precisions[0], "Medium": precisions[1], "Low": precisions[2]},
            "class_recall": {"High": recalls[0], "Medium": recalls[1], "Low": recalls[2]}
        }
    
    def calculate_high_risk_f2(self) -> float:
        """
        è®¡ç®—é«˜é£é™©ç±»åˆ«çš„ F2-Score (Recall-Oriented)
        
        F2 = (1 + Î²Â²) Ã— (P Ã— R) / (Î²Â² Ã— P + R)ï¼Œå…¶ä¸­ Î² = 2
        F2 åˆ†æ•°ä¸­ Recall æƒé‡æ˜¯ Precision çš„ 2 å€ï¼Œé€‚åˆ"å®å¯é”™æ€"çš„é£æ§åœºæ™¯
        """
        # High = index 0 in confusion matrix
        tp = self.conf_matrix[0][0]
        fp = sum(self.conf_matrix[i][0] for i in range(3)) - tp
        fn = sum(self.conf_matrix[0]) - tp
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        
        beta = 2  # F2-Score
        if (precision + recall) == 0:
            return 0.0
        
        f2 = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall)
        return f2
    
    def weighted_accuracy(self) -> float:
        """åŠ æƒå‡†ç¡®ç‡ï¼ˆè€ƒè™‘éƒ¨åˆ†åŒ¹é…ï¼‰"""
        return self.total_weighted_score / self.total if self.total > 0 else 0
    
    def accuracy(self) -> float:
        return self.correct_risk / self.total if self.total > 0 else 0
    
    def precision(self) -> float:
        denom = self.true_positive + self.false_positive
        return self.true_positive / denom if denom > 0 else 0
    
    def recall(self) -> float:
        denom = self.true_positive + self.false_negative
        return self.true_positive / denom if denom > 0 else 0
    
    def f1(self) -> float:
        p, r = self.precision(), self.recall()
        return 2 * p * r / (p + r) if (p + r) > 0 else 0
    
    def parse_rate(self) -> float:
        return self.parse_success / self.total if self.total > 0 else 0
    
    def hallucination_rate(self) -> float:
        total_evidence = self.evidence_valid + self.evidence_invalid
        return self.evidence_invalid / total_evidence if total_evidence > 0 else 0
    
    # ===== æ–°å¢ï¼šç»†åˆ†å¹»è§‰ç‡ =====
    def clause_hallucination_rate(self) -> float:
        """åˆåŒæ¡æ¬¾è¯æ®å¹»è§‰ç‡"""
        total = self.clause_evidence_valid + self.clause_evidence_invalid
        return self.clause_evidence_invalid / total if total > 0 else 0
    
    def law_hallucination_rate(self) -> float:
        """æ³•å¾‹å¼•ç”¨å¹»è§‰ç‡"""
        total = self.law_citation_valid + self.law_citation_invalid
        return self.law_citation_invalid / total if total > 0 else 0
    
    # ===== æ–°å¢ï¼šè§„åˆ™è§¦å‘ä¸€è‡´æ€§ =====
    def rule_recall(self) -> float:
        """è§„åˆ™å¬å›ç‡ï¼šæ­£ç¡®è§¦å‘ / åº”è§¦å‘"""
        return self.rule_correct_count / self.rule_target_count if self.rule_target_count > 0 else 0
    
    def rule_precision(self) -> float:
        """è§„åˆ™ç²¾ç¡®ç‡ï¼šæ­£ç¡®è§¦å‘ / å®é™…è§¦å‘"""
        return self.rule_correct_count / self.rule_trigger_count if self.rule_trigger_count > 0 else 0
    
    # ===== æ–°å¢ï¼šä»»åŠ¡æˆåŠŸç‡ =====
    def task_success_rate(self) -> float:
        """ä»»åŠ¡æˆåŠŸç‡ï¼šå®Œå…¨æˆåŠŸçš„æ ·æœ¬ / æ€»æ ·æœ¬"""
        return self.task_success_count / self.total if self.total > 0 else 0
    
    def risk_id_accuracy(self) -> float:
        return self.risk_id_match / self.risk_id_total if self.risk_id_total > 0 else 0
    
    def avg_latency(self) -> float:
        return self.total_latency / self.total if self.total > 0 else 0
    
    def to_dict(self) -> dict:
        macro = self.calculate_macro_f1()
        return {
            "total": self.total,
            # åŸºç¡€æŒ‡æ ‡
            "accuracy": round(self.correct_risk / self.total, 4) if self.total > 0 else 0,
            "weighted_accuracy": round(self.total_weighted_score / self.total, 4) if self.total > 0 else 0,
            
            # Kappa æŒ‡æ ‡ (æ–°å¢ LWK å’Œ QWK å¯¹æ¯”)
            "kappa_linear": round(self.calculate_kappa(use_linear=True), 4),  # çº¿æ€§åŠ æƒ Kappa (æ¨è)
            "kappa_quadratic": round(self.calculate_kappa(use_linear=False), 4),  # äºŒæ¬¡æ–¹åŠ æƒ Kappa (å¯¹æ¯”)
            "kappa": round(self.calculate_kappa(use_linear=True), 4),  # é»˜è®¤ä½¿ç”¨ LWKï¼Œä¿æŒå…¼å®¹
            
            # ä¸‰åˆ†ç±»æŒ‡æ ‡
            "macro_precision": round(macro["macro_precision"], 4),
            "macro_recall": round(macro["macro_recall"], 4),
            "macro_f1": round(macro["macro_f1"], 4),
            
            # é«˜é£é™© F2-Score (æ–°å¢ - Recall ä¼˜å…ˆ)
            "high_risk_f2": round(self.calculate_high_risk_f2(), 4),
            
            # åˆ†ç±»åˆ«æŒ‡æ ‡
            "class_f1": {
                "High": round(macro["class_f1"]["High"], 4),
                "Medium": round(macro["class_f1"]["Medium"], 4),
                "Low": round(macro["class_f1"]["Low"], 4)
            },
            "class_precision": {
                "High": round(macro["class_precision"]["High"], 4),
                "Medium": round(macro["class_precision"]["Medium"], 4),
                "Low": round(macro["class_precision"]["Low"], 4)
            },
            "class_recall": {
                "High": round(macro["class_recall"]["High"], 4),
                "Medium": round(macro["class_recall"]["Medium"], 4),
                "Low": round(macro["class_recall"]["Low"], 4)
            },
            
            # æ··æ·†çŸ©é˜µ (ç”¨äºç»˜å›¾)
            "conf_matrix": self.conf_matrix,
            
            # è¿‡ç¨‹æŒ‡æ ‡
            "parse_rate": round(self.parse_success / self.total, 4) if self.total > 0 else 0,
            
            # å¹»è§‰æ£€æµ‹
            "hallucination_rate": round(self.evidence_invalid / (self.evidence_valid + self.evidence_invalid), 4) if (self.evidence_valid + self.evidence_invalid) > 0 else 0,
            "clause_hallucination_rate": round(self.clause_evidence_invalid / (self.clause_evidence_valid + self.clause_evidence_invalid), 4) if (self.clause_evidence_valid + self.clause_evidence_invalid) > 0 else 0,
            "law_hallucination_rate": round(self.law_citation_invalid / (self.law_citation_valid + self.law_citation_invalid), 4) if (self.law_citation_valid + self.law_citation_invalid) > 0 else 0,
            
            # è§„åˆ™è§¦å‘
            "rule_recall": round(self.rule_correct_count / self.rule_target_count, 4) if self.rule_target_count > 0 else 0,
            "rule_precision": round(self.rule_correct_count / self.rule_trigger_count, 4) if self.rule_trigger_count > 0 else 0,
            
            # ä»»åŠ¡æˆåŠŸç‡
            "task_success_rate": round(self.task_success_count / self.total, 4) if self.total > 0 else 0,
            
            # Risk ID å‡†ç¡®ç‡
            "risk_id_accuracy": round(self.risk_id_match / self.risk_id_total, 4) if self.risk_id_total > 0 else 0,
            
            # æ€§èƒ½
            "avg_latency_sec": round(self.total_latency / self.total, 3) if self.total > 0 else 0,
            "reason_quality": round(self.correct_reason / self.total, 4) if self.total > 0 else 0,
            
            # è‡ªåæ€æœºåˆ¶ç»Ÿè®¡
            "reflection_calls": self.reflection_calls,
            "reflection_adjustments": self.reflection_adjustments,
            "reflection_maintain": self.reflection_maintain,
            "reflection_adjustment_rate": round(self.reflection_adjustments / self.reflection_calls, 4) if self.reflection_calls > 0 else 0,
        }


def verify_evidence(
    evidence: str, 
    clause: str, 
    embedding_model=None, 
    reranker=None,
    threshold: float = 0.7
) -> tuple:
    """éªŒè¯è¯æ®æ˜¯å¦å­˜åœ¨äºåŸæ–‡ä¸­ï¼ˆä¸¤é˜¶æ®µæ£€ç´¢ + ç²¾æ’ï¼‰
    
    æµç¨‹ï¼š
    1. ç²¾ç¡®å­ä¸²åŒ¹é…ï¼ˆå¿«é€Ÿè·¯å¾„ï¼‰
    2. Stage 1: BGE-M3 Dense å¬å› Top-K å€™é€‰
    3. Stage 2: Reranker ç²¾æ’ï¼Œåˆ¤å®šç›¸ä¼¼åº¦æ˜¯å¦ >= threshold
    4. å›é€€ï¼šæ¨¡ç³ŠåŒ¹é…ï¼ˆå­—ç¬¦é‡å ï¼‰
    
    Args:
        evidence: LLM è¾“å‡ºçš„è¯æ®å­—æ®µ
        clause: åŸå§‹åˆåŒæ¡æ¬¾æ–‡æœ¬
        embedding_model: Sentence Transformer æ¨¡å‹ï¼ˆç”¨äºå¬å›ï¼‰
        reranker: BGE-Reranker æ¨¡å‹ï¼ˆç”¨äºç²¾æ’ï¼‰
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.7ï¼‰
    
    Returns:
        tuple: (is_valid: bool, similarity_score: float, match_type: str)
        - is_valid: è¯æ®æ˜¯å¦æœ‰æ•ˆ
        - similarity_score: ç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆ0-1ï¼‰
        - match_type: åŒ¹é…ç±»å‹ï¼ˆexact/reranker_match/semantic/fuzzy/hallucinationï¼‰
    """
    if not evidence or evidence in ["æ— ", "None", "", "ç•™ç©º"]:
        return (True, 1.0, "empty")  # æ— è¯æ®ä¸ç®—å¹»è§‰
    
    # æ¸…ç†è¯æ®æ–‡æœ¬
    clean_evidence = evidence.replace("ã€Œ", "").replace("ã€", "").replace("\"", "").strip()
    if len(clean_evidence) < 5:
        return (True, 1.0, "too_short")
    
    # 1. ç²¾ç¡®åŒ¹é…ï¼ˆå­ä¸²åŒ¹é…ï¼‰- å¿«é€Ÿè·¯å¾„
    if clean_evidence in clause:
        return (True, 1.0, "exact")
    
    # åˆ†å¥
    sentences = [s.strip() for s in clause.replace("ã€‚", "ã€‚\n").split("\n") if s.strip()]
    if not sentences:
        sentences = [clause]
    
    # 2. Stage 1: BGE-M3 Dense å¬å› Top-K å€™é€‰
    candidates = sentences[:3]  # é»˜è®¤å–å‰3å¥
    if embedding_model is not None:
        try:
            from sklearn.metrics.pairwise import cosine_similarity
            
            evidence_embedding = embedding_model.encode([clean_evidence])
            sentence_embeddings = embedding_model.encode(sentences)
            
            similarities = cosine_similarity(evidence_embedding, sentence_embeddings)[0]
            
            # å– Top-3 å€™é€‰è¿›å…¥ç²¾æ’
            top_k_indices = similarities.argsort()[-3:][::-1]
            candidates = [sentences[i] for i in top_k_indices]
            
            # å¦‚æœæ²¡æœ‰ rerankerï¼Œç›´æ¥ä½¿ç”¨ embedding ç›¸ä¼¼åº¦
            if reranker is None:
                max_similarity = float(max(similarities))
                if max_similarity >= threshold:
                    return (True, max_similarity, "semantic")
        except Exception as e:
            logging.warning(f"è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
    
    # 3. Stage 2: Reranker ç²¾æ’
    if reranker is not None and candidates:
        try:
            rerank_results = reranker.rerank(
                query=clean_evidence,
                candidates=[{"text": s} if isinstance(s, str) else s for s in candidates],
                top_k=1
            )
            if rerank_results:
                best_score = rerank_results[0][1]  # (doc, score)
                if best_score >= threshold:
                    return (True, best_score, "reranker_match")
                else:
                    return (False, best_score, "hallucination")
        except Exception as e:
            logging.warning(f"Reranker ç²¾æ’å¤±è´¥: {e}")
    
    # 4. å›é€€ï¼šæ¨¡ç³ŠåŒ¹é…ï¼ˆå­—ç¬¦é‡å ï¼‰
    evidence_words = set(clean_evidence)
    clause_words = set(clause)
    overlap = len(evidence_words & clause_words) / len(evidence_words) if evidence_words else 0
    
    if overlap >= 0.6:
        return (True, overlap, "fuzzy")
    
    return (False, overlap, "hallucination")



def verify_law_citation(
    law_reference: str, 
    reference_info: str = "",
    law_db_path: str = None
) -> tuple:
    """éªŒè¯æ³•å¾‹å¼•ç”¨æ˜¯å¦æœ‰æ•ˆï¼ˆåˆ†çº§éªŒè¯ï¼‰
    
    éªŒè¯é¡ºåº:
    1. æ£€æŸ¥æ˜¯å¦åœ¨ reference_info ä¸­æåŠ
    2. æ£€æŸ¥æ³•å¾‹åç§°æ˜¯å¦åœ¨æ•°æ®åº“/ç™½åå•ä¸­
    3. æ£€æŸ¥æ¡æ¬¾å·æ ¼å¼æ˜¯å¦æ­£ç¡®
    4. æ£€æŸ¥å¼•ç”¨å†…å®¹æ˜¯å¦ä¸æ•°æ®åº“ä¸€è‡´ï¼ˆå¦‚æœ‰ï¼‰
    
    Args:
        law_reference: LLM è¾“å‡ºçš„æ³•æ¡å¼•ç”¨
        reference_info: è§„åˆ™å¼•æ“æä¾›çš„å‚è€ƒä¿¡æ¯
        law_db_path: æ³•å¾‹æ•°æ®åº“è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        tuple: (is_valid: bool, validation_level: int, detail: str)
        - is_valid: æ³•æ¡æ˜¯å¦æœ‰æ•ˆ
        - validation_level: éªŒè¯é€šè¿‡çš„å±‚çº§ï¼ˆ1-4ï¼Œ0è¡¨ç¤ºå¤±è´¥ï¼‰
        - detail: éªŒè¯è¯¦æƒ…
    """
    import re
    
    if not law_reference or law_reference in ["æ— ", "None", "", "ç•™ç©º"]:
        return (True, 0, "empty")  # æ— å¼•ç”¨ä¸ç®—å¹»è§‰
    
    # æå–æ³•å¾‹åç§°ï¼ˆæ”¯æŒã€Šã€‹å’Œã€ã€‘ä¸¤ç§æ ¼å¼ï¼‰
    # æ ¼å¼1: ã€ŠåŠ³åŠ¨åˆåŒæ³•ã€‹ç¬¬Xæ¡
    # æ ¼å¼2: ã€æ°‘æ³•å…¸ ç¬¬Xæ¡ã€‘
    law_name_pattern1 = r"[ã€Š]([^ã€‹]+)[ã€‹]"  # ä¹¦åå·æ ¼å¼
    law_name_pattern2 = r"[ã€]([^ç¬¬]+?)[\sç¬¬]"  # æ–¹æ‹¬å·æ ¼å¼ï¼ˆæå–åˆ°"ç¬¬"å­—ä¹‹å‰ï¼‰
    law_name_pattern3 = r"(æ°‘æ³•å…¸|åŠ³åŠ¨åˆåŒæ³•|åŠ³åŠ¨æ³•|åˆåŒæ³•|ç‰©æƒæ³•|æ¶ˆè´¹è€…æƒç›Šä¿æŠ¤æ³•|ç¤¾ä¼šä¿é™©æ³•|ä¸ªäººä¿¡æ¯ä¿æŠ¤æ³•)"  # ç›´æ¥åŒ¹é…å¸¸è§æ³•å¾‹å
    
    law_names_found = re.findall(law_name_pattern1, law_reference)
    law_names_found.extend(re.findall(law_name_pattern2, law_reference))
    law_names_found.extend(re.findall(law_name_pattern3, law_reference))
    # å»é‡
    law_names_found = list(set([name.strip() for name in law_names_found if name.strip()]))
    
    # æå–æ¡æ¬¾å·
    article_pattern = r"ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒé›¶\d]+)æ¡"
    articles_found = re.findall(article_pattern, law_reference)
    
    # ========== å±‚çº§1ï¼šæ£€æŸ¥æ˜¯å¦åœ¨ reference_info ä¸­ ==========
    if reference_info:
        for law_name in law_names_found:
            if law_name in reference_info:
                return (True, 1, f"åœ¨ reference_info ä¸­æ‰¾åˆ°: {law_name}")
    
    # ========== å±‚çº§2ï¼šæ£€æŸ¥æ³•å¾‹åç§°æ˜¯å¦åœ¨ç™½åå•/æ•°æ®åº“ä¸­ ==========
    valid_laws = [
        # åŠ³åŠ¨æ³•ç›¸å…³
        "åŠ³åŠ¨åˆåŒæ³•", "åŠ³åŠ¨æ³•", "ç¤¾ä¼šä¿é™©æ³•", "å·¥ä¼¤ä¿é™©æ¡ä¾‹", 
        "åŠ³åŠ¨äº‰è®®è°ƒè§£ä»²è£æ³•", "å°±ä¸šä¿ƒè¿›æ³•", "èŒä¸šç—…é˜²æ²»æ³•",
        "å·¥èµ„æ”¯ä»˜æš‚è¡Œè§„å®š", "å¸¦è–ªå¹´ä¼‘å‡æ¡ä¾‹", "æœ€ä½å·¥èµ„è§„å®š",
        "å¥³èŒå·¥åŠ³åŠ¨ä¿æŠ¤ç‰¹åˆ«è§„å®š", "åŠ³åŠ¨ä¿éšœç›‘å¯Ÿæ¡ä¾‹",
        # æ°‘äº‹æ³•ç›¸å…³
        "æ°‘æ³•å…¸", "åˆåŒæ³•", "ç‰©æƒæ³•", "æ‹…ä¿æ³•", "ä¾µæƒè´£ä»»æ³•",
        # æ¶ˆè´¹è€…ä¿æŠ¤ç›¸å…³
        "æ¶ˆè´¹è€…æƒç›Šä¿æŠ¤æ³•", "äº§å“è´¨é‡æ³•", "ç”µå­å•†åŠ¡æ³•",
        # å…¶ä»–
        "ä¸ªäººä¿¡æ¯ä¿æŠ¤æ³•", "æ•°æ®å®‰å…¨æ³•", "åä¸æ­£å½“ç«äº‰æ³•",
        "å…¬å¸æ³•", "ä¿é™©æ³•", "æ‹›æ ‡æŠ•æ ‡æ³•", "æ”¿åºœé‡‡è´­æ³•",
    ]
    
    law_name_valid = False
    for law_name in law_names_found:
        # å»é™¤"ä¸­åäººæ°‘å…±å’Œå›½"å‰ç¼€
        clean_name = law_name.replace("ä¸­åäººæ°‘å…±å’Œå›½", "")
        if clean_name in valid_laws or any(v in clean_name for v in valid_laws):
            law_name_valid = True
            break
    
    if law_name_valid:
        # ========== å±‚çº§3ï¼šæ£€æŸ¥æ¡æ¬¾å·æ ¼å¼ ==========
        if articles_found:
            # éªŒè¯æ¡æ¬¾å·æ˜¯å¦åˆç†ï¼ˆ1-999æ¡ï¼‰
            for article in articles_found:
                try:
                    # è½¬æ¢ä¸­æ–‡æ•°å­—
                    cn_to_num = {
                        "ä¸€": 1, "äºŒ": 2, "ä¸‰": 3, "å››": 4, "äº”": 5,
                        "å…­": 6, "ä¸ƒ": 7, "å…«": 8, "ä¹": 9, "å": 10,
                        "åä¸€": 11, "åäºŒ": 12, "åä¸‰": 13, "åå››": 14, "åäº”": 15,
                        "äºŒå": 20, "ä¸‰å": 30, "å››å": 40, "äº”å": 50,
                        "å…­å": 60, "ä¸ƒå": 70, "å…«å": 80, "ä¹å": 90,
                        "ä¸€ç™¾": 100
                    }
                    if article in cn_to_num:
                        article_num = cn_to_num[article]
                    elif article.isdigit():
                        article_num = int(article)
                    else:
                        # å¤æ‚ä¸­æ–‡æ•°å­—ï¼ˆå¦‚"äºŒåä¸‰"ï¼‰æš‚æ—¶è·³è¿‡éªŒè¯
                        return (True, 3, f"æ¡æ¬¾å·æ ¼å¼æ­£ç¡®: ç¬¬{article}æ¡")
                    
                    if 1 <= article_num <= 999:
                        return (True, 3, f"æ¡æ¬¾å·æ ¼å¼æ­£ç¡®: ç¬¬{article}æ¡")
                except:
                    pass
            
            return (True, 3, "æ¡æ¬¾å·æ ¼å¼æ­£ç¡®ï¼ˆæœªéªŒè¯æ•°å€¼ï¼‰")
        else:
            return (True, 2, f"æ³•å¾‹åç§°æœ‰æ•ˆ: {law_names_found}")
    
    # ========== å±‚çº§4ï¼šå†…å®¹éªŒè¯ï¼ˆéœ€è¦æ³•å¾‹æ•°æ®åº“ï¼‰==========
    # æš‚æœªå®ç°ï¼Œé¢„ç•™æ¥å£
    
    # éªŒè¯å¤±è´¥
    if law_names_found:
        return (False, 0, f"æœªçŸ¥æ³•å¾‹åç§°: {law_names_found}")
    else:
        return (False, 0, f"æ— æ³•è¯†åˆ«æ³•å¾‹å¼•ç”¨: {law_reference[:50]}...")


# ============================================================================
# è¯„æµ‹æ‰§è¡Œå™¨
# ============================================================================

class AblationBenchmark:
    """æ¶ˆèå®éªŒè¯„æµ‹å™¨"""
    
    def __init__(self, mode: int, source: str = "local"):
        self.mode = mode
        self.config = load_config()
        
        # è¯»å–æ¶ˆèå®éªŒé…ç½®ï¼ˆå¤ç”¨æ··åˆæ£€ç´¢å’Œ Reranker çš„å…±äº«å‚æ•°ï¼‰
        ablation_cfg = self.config.get("ablation_benchmark_config", {})
        hallucination_cfg = ablation_cfg.get("hallucination_detection", {})
        reranker_cfg = self.config.get("reranker_config", {})
        hybrid_cfg = self.config.get("hybrid_search_config", {})
        
        # å¹»è§‰æ£€æµ‹é˜ˆå€¼ï¼šå¤ç”¨ reranker_config.threshold
        self.hallucination_threshold = reranker_cfg.get("threshold", 0.3)
        # å¬å› Top-Kï¼šå¤ç”¨ hybrid_search_config.top_k
        self.recall_top_k = hybrid_cfg.get("top_k", 3)
        # æ˜¯å¦ä½¿ç”¨ä¸¤é˜¶æ®µæ£€æµ‹
        self.use_two_stage = hallucination_cfg.get("use_two_stage", True)
        
        # åˆå§‹åŒ– LLM (ä½¿ç”¨ OllamaClient æ›¿ä»£ ChatOllama)
        if source == "local":
            llm_cfg = self.config.get("llm_config", {})
        else:
            llm_cfg = self.config.get("llm_cloud_config", {})
        
        self.llm = OllamaClient(
            base_url=llm_cfg.get("base_url", "http://localhost:11434"),
            model=llm_cfg.get("model_name", "qwen3:4b-instruct"),
            temperature=0,
        )
        
        # æ¨¡å¼3å’Œ4éœ€è¦è§„åˆ™å¼•æ“
        self.rule_engine = None
        if mode in [EvalMode.CURRENT_WORKFLOW, EvalMode.OPTIMIZED_WORKFLOW]:
            from src.core.rule_engine import RuleEngine
            self.rule_engine = RuleEngine()
        
        # åˆå§‹åŒ– Embedding æ¨¡å‹ï¼ˆç”¨äºè¯­ä¹‰ç›¸ä¼¼åº¦è¯„ä¼°ï¼Œæ›¿ä»£ LLM-as-a-Judgeï¼‰
        self.embedding_model = None
        try:
            from sentence_transformers import SentenceTransformer
            embedding_cfg = self.config.get("embedding_config", {})
            model_path = embedding_cfg.get("model_path", "BAAI/bge-small-zh-v1.5")
            self.embedding_model = SentenceTransformer(model_path)
            print(f"âœ… Embedding æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
        except Exception as e:
            print(f"âš ï¸ Embedding æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä»…ä½¿ç”¨å…³é”®è¯åŒ¹é…: {e}")
        
        # åˆå§‹åŒ– Reranker æ¨¡å‹ï¼ˆç”¨äºä¸¤é˜¶æ®µå¹»è§‰æ£€æµ‹ï¼‰
        self.reranker = None
        if self.use_two_stage:
            try:
                from src.core.reranker import get_reranker
                self.reranker = get_reranker()
                print(f"âœ… Reranker æ¨¡å‹åŠ è½½æˆåŠŸ (å¹»è§‰æ£€æµ‹é˜ˆå€¼: {self.hallucination_threshold})")
            except Exception as e:
                print(f"âš ï¸ Reranker æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä»…ä½¿ç”¨ Embedding ç›¸ä¼¼åº¦: {e}")
    
    def get_prompt(self, clause: str, reference_info: str = "") -> str:
        """æ ¹æ®æ¨¡å¼è·å–Prompt"""
        if self.mode == EvalMode.RAW_LLM:
            return RAW_LLM_PROMPT.format(clause=clause)
        elif self.mode == EvalMode.BASIC_PROMPT:
            return BASIC_PROMPT.format(clause=clause)
        elif self.mode == EvalMode.CURRENT_WORKFLOW:
            return CURRENT_WORKFLOW_PROMPT.format(clause=clause, reference_info=reference_info or "æ— ")
        elif self.mode == EvalMode.OPTIMIZED_WORKFLOW:
            return OPTIMIZED_WORKFLOW_PROMPT.format(clause=clause, reference_info=reference_info or "æ— ")
        else:
            return RAW_LLM_PROMPT.format(clause=clause)
    
    def get_reference_info(self, clause: str) -> tuple:
        """è·å–è§„åˆ™å¼•æ“çš„å‚è€ƒä¿¡æ¯ï¼ˆä½¿ç”¨ç»Ÿä¸€æ£€ç´¢å™¨ï¼‰
        
        Returns:
            tuple: (reference_info, law_contents, risk_ids, scores)
        """
        if self.rule_engine is None:
            return "", [], [], []
        
        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„æ£€ç´¢å™¨æ¨¡å—
            from src.core.reference_retriever import retrieve_reference
            result = retrieve_reference(clause)
            return result.reference_info, result.law_contents, result.risk_ids, result.scores
        except Exception as e:
            print(f"Reference retrieval error: {e}")
        
        return "", [], [], []
    
    async def analyze_clause(self, clause: str) -> tuple:
        """åˆ†æå•ä¸ªæ¡æ¬¾
        
        Returns:
            tuple: (ParsedResult, reference_info, risk_ids, scores, reflection_info)
        """
        # è·å–å‚è€ƒä¿¡æ¯ï¼ˆæ¨¡å¼3å’Œ4ä½¿ç”¨ Top-K æ£€ç´¢ï¼‰
        reference_info, law_contents, risk_ids, scores = self.get_reference_info(clause)
        
        # æ„å»ºPrompt
        prompt = self.get_prompt(clause, reference_info)
        
        reflection_info = None  # è‡ªåæ€ç»“æœ
        
        try:
            # ä½¿ç”¨ OllamaClient çš„ achat æ–¹æ³•
            content = await self.llm.achat(prompt)
            
            # è§£æè¾“å‡º
            result = parse_markdown_output(content)
            
            # æ³¨å…¥æ³•æ¡å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
            if law_contents and result.law_reference == "":
                result.law_reference = law_contents[0] if law_contents[0] else ""
            
            # ========== è‡ªåæ€æœºåˆ¶ï¼ˆä»… Mode 3 å’Œ Mode 4ï¼‰==========
            if self.mode in [EvalMode.CURRENT_WORKFLOW, EvalMode.OPTIMIZED_WORKFLOW]:
                result, reflection_info = await self._apply_self_reflection(clause, result)
            
            return result, reference_info, risk_ids, scores, reflection_info
            
        except Exception as e:
            print(f"LLM error: {e}")
            return ParsedResult(), reference_info, risk_ids, scores, None
    
    async def _apply_self_reflection(self, clause: str, initial_result: ParsedResult) -> tuple:
        """åº”ç”¨è‡ªåæ€æœºåˆ¶
        
        Args:
            clause: æ¡æ¬¾åŸæ–‡
            initial_result: åˆæ¬¡åˆ†æç»“æœ
        
        Returns:
            tuple: (è°ƒæ•´åçš„ ParsedResult, reflection_info dict)
        """
        reflection_info = {
            "applied": True,
            "initial_level": initial_result.risk_level,
            "final_level": initial_result.risk_level,
            "adjusted": False,
            "reason": ""
        }
        
        # æ„å»ºè‡ªåæ€ Prompt
        reflection_prompt = SELF_REFLECTION_PROMPT.format(
            clause_text=clause,
            risk_level=initial_result.risk_level or "æœªçŸ¥",
            risk_reason=initial_result.risk_name or "",
            evidence=initial_result.evidence or "æ— ",
            analysis=initial_result.analysis or ""
        )
        
        try:
            # è°ƒç”¨ LLM è¿›è¡Œåæ€
            reflection_content = await self.llm.achat(reflection_prompt)
            
            # è§£æåæ€ç»“æœ
            reflection_result = parse_reflection_output(reflection_content)
            reflection_info["reason"] = reflection_result.get("reason", "")
            
            # å¦‚æœç»“è®ºæ˜¯"è°ƒçº§"ï¼Œä¸”æœ‰æ–°çš„ç­‰çº§
            if reflection_result.get("conclusion") == "è°ƒçº§" and reflection_result.get("new_level"):
                new_level = reflection_result["new_level"]
                reflection_info["final_level"] = new_level
                reflection_info["adjusted"] = True
                
                # æ›´æ–° ParsedResult çš„é£é™©ç­‰çº§
                initial_result.risk_level = new_level
                
                # åœ¨åˆ†æä¸­æ·»åŠ è°ƒçº§è¯´æ˜
                adjustment_note = f"\n[äºŒå®¡è°ƒçº§: {reflection_info['initial_level']}â†’{new_level}ï¼Œç†ç”±: {reflection_info['reason']}]"
                initial_result.analysis = (initial_result.analysis or "") + adjustment_note
                
                print(f"  ğŸ”„ è‡ªåæ€è°ƒçº§: {reflection_info['initial_level']} â†’ {new_level}")
            else:
                print(f"  âœ“ è‡ªåæ€ç»´æŒ: {initial_result.risk_level}")
                
        except Exception as e:
            print(f"  âš ï¸ è‡ªåæ€å¤±è´¥: {e}")
            reflection_info["applied"] = False
        
        return initial_result, reflection_info
    
    def evaluate_reason(self, clause: str, gt_keywords: List[str], ai_reason: str) -> float:
        """ä½¿ç”¨ç®—æ³•+è¯­ä¹‰åŒ¹é…è¯„ä¼°è®ºè¯è´¨é‡ï¼ˆæ›¿ä»£ LLM-as-a-Judgeï¼‰
        
        è¯„åˆ†æ–¹æ³•ï¼š
        1. å…³é”®è¯åŒ¹é…ï¼ˆ60%æƒé‡ï¼‰ï¼šæ£€æŸ¥ AI ç†ç”±æ˜¯å¦åŒ…å« ground_truth å…³é”®è¯
        2. è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆ40%æƒé‡ï¼‰ï¼šä½¿ç”¨ Embedding è®¡ç®—å‘é‡ä½™å¼¦ç›¸ä¼¼åº¦
        
        Returns:
            float: 0.0-1.0 çš„åŒ¹é…åˆ†æ•°
        """
        if not ai_reason or not gt_keywords:
            return 0.0
        
        ai_reason_lower = ai_reason.lower()
        
        # 1. å…³é”®è¯ç²¾ç¡®åŒ¹é…
        matched_keywords = sum(1 for kw in gt_keywords if kw.lower() in ai_reason_lower)
        keyword_ratio = matched_keywords / len(gt_keywords) if gt_keywords else 0
        
        # 2. è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆä½¿ç”¨ Embeddingï¼‰
        semantic_sim = 0.0
        if self.embedding_model:
            try:
                from sklearn.metrics.pairwise import cosine_similarity
                gt_text = " ".join(gt_keywords)
                embeddings = self.embedding_model.encode([gt_text, ai_reason])
                semantic_sim = float(cosine_similarity([embeddings[0]], [embeddings[1]])[0][0])
            except Exception as e:
                print(f"è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
                semantic_sim = 0.0
        
        # 3. ç»¼åˆè¯„åˆ†ï¼ˆåŠ æƒæ±‚å’Œï¼‰
        alpha = 0.6  # å…³é”®è¯åŒ¹é…æƒé‡
        score = alpha * keyword_ratio + (1 - alpha) * semantic_sim
        
        return score
    
    async def evaluate_single(self, item: Dict[str, Any], metrics: EvalMetrics) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªæ ·æœ¬"""
        import time
        
        text = item.get("text", "")
        gt = item.get("ground_truth", {})
        original_data = item.get("original_data", {})  # LLM æ•°æ®é›†çš„åŸå§‹æ•°æ®
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # åˆ†ææ¡æ¬¾ï¼ˆè¿”å›å…ƒç»„ï¼šresult, reference_info, risk_ids, scores, reflection_infoï¼‰
        result, reference_info, matched_risk_ids, matched_scores, reflection_info = await self.analyze_clause(text)
        
        # è®°å½•å“åº”æ—¶é—´
        latency = time.time() - start_time
        metrics.total_latency += latency
        
        # è‡ªåæ€ç»Ÿè®¡
        if reflection_info and reflection_info.get("applied"):
            metrics.reflection_calls += 1
            if reflection_info.get("adjusted"):
                metrics.reflection_adjustments += 1
            else:
                metrics.reflection_maintain += 1
        
        metrics.total += 1
        
        # è§£ææˆåŠŸç‡
        if result.parse_success:
            metrics.parse_success += 1
        
        # é£é™©ç­‰çº§è¯„ä¼°ï¼ˆæ”¯æŒé«˜/ä¸­/ä½ä¸‰çº§ï¼‰
        gt_risk = gt.get("risk_level", "")
        pred_risk = result.risk_level
        
        # ç²¾ç¡®åŒ¹é…æˆ–å…¼å®¹åŒ¹é…
        is_risk_correct = (gt_risk == pred_risk) or (gt_risk in pred_risk)
        
        if is_risk_correct:
            metrics.correct_risk += 1
        
        # åŠ æƒè¯„åˆ†ï¼ˆç²¾ç¡®åŒ¹é…1åˆ†ï¼Œå·®ä¸€çº§0.5åˆ†ï¼Œå·®ä¸¤çº§0åˆ†ï¼‰
        weighted_score = EvalMetrics.calculate_weighted_score(gt_risk, pred_risk)
        metrics.total_weighted_score += weighted_score
        
        # æ··æ·†çŸ©é˜µæ›´æ–°ï¼ˆåŸºäº"æœ‰é£é™©"(é«˜/ä¸­) vs "æ— é£é™©"(ä½)çš„äºŒåˆ†ç±»ï¼‰
        # é«˜/ä¸­é£é™©è§†ä¸ºæ­£ä¾‹(Positive)ï¼Œä½é£é™©è§†ä¸ºè´Ÿä¾‹(Negative)
        gt_is_risky = gt_risk in ["é«˜", "ä¸­"]
        pred_is_risky = pred_risk in ["é«˜", "ä¸­"]
        
        if pred_is_risky and gt_is_risky:
            metrics.true_positive += 1
        elif pred_is_risky and not gt_is_risky:
            metrics.false_positive += 1
        elif not pred_is_risky and gt_is_risky:
            metrics.false_negative += 1
        else:
            metrics.true_negative += 1
            
        # [æ–°å¢] æ›´æ–°ä¸‰åˆ†ç±»æ··æ·†çŸ©é˜µ
        metrics.update_confusion_matrix(gt_risk, pred_risk)
        
        # risk_id åŒ¹é…è¯„ä¼°ï¼ˆé’ˆå¯¹ LLM ç”Ÿæˆçš„æ•°æ®é›†ï¼‰
        expected_risks = original_data.get("expected_risks", [])
        if expected_risks:
            # æœ‰é¢„æœŸé£é™©ï¼Œæ£€æŸ¥æ˜¯å¦æ­£ç¡®è¯†åˆ«
            metrics.risk_id_total += 1
            # å¦‚æœé¢„æµ‹ä¸ºæœ‰é£é™©ä¸”æ ·æœ¬ç¡®å®åŒ…å«é£é™©ï¼Œç®—åŒ¹é…æˆåŠŸ
            if pred_is_risky and gt_is_risky:
                metrics.risk_id_match += 1
        
        # ===== æ–¹æ³•ä¸€ï¼šè¯æ®ä¸€è‡´æ€§è¯„ä¼°ï¼ˆä¸¤é˜¶æ®µï¼šBGE-M3 + Rerankerï¼‰=====
        # åˆåŒæ¡æ¬¾è¯æ®éªŒè¯
        evidence_result = verify_evidence(
            result.evidence, text, 
            embedding_model=self.embedding_model,
            reranker=self.reranker,
            threshold=self.hallucination_threshold
        )
        clause_evidence_valid = evidence_result[0]  # is_valid
        evidence_similarity = evidence_result[1]    # similarity_score
        evidence_match_type = evidence_result[2]    # match_type
        
        if clause_evidence_valid:
            metrics.clause_evidence_valid += 1
            metrics.evidence_valid += 1  # ä¿æŒå…¼å®¹
        else:
            metrics.clause_evidence_invalid += 1
            metrics.evidence_invalid += 1  # ä¿æŒå…¼å®¹
        
        # 1b. æ³•å¾‹å¼•ç”¨éªŒè¯ï¼ˆåˆ†çº§éªŒè¯ï¼šreference_info â†’ ç™½åå• â†’ æ¡æ¬¾å·ï¼‰
        # æ³¨æ„ï¼šä½é£é™©æ ·æœ¬ä¸éœ€è¦æ³•å¾‹å¼•ç”¨ï¼Œè·³è¿‡éªŒè¯
        if gt_risk == "ä½":
            # ä½é£é™©æ ·æœ¬æ³•å¾‹å¼•ç”¨é»˜è®¤æœ‰æ•ˆ
            law_citation_valid = True
            law_validation_level = 0
            law_validation_detail = "ä½é£é™©æ ·æœ¬æ— éœ€æ³•å¾‹å¼•ç”¨"
            metrics.law_citation_valid += 1
        else:
            # é«˜/ä¸­é£é™©æ ·æœ¬éœ€è¦éªŒè¯æ³•å¾‹å¼•ç”¨
            law_result = verify_law_citation(result.law_reference, reference_info=reference_info)
            law_citation_valid = law_result[0]       # is_valid
            law_validation_level = law_result[1]     # validation_level
            law_validation_detail = law_result[2]    # detail
            
            if law_citation_valid:
                metrics.law_citation_valid += 1
            else:
                metrics.law_citation_invalid += 1
        
        # ===== æ–¹æ³•äºŒï¼šè§„åˆ™è§¦å‘ä¸€è‡´æ€§ï¼ˆRAG æ£€ç´¢å™¨è¯„æµ‹ï¼‰=====
        # ä»æµ‹è¯•ç”¨ä¾‹è·å–åº”è§¦å‘çš„è§„åˆ™ ID
        expected_rule_ids = set()
        if expected_risks:
            for risk in expected_risks:
                if isinstance(risk, dict) and risk.get("risk_id"):
                    expected_rule_ids.add(risk.get("risk_id"))
        
        # å®é™…è§¦å‘çš„è§„åˆ™ IDï¼ˆæ¥è‡ªæ£€ç´¢å™¨ ReferenceResult.risk_idsï¼‰
        triggered_rule_ids = set(matched_risk_ids) if matched_risk_ids else set()
        
        # è®¡ç®—æŒ‡æ ‡ï¼ˆç²¾ç¡®å­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if expected_rule_ids:
            metrics.rule_target_count += len(expected_rule_ids)          # åº”è§¦å‘
            metrics.rule_trigger_count += len(triggered_rule_ids)        # å®é™…è§¦å‘
            correct_triggers = expected_rule_ids & triggered_rule_ids    # äº¤é›†
            metrics.rule_correct_count += len(correct_triggers)          # æ­£ç¡®è§¦å‘

        
        # ===== æ–¹æ³•ä¸‰ï¼šä»»åŠ¡æˆåŠŸç‡è¯„ä¼° =====
        # ä»»åŠ¡æˆåŠŸéœ€åŒæ—¶æ»¡è¶³ï¼šè§£ææˆåŠŸ + é£é™©ç­‰çº§æ­£ç¡®(Â±1çº§) + è¯æ®æœ‰æ•ˆ + æœ‰å»ºè®®
        has_suggestion = bool(result.suggestion) and result.suggestion not in ["æ— ", "None", ""]
        is_risk_acceptable = weighted_score >= 0.5  # ç²¾ç¡®åŒ¹é…æˆ–å·®ä¸€çº§éƒ½ç®—å¯æ¥å—
        
        if result.parse_success and is_risk_acceptable and clause_evidence_valid and has_suggestion:
            metrics.task_success_count += 1
        
        # è®ºè¯è´¨é‡è¯„ä¼°ï¼ˆå¦‚æœæœ‰ reason_keywordsï¼‰
        reason_keywords = gt.get("reason_keywords", [])
        if reason_keywords:
            reason_score = self.evaluate_reason(text, reason_keywords, result.analysis)
            if reason_score >= 0.5:  # ä½¿ç”¨é˜ˆå€¼åˆ¤æ–­åŒ¹é…æˆåŠŸ
                metrics.correct_reason += 1
        
        return {
            "id": item.get("id"),
            "prediction": {
                "risk_level": result.risk_level,
                "evidence": result.evidence,
                "analysis": result.analysis,
                "parse_success": result.parse_success,
                "latency": round(latency, 3),
            },
            "ground_truth": gt,
            "correct_risk": is_risk_correct,
            "weighted_score": weighted_score,
            # è¯æ®éªŒè¯è¯¦æƒ…
            "evidence_valid": clause_evidence_valid,
            "evidence_similarity": round(evidence_similarity, 3),
            "evidence_match_type": evidence_match_type,
            # æ³•æ¡éªŒè¯è¯¦æƒ…
            "law_citation_valid": law_citation_valid,
            "law_validation_level": law_validation_level,
            "law_validation_detail": law_validation_detail,
            # ä»»åŠ¡æˆåŠŸ
            "task_success": result.parse_success and is_risk_acceptable and clause_evidence_valid and has_suggestion,
        }


def convert_llm_dataset_item(item: Dict[str, Any]) -> Dict[str, Any]:
    """
    å°† LLM ç”Ÿæˆçš„æ•°æ®é›†æ ¼å¼è½¬æ¢ä¸º benchmark æœŸæœ›çš„æ ¼å¼
    
    æ–°æ ¼å¼ (399æ ·æœ¬):
    {
        "id": "LABOR_001_high_positive_1",
        "contract_text": "...",
        "expected_risks": [{"risk_id": "...", "risk_level": "high", ...}],
        "case_type": "high_positive/medium_positive/negative",
        "source_domain": "LABOR"
    }
    
    æ—§æ ¼å¼ (å…¼å®¹):
    {
        "id": "GENERAL_001_pos_1",
        "contract_text": "...",
        "expected_risks": [...],
        "case_type": "positive/negative/boundary"
    }
    
    Benchmark æ ¼å¼:
    {
        "id": "...",
        "text": "...",
        "ground_truth": {"risk_level": "é«˜/ä¸­/ä½", "reason_keywords": [...]},
        "original_data": {...}  # ä¿ç•™åŸå§‹æ•°æ®ç”¨äº risk_id åŒ¹é…
    }
    """
    # æ£€æµ‹æ˜¯å¦ä¸º LLM æ•°æ®é›†æ ¼å¼
    if "contract_text" in item:
        expected_risks = item.get("expected_risks", [])
        case_type = item.get("case_type", "")
        
        # æ ¹æ® case_type ç¡®å®šé£é™©ç­‰çº§
        if case_type == "high_positive":
            risk_level = "é«˜"
        elif case_type == "medium_positive":
            risk_level = "ä¸­"
        else:  # negative æˆ–å…¶ä»–
            risk_level = "ä½"
        
        # æå–å…³é”®è¯ä½œä¸º reason_keywords
        reason_keywords = []
        for risk in expected_risks:
            if isinstance(risk, dict) and risk.get("risk_name"):
                reason_keywords.append(risk["risk_name"])
        
        return {
            "id": item.get("id", ""),
            "text": item.get("contract_text", ""),
            "ground_truth": {
                "risk_level": risk_level,
                "reason_keywords": reason_keywords,
            },
            "original_data": item,  # ä¿ç•™åŸå§‹æ•°æ®ç”¨äº risk_id åŒ¹é…
            "source_domain": item.get("source_domain", ""),  # ä¿ç•™ source_domain
        }
    
    # å·²ç»æ˜¯æ—§æ ¼å¼ï¼Œç›´æ¥è¿”å›
    return item



async def run_ablation_benchmark(
    data_path: str = None,
    mode: int = 2,
    limit: int = None,
    source: str = "local",
    log_callback=None,
    dataset: List[Dict[str, Any]] = None  # æ”¯æŒä¼ å…¥é¢„åŠ è½½çš„æ•°æ®é›†
) -> Dict[str, Any]:
    """
    è¿è¡Œæ¶ˆèå®éªŒè¯„æµ‹
    
    Args:
        data_path: æ•°æ®é›†è·¯å¾„ (å½“ dataset ä¸º None æ—¶å¿…é¡»æä¾›)
        mode: è¯„æµ‹æ¨¡å¼ (1-4)
        limit: æ ·æœ¬æ•°é‡é™åˆ¶ (ä»…å½“ dataset ä¸º None æ—¶ç”Ÿæ•ˆ)
        source: LLM æ¥æº (local/cloud)
        log_callback: æ—¥å¿—å›è°ƒå‡½æ•°
        dataset: é¢„åŠ è½½çš„æ•°æ®é›† (ç”¨äºæ¶ˆèå®éªŒä¸­æ§åˆ¶å˜é‡)
    """
    
    def log(msg):
        print(msg)
        if log_callback:
            log_callback(msg)
    
    log(f"\n{'='*60}")
    log(f"ğŸ§ª æ¶ˆèå®éªŒè¯„æµ‹ - æ¨¡å¼ {mode}: {EvalMode.name(mode)}")
    log(f"{'='*60}")
    
    # å¦‚æœæ²¡æœ‰ä¼ å…¥æ•°æ®é›†ï¼Œåˆ™ä»æ–‡ä»¶åŠ è½½
    if dataset is None:
        if not data_path or not os.path.exists(data_path):
            log(f"Error: Data file not found at {data_path}")
            return None
        
        dataset = []
        with open(data_path, "r", encoding="utf-8") as f:
            content = f.read().strip()
            
            # æ£€æµ‹æ–‡ä»¶æ ¼å¼ï¼šJSON æ•°ç»„ vs JSONL
            if content.startswith("["):
                # JSON æ•°ç»„æ ¼å¼
                raw_items = json.loads(content)
                for raw_item in raw_items:
                    converted_item = convert_llm_dataset_item(raw_item)
                    dataset.append(converted_item)
            else:
                # JSONL æ ¼å¼ï¼ˆé€è¡Œè§£æï¼‰
                for line in content.split("\n"):
                    if line.strip():
                        raw_item = json.loads(line)
                        converted_item = convert_llm_dataset_item(raw_item)
                        dataset.append(converted_item)
        
        # ä»…åœ¨ç‹¬ç«‹è¿è¡Œæ—¶è¿›è¡Œéšæœºé‡‡æ ·
        total_samples = len(dataset)
        if limit and limit < total_samples:
            import random
            dataset = random.sample(dataset, limit)
            log(f"ğŸ“Š éšæœºé‡‡æ · {limit} æ¡ï¼ˆå…± {total_samples} æ¡å¯ç”¨ï¼‰")
    
    log(f"ğŸ“Š å®é™…è¯„æµ‹æ ·æœ¬æ•°: {len(dataset)}")
    
    # ç»Ÿè®¡æ•°æ®é›†ä¿¡æ¯ï¼ˆé«˜/ä¸­/ä½ä¸‰çº§é£é™©ï¼‰
    high_count = sum(1 for d in dataset if d.get("ground_truth", {}).get("risk_level") == "é«˜")
    medium_count = sum(1 for d in dataset if d.get("ground_truth", {}).get("risk_level") == "ä¸­")
    low_count = sum(1 for d in dataset if d.get("ground_truth", {}).get("risk_level") == "ä½")
    log(f"ğŸ“Š é«˜é£é™©: {high_count}, ä¸­é£é™©: {medium_count}, ä½é£é™©: {low_count}")
    
    # åˆå§‹åŒ–è¯„æµ‹å™¨
    benchmark = AblationBenchmark(mode=mode, source=source)
    metrics = EvalMetrics()
    results = []
    
    # é€ä¸ªè¯„æµ‹
    for i, item in enumerate(dataset):
        log(f"è¯„æµ‹è¿›åº¦: {i+1}/{len(dataset)} - {item.get('id', 'unknown')}")
        result = await benchmark.evaluate_single(item, metrics)
        results.append(result)
    
    # è¾“å‡ºæŠ¥å‘Š
    log(f"\n{'='*60}")
    log(f"ğŸ“ˆ è¯„æµ‹ç»“æœ - æ¨¡å¼ {mode}: {EvalMode.name(mode)}")
    log(f"{'='*60}")
    
    metrics_dict = metrics.to_dict()
    for key, value in metrics_dict.items():
        if isinstance(value, float):
            # latency ä½¿ç”¨ç§’æ•°æ ¼å¼ï¼Œå…¶ä»–ä½¿ç”¨ç™¾åˆ†æ¯”æ ¼å¼
            if "latency" in key:
                log(f"  {key}: {value:.3f}s")
            else:
                log(f"  {key}: {value:.2%}")
        else:
            log(f"  {key}: {value}")
    
    return {
        "mode": mode,
        "mode_name": EvalMode.name(mode),
        "metrics": metrics_dict,
        "results": results,
    }


async def run_full_ablation_study(
    data_path: str,
    modes: List[int] = None,
    limit: int = None,
    source: str = "local"
) -> Dict[str, Any]:
    """è¿è¡Œå®Œæ•´æ¶ˆèå®éªŒï¼ˆæ‰€æœ‰æ¨¡å¼å¯¹æ¯”ï¼Œä½¿ç”¨ç›¸åŒæ ·æœ¬æ§åˆ¶å˜é‡ï¼‰"""
    
    if modes is None:
        modes = [1, 2, 3, 4]  # é»˜è®¤è¿è¡Œæ‰€æœ‰æ¨¡å¼
    
    print("\n" + "="*70)
    print("ğŸ”¬ æ¶ˆèå®éªŒ (Ablation Study) - å¤šæ¨¡å¼å¯¹æ¯”è¯„æµ‹")
    print("="*70)
    
    # ========== ç»Ÿä¸€åŠ è½½å’Œé‡‡æ ·æ•°æ®ï¼ˆæ§åˆ¶å˜é‡ï¼‰ ==========
    if not os.path.exists(data_path):
        print(f"Error: Data file not found at {data_path}")
        return {}
    
    dataset = []
    with open(data_path, "r", encoding="utf-8") as f:
        content = f.read().strip()
        
        # æ£€æµ‹æ–‡ä»¶æ ¼å¼ï¼šJSON æ•°ç»„ vs JSONL
        if content.startswith("["):
            # JSON æ•°ç»„æ ¼å¼
            raw_items = json.loads(content)
        else:
            # JSONL æ ¼å¼ï¼ˆé€è¡Œè§£æï¼‰
            raw_items = [json.loads(line) for line in content.split("\n") if line.strip()]
        
        for raw_item in raw_items:
            converted_item = convert_llm_dataset_item(raw_item)
            dataset.append(converted_item)
    
    total_samples = len(dataset)
    if limit and limit < total_samples:
        # ä½¿ç”¨åˆ†å±‚é‡‡æ ·ï¼Œç¡®ä¿ High/Medium/Low å‡è¡¡ (1:1:1)
        dataset = stratified_sample(dataset, limit)
        print(f"ğŸ“Š ä½¿ç”¨åˆ†å±‚é‡‡æ · (Stratified Sampling) æŠ½å– {limit} æ¡ï¼ˆHigh/Medium/Low 1:1:1ï¼‰")
    
    print(f"ğŸ“Š æ‰€æœ‰æ¨¡å¼å°†ä½¿ç”¨ç›¸åŒçš„ {len(dataset)} æ¡æ ·æœ¬")
    
    # æ‰“å°æ ·æœ¬ ID ä»¥ä¾¿éªŒè¯
    sample_ids = [item.get("id", "unknown") for item in dataset[:5]]
    print(f"ğŸ“Š æ ·æœ¬ ID é¢„è§ˆ: {sample_ids}{'...' if len(dataset) > 5 else ''}")
    
    all_results = {}
    
    # ========== å¯¹æ¯ä¸ªæ¨¡å¼è¿è¡Œè¯„æµ‹ï¼ˆä¼ å…¥ç›¸åŒæ•°æ®é›†ï¼‰ ==========
    for mode in modes:
        result = await run_ablation_benchmark(
            mode=mode,
            source=source,
            dataset=dataset  # ä¼ å…¥é¢„åŠ è½½çš„æ•°æ®é›†
        )
        if result:
            all_results[f"mode_{mode}"] = result
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print("\n" + "="*70)
    print("ğŸ“Š æ¨¡å¼å¯¹æ¯”æ±‡æ€»")
    print("="*70)
    
    # è¡¨å¤´
    print(f"{'æŒ‡æ ‡':<20}", end="")
    for mode in modes:
        print(f"{EvalMode.name(mode):<18}", end="")
    print()
    print("-" * (20 + 18 * len(modes)))
    
    # æŒ‡æ ‡å¯¹æ¯” (åŒ…å«æ–°å¢çš„è¯„ä¼°æŒ‡æ ‡)
    metric_keys = [
        "accuracy", "weighted_accuracy", "f1", "precision", "recall", "parse_rate",
        "hallucination_rate", "clause_hallucination_rate", "law_hallucination_rate",
        "rule_recall", "rule_precision", "task_success_rate", "avg_latency_sec"
    ]
    for key in metric_keys:
        print(f"{key:<25}", end="")
        for mode in modes:
            mode_key = f"mode_{mode}"
            if mode_key in all_results:
                value = all_results[mode_key]["metrics"].get(key, 0)
                # latency ä½¿ç”¨ç§’æ•°æ ¼å¼
                if "latency" in key:
                    print(f"{value:.3f}s".ljust(15), end="")
                else:
                    print(f"{value:.2%}".ljust(15), end="")
            else:
                print(f"{'N/A':<15}", end="")
        print()
    
    # ========== åˆ›å»ºç‹¬ç«‹è¾“å‡ºç›®å½• ==========
    script_dir = Path(__file__).parent
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = script_dir / f"results_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜ç»“æœ JSON
    output_path = output_dir / "ablation_results.json"
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {output_dir}")
    
    # ========== ç”Ÿæˆä¸¤å¼ å›¾è¡¨ï¼ˆåŸºç¡€æŒ‡æ ‡ + é«˜çº§æŒ‡æ ‡ï¼‰==========    # ç”Ÿæˆå›¾è¡¨
    try:
        from evaluation.chart_generator import generate_report_charts
        
        # ç»“æœä¿å­˜ç›®å½•
        output_dir = Path(output_path).parent
        
        # ç”Ÿæˆå›¾è¡¨
        chart_paths = generate_report_charts(all_results, output_dir, timestamp)
        all_results["chart_paths"] = chart_paths
        all_results["timestamp"] = timestamp
        
        # æ›´æ–°ä¿å­˜çš„ JSONï¼ˆåŒ…å«å›¾è¡¨è·¯å¾„ï¼‰
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š å·²ç”Ÿæˆ {len(chart_paths)} å¼ å›¾è¡¨")
    except ImportError as e:
        print(f"âš ï¸ å›¾è¡¨ç”Ÿæˆå¤±è´¥ï¼ˆè¯·ç¡®ä¿å®‰è£… matplotlibï¼‰: {e}")
    except Exception as e:
        print(f"âš ï¸ å›¾è¡¨ç”Ÿæˆå¼‚å¸¸: {e}")
    
    return all_results


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="æ¶ˆèå®éªŒè¯„æµ‹è„šæœ¬")
    parser.add_argument("--data", type=str, default="evaluation/llm_benchmark_dataset.json",
                        help="è¯„æµ‹æ•°æ®é›†è·¯å¾„ (æ”¯æŒæ–°æ—§ä¸¤ç§æ ¼å¼)")
    parser.add_argument("--mode", type=int, choices=[1, 2, 3, 4], default=None,
                        help="è¯„æµ‹æ¨¡å¼ (1-4)ï¼Œä¸æŒ‡å®šåˆ™è¿è¡Œæ‰€æœ‰æ¨¡å¼")
    parser.add_argument("--limit", type=int, default=None,
                        help="é™åˆ¶è¯„æµ‹æ ·æœ¬æ•°é‡")
    parser.add_argument("--source", type=str, choices=["local", "cloud"], default="local",
                        help="LLMæ¥æº (local/cloud)")
    
    args = parser.parse_args()
    
    if args.mode:
        # è¿è¡Œå•ä¸ªæ¨¡å¼
        asyncio.run(run_ablation_benchmark(
            data_path=args.data,
            mode=args.mode,
            limit=args.limit,
            source=args.source
        ))
    else:
        # è¿è¡Œå®Œæ•´æ¶ˆèå®éªŒ
        asyncio.run(run_full_ablation_study(
            data_path=args.data,
            limit=args.limit,
            source=args.source
        ))
