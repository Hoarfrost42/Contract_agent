"""
æ¶ˆèå®éªŒè¯„æµ‹è„šæœ¬ (Ablation Study Benchmark)

æ”¯æŒ4ç§è¯„æµ‹æ¨¡å¼å¯¹æ¯”ï¼š
- Mode 1: çº¯LLMï¼ˆæ— Promptæ¨¡æ¿ï¼Œç›´æ¥è¾“å…¥æ¡æ¬¾ï¼‰
- Mode 2: åŸºç¡€Promptï¼ˆæœ‰æ ¼å¼åŒ–Promptï¼Œæ— è§„åˆ™å¼•æ“ï¼‰
- Mode 3: å½“å‰å·¥ä½œæµï¼ˆPrompt + è§„åˆ™å¼•æ“ï¼‰
- Mode 4: ä¼˜åŒ–å·¥ä½œæµï¼ˆæ”¹è¿›Prompt + è§„åˆ™å¼•æ“ï¼‰

è¯„æµ‹æŒ‡æ ‡ï¼š
- é£é™©ç­‰çº§å‡†ç¡®ç‡ (Accuracy)
- F1 åˆ†æ•° (Precision/Recall å¹³è¡¡)
- Jaccard ç›¸ä¼¼åº¦
- å¹»è§‰ç‡ (å¼•ç”¨éªŒè¯å¤±è´¥æ¯”ä¾‹)
- è§£ææˆåŠŸç‡
"""

import argparse
import asyncio
import json
import os
import sys
import re
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict

# Add project root to sys.path
sys.path.append(str(Path(__file__).resolve().parents[1]))

import requests
from src.utils.config_loader import load_config


# ============================================================================
# Ollama å®¢æˆ·ç«¯ (ä½¿ç”¨ requests æ›¿ä»£ httpxï¼Œè§£å†³ 502 å…¼å®¹æ€§é—®é¢˜)
# ============================================================================

class OllamaClient:
    """ç›´æ¥ä½¿ç”¨ requests è°ƒç”¨ Ollama APIï¼Œé¿å… httpx å…¼å®¹æ€§é—®é¢˜"""
    
    def __init__(self, base_url: str, model: str, temperature: float = 0):
        self.base_url = base_url.rstrip('/')
        self.model = model
        self.temperature = temperature
    
    def chat(self, prompt: str, timeout: int = 120) -> str:
        """å‘é€èŠå¤©è¯·æ±‚"""
        try:
            response = requests.post(
                f"{self.base_url}/api/chat",
                json={
                    "model": self.model,
                    "messages": [{"role": "user", "content": prompt}],
                    "stream": False,
                    "options": {"temperature": self.temperature}
                },
                timeout=timeout
            )
            response.raise_for_status()
            return response.json().get("message", {}).get("content", "")
        except requests.RequestException as e:
            raise RuntimeError(f"Ollama API error: {e}")
    
    async def achat(self, prompt: str, timeout: int = 120) -> str:
        """å¼‚æ­¥ç‰ˆæœ¬ï¼ˆå®é™…ä½¿ç”¨åŒæ­¥è°ƒç”¨ï¼Œå› ä¸º asyncio ç¯å¢ƒä¸‹å¯ä»¥ç”¨ run_in_executorï¼‰"""
        import asyncio
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, lambda: self.chat(prompt, timeout))


# ============================================================================
# è¯„æµ‹æ¨¡å¼å®šä¹‰
# ============================================================================

class EvalMode:
    """è¯„æµ‹æ¨¡å¼æšä¸¾"""
    RAW_LLM = 1           # çº¯LLMï¼Œæ— Promptæ¨¡æ¿
    BASIC_PROMPT = 2      # åŸºç¡€Promptï¼Œæ— è§„åˆ™å¼•æ“
    CURRENT_WORKFLOW = 3  # å½“å‰å·¥ä½œæµï¼ˆPrompt + è§„åˆ™å¼•æ“ï¼‰
    OPTIMIZED_WORKFLOW = 4  # ä¼˜åŒ–å·¥ä½œæµï¼ˆæ”¹è¿›Prompt + è§„åˆ™å¼•æ“ï¼‰
    
    @staticmethod
    def name(mode: int) -> str:
        names = {
            1: "çº¯LLM (Raw)",
            2: "åŸºç¡€Prompt",
            3: "å½“å‰å·¥ä½œæµ",
            4: "ä¼˜åŒ–å·¥ä½œæµ"
        }
        return names.get(mode, "æœªçŸ¥")


# ============================================================================
# Prompt æ¨¡æ¿å®šä¹‰
# ============================================================================

# æ¨¡å¼1ï¼šçº¯LLMï¼Œæœ€ç®€å•çš„è¾“å…¥
RAW_LLM_PROMPT = """è¯·åˆ†æä»¥ä¸‹åˆåŒæ¡æ¬¾æ˜¯å¦å­˜åœ¨é£é™©ï¼š

{clause}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
## é£é™©ï¼š[é£é™©ç®€è¿°]
- **ç­‰çº§**ï¼š[é«˜/ä½]
- **è¯æ®**ï¼š[è¯æ®æ‘˜å½•]
- **åˆ†æ**ï¼š[åˆ†æå†…å®¹]
- **æ³•æ¡**ï¼š[ç›¸å…³æ³•æ¡]
- **å»ºè®®**ï¼š[ä¿®æ”¹å»ºè®®]
"""

# æ¨¡å¼2ï¼šåŸºç¡€Promptï¼Œæœ‰æ ¼å¼çº¦æŸå’Œè§’è‰²å®šä¹‰ï¼Œä½†æ— è§„åˆ™å¼•æ“
BASIC_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åˆåŒæ³•å¾‹é¡¾é—®ï¼Œè¯·åˆ†æä»¥ä¸‹åˆåŒæ¡æ¬¾æ˜¯å¦å­˜åœ¨é£é™©ã€‚

### æ¡æ¬¾åŸæ–‡
{clause}

### è¾“å‡ºè¦æ±‚
è¯·æŒ‰ä»¥ä¸‹Markdownæ ¼å¼è¾“å‡ºï¼ˆä¸è¦åŒ…å«```markdownæ ‡è®°ï¼‰ï¼š

## é£é™©ï¼š[é£é™©ç®€è¿°]
- **ç­‰çº§**ï¼š[é«˜/ä½] (ä»…é™äºŒé€‰ä¸€)
- **è¯æ®**ï¼š[ä»æ¡æ¬¾åŸæ–‡ä¸­é€å­—æ‘˜å½•èƒ½è¯æ˜è¯¥é£é™©çš„å…³é”®è¯­å¥ï¼Œç”¨ã€Œã€æ‹¬èµ·æ¥]
- **åˆ†æ**ï¼š[è¯¦ç»†åˆ†æï¼Œ100å­—ä»¥å†…]
- **æ³•æ¡**ï¼š[ç›¸å…³æ³•æ¡ï¼Œè‹¥æ— åˆ™ç•™ç©º]
- **å»ºè®®**ï¼š[é’ˆå¯¹æ€§çš„ä¿®æ”¹å»ºè®®]
"""

# æ¨¡å¼3ï¼šå½“å‰å·¥ä½œæµPromptï¼ˆä»llm.pyå¤åˆ¶ï¼‰
CURRENT_WORKFLOW_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åˆåŒæ³•å¾‹é¡¾é—®ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯åˆ†ææ¡æ¬¾é£é™©ã€‚

### ğŸ“Œ æ¡æ¬¾åŸæ–‡
{clause}

### ğŸ“š å‚è€ƒä¿¡æ¯ (å·²åŒ¹é…é£é™©åº“)
{reference_info}

### ğŸ“ å†™ä½œæŒ‡ä»¤
1. **ã€ç›¸å…³æ€§å¼ºåˆ¶åˆ¤æ–­è§„åˆ™ã€‘**
   è¿›è¡Œåˆ†æå‰å¿…é¡»å…ˆåˆ¤æ–­æ¡æ¬¾æ˜¯å¦åŒ…å«å‚è€ƒä¿¡æ¯ä¸­é£é™©ç‚¹çš„"å…³é”®åŠ¨ä½œæˆ–å…¸å‹æªè¾"ã€‚
   è‹¥æ¡æ¬¾æœªå‡ºç°æ ¸å¿ƒåŠ¨è¯ï¼ˆå¦‚"ä¿®æ”¹ã€å˜æ›´ã€è§£é™¤ã€ç»ˆæ­¢"ç­‰ï¼‰æˆ–ä¸»ä½“ç»“æ„ï¼ˆå¦‚"å•æ–¹/ç”²æ–¹æœ‰æƒ"ç­‰ï¼‰ï¼Œ
   åˆ™è§†ä¸º"ä¸ç›¸å…³"ï¼Œå¿…é¡»åˆ¤å®šä¸ºä½é£é™©ã€‚

2. **è‹¥å‚è€ƒä¿¡æ¯ç›¸å…³æ€§æˆç«‹**ï¼šæ‰©å†™å‚è€ƒä¿¡æ¯çš„ä¸“å®¶é€»è¾‘ï¼Œä¿ç•™æ³•å¾‹ä¾æ®ã€‚
3. **è‹¥æ— å‚è€ƒä¿¡æ¯æˆ–æœªå‘½ä¸­**ï¼šåŸºäºå…¬å¹³åŸåˆ™ç®€è¦åˆ†æã€‚

è¯·æŒ‰ä»¥ä¸‹Markdownæ ¼å¼è¾“å‡ºï¼š

## é£é™©ï¼š[é£é™©ç®€è¿°]
- **ç­‰çº§**ï¼š[é«˜/ä½] (ä»…é™äºŒé€‰ä¸€)
- **è¯æ®**ï¼š[ä»æ¡æ¬¾åŸæ–‡ä¸­é€å­—æ‘˜å½•ï¼Œç”¨ã€Œã€æ‹¬èµ·æ¥]
- **åˆ†æ**ï¼š[è¯¦ç»†åˆ†æï¼Œ100å­—ä»¥å†…]
- **æ³•æ¡**ï¼š[æ³•å¾‹ä¾æ®]
- **å»ºè®®**ï¼š[ä¿®æ”¹å»ºè®®]
"""

# æ¨¡å¼4ï¼šä¼˜åŒ–å·¥ä½œæµPromptï¼ˆåŠ å…¥CoTåˆ†æ­¥æ¨ç†ï¼‰
OPTIMIZED_WORKFLOW_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åˆåŒæ³•å¾‹é¡¾é—®ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤åˆ†ææ¡æ¬¾é£é™©ã€‚

### ğŸ“Œ æ¡æ¬¾åŸæ–‡
{clause}

### ğŸ“š å‚è€ƒä¿¡æ¯ (å·²åŒ¹é…é£é™©åº“)
{reference_info}

### ğŸ” åˆ†ææ­¥éª¤ï¼ˆChain-of-Thoughtï¼‰

**ç¬¬ä¸€æ­¥ï¼šå…³é”®è¦ç´ è¯†åˆ«**
- è¯†åˆ«ä¸»ä½“ï¼šç”²æ–¹/ä¹™æ–¹çš„æƒåˆ©ä¹‰åŠ¡
- è¯†åˆ«åŠ¨ä½œï¼šæƒåˆ©/ä¹‰åŠ¡/é™åˆ¶/ç¦æ­¢
- è¯†åˆ«æ•°å­—ï¼šé‡‘é¢/æœŸé™/æ¯”ä¾‹

**ç¬¬äºŒæ­¥ï¼šä¸å‚è€ƒä¿¡æ¯å¯¹ç…§**
- æ£€æŸ¥æ¡æ¬¾æ˜¯å¦åŒ…å«å‚è€ƒä¿¡æ¯ä¸­çš„æ ¸å¿ƒåŠ¨è¯æˆ–é£é™©æªè¾
- è‹¥ä¸åŒ…å«ï¼Œç›´æ¥åˆ¤å®šä¸ºä½é£é™©
- è‹¥åŒ…å«ï¼Œè¿›å…¥æ·±åº¦åˆ†æ

**ç¬¬ä¸‰æ­¥ï¼šé£é™©è¯„ä¼°**
- è¯„ä¼°æƒåˆ©æ˜¯å¦å¯¹ç­‰
- è¯„ä¼°æ˜¯å¦è¿åæ³•å¾‹å¼ºåˆ¶æ€§è§„å®š
- ç»¼åˆç»™å‡ºé£é™©ç­‰çº§

è¯·æŒ‰ä»¥ä¸‹Markdownæ ¼å¼è¾“å‡ºï¼š

## é£é™©ï¼š[é£é™©ç®€è¿°]
- **ç­‰çº§**ï¼š[é«˜/ä½] (ä»…é™äºŒé€‰ä¸€)
- **è¯æ®**ï¼š[ä»æ¡æ¬¾åŸæ–‡ä¸­é€å­—æ‘˜å½•ï¼Œç”¨ã€Œã€æ‹¬èµ·æ¥]
- **åˆ†æ**ï¼š[è¯¦ç»†åˆ†æï¼Œ100å­—ä»¥å†…]
- **æ³•æ¡**ï¼š[æ³•å¾‹ä¾æ®]
- **å»ºè®®**ï¼š[ä¿®æ”¹å»ºè®®]
"""


# ============================================================================
# è§£æå™¨
# ============================================================================

@dataclass
class ParsedResult:
    """è§£æåçš„åˆ†æç»“æœ"""
    risk_level: str = "æœªçŸ¥"
    evidence: str = ""
    analysis: str = ""
    law_reference: str = ""
    suggestion: str = ""
    parse_success: bool = False
    raw_output: str = ""


def parse_markdown_output(content: str) -> ParsedResult:
    """è§£æLLMçš„Markdownè¾“å‡º"""
    result = ParsedResult(raw_output=content)
    
    if not content:
        return result
    
    try:
        # è§£æé£é™©ç­‰çº§
        level_patterns = [
            r"\*\*ç­‰çº§\*\*[ï¼š:]\s*(é«˜|ä½)",
            r"ç­‰çº§[ï¼š:]\s*(é«˜|ä½)",
            r"\*\*(é«˜|ä½)\*\*"
        ]
        for pattern in level_patterns:
            match = re.search(pattern, content)
            if match:
                result.risk_level = match.group(1)
                break
        
        # è§£æè¯æ®
        evidence_patterns = [
            r"\*\*è¯æ®\*\*[ï¼š:]\s*(.*?)(?=\n\s*-|\n\s*\*\*|$)",
            r"è¯æ®[ï¼š:]\s*ã€Œ(.+?)ã€"
        ]
        for pattern in evidence_patterns:
            match = re.search(pattern, content, re.DOTALL)
            if match:
                result.evidence = match.group(1).strip()
                break
        
        # è§£æåˆ†æ
        analysis_patterns = [
            r"\*\*åˆ†æ\*\*[ï¼š:]\s*(.*?)(?=\n\s*-|\n\s*\*\*|$)",
        ]
        for pattern in analysis_patterns:
            match = re.search(pattern, content, re.DOTALL)
            if match:
                result.analysis = match.group(1).strip()
                break
        
        # è§£ææ³•æ¡
        law_patterns = [
            r"\*\*æ³•æ¡\*\*[ï¼š:]\s*(.*?)(?=\n\s*-|\n\s*\*\*|$)",
        ]
        for pattern in law_patterns:
            match = re.search(pattern, content, re.DOTALL)
            if match:
                result.law_reference = match.group(1).strip()
                break
        
        # è§£æå»ºè®®
        suggestion_patterns = [
            r"\*\*å»ºè®®\*\*[ï¼š:]\s*(.*?)(?=\n\s*-|\n\s*\*\*|---|$)",
        ]
        for pattern in suggestion_patterns:
            match = re.search(pattern, content, re.DOTALL)
            if match:
                result.suggestion = match.group(1).strip()
                break
        
        # åˆ¤æ–­è§£ææ˜¯å¦æˆåŠŸï¼ˆè‡³å°‘æœ‰é£é™©ç­‰çº§ï¼‰
        result.parse_success = result.risk_level in ["é«˜", "ä½"]
        
    except Exception as e:
        print(f"Parse error: {e}")
    
    return result


# ============================================================================
# è¯„ä¼°å™¨
# ============================================================================

@dataclass
class EvalMetrics:
    """è¯„ä¼°æŒ‡æ ‡"""
    # åŸºç¡€æŒ‡æ ‡
    total: int = 0
    correct_risk: int = 0
    correct_reason: int = 0
    parse_success: int = 0
    
    # æ··æ·†çŸ©é˜µï¼ˆç”¨äº Precision/Recall/F1ï¼‰
    true_positive: int = 0   # é¢„æµ‹é«˜é£é™©ï¼Œå®é™…é«˜é£é™©
    false_positive: int = 0  # é¢„æµ‹é«˜é£é™©ï¼Œå®é™…ä½é£é™©
    false_negative: int = 0  # é¢„æµ‹ä½é£é™©ï¼Œå®é™…é«˜é£é™©
    true_negative: int = 0   # é¢„æµ‹ä½é£é™©ï¼Œå®é™…ä½é£é™©
    
    # å¹»è§‰æ£€æµ‹
    evidence_valid: int = 0
    evidence_invalid: int = 0
    
    # æ–°å¢ï¼šrisk_id åŒ¹é…ï¼ˆå¤šæ ‡ç­¾åœºæ™¯ï¼‰
    risk_id_match: int = 0
    risk_id_total: int = 0
    
    # æ–°å¢ï¼šå“åº”æ—¶é—´ç»Ÿè®¡
    total_latency: float = 0.0
    
    def accuracy(self) -> float:
        return self.correct_risk / self.total if self.total > 0 else 0
    
    def precision(self) -> float:
        denom = self.true_positive + self.false_positive
        return self.true_positive / denom if denom > 0 else 0
    
    def recall(self) -> float:
        denom = self.true_positive + self.false_negative
        return self.true_positive / denom if denom > 0 else 0
    
    def f1(self) -> float:
        p, r = self.precision(), self.recall()
        return 2 * p * r / (p + r) if (p + r) > 0 else 0
    
    def parse_rate(self) -> float:
        return self.parse_success / self.total if self.total > 0 else 0
    
    def hallucination_rate(self) -> float:
        total_evidence = self.evidence_valid + self.evidence_invalid
        return self.evidence_invalid / total_evidence if total_evidence > 0 else 0
    
    def risk_id_accuracy(self) -> float:
        return self.risk_id_match / self.risk_id_total if self.risk_id_total > 0 else 0
    
    def avg_latency(self) -> float:
        return self.total_latency / self.total if self.total > 0 else 0
    
    def to_dict(self) -> dict:
        return {
            "total": self.total,
            "accuracy": round(self.accuracy(), 4),
            "precision": round(self.precision(), 4),
            "recall": round(self.recall(), 4),
            "f1": round(self.f1(), 4),
            "parse_rate": round(self.parse_rate(), 4),
            "hallucination_rate": round(self.hallucination_rate(), 4),
            "risk_id_accuracy": round(self.risk_id_accuracy(), 4),
            "avg_latency_sec": round(self.avg_latency(), 3),
            "reason_quality": round(self.correct_reason / self.total if self.total > 0 else 0, 4),
        }


def verify_evidence(evidence: str, clause: str) -> bool:
    """éªŒè¯è¯æ®æ˜¯å¦å­˜åœ¨äºåŸæ–‡ä¸­"""
    if not evidence or evidence in ["æ— ", "None", ""]:
        return True  # æ— è¯æ®ä¸ç®—å¹»è§‰
    
    # æ¸…ç†è¯æ®æ–‡æœ¬
    clean_evidence = evidence.replace("ã€Œ", "").replace("ã€", "").strip()
    if len(clean_evidence) < 5:
        return True
    
    # æ£€æŸ¥è¯æ®æ˜¯å¦åœ¨åŸæ–‡ä¸­
    return clean_evidence in clause


# ============================================================================
# è¯„æµ‹æ‰§è¡Œå™¨
# ============================================================================

class AblationBenchmark:
    """æ¶ˆèå®éªŒè¯„æµ‹å™¨"""
    
    def __init__(self, mode: int, source: str = "local"):
        self.mode = mode
        self.config = load_config()
        
        # åˆå§‹åŒ– LLM (ä½¿ç”¨ OllamaClient æ›¿ä»£ ChatOllama)
        if source == "local":
            llm_cfg = self.config.get("llm_config", {})
        else:
            llm_cfg = self.config.get("llm_cloud_config", {})
        
        self.llm = OllamaClient(
            base_url=llm_cfg.get("base_url", "http://localhost:11434"),
            model=llm_cfg.get("model_name", "qwen3:4b-instruct"),
            temperature=0,
        )
        
        # æ¨¡å¼3å’Œ4éœ€è¦è§„åˆ™å¼•æ“
        self.rule_engine = None
        if mode in [EvalMode.CURRENT_WORKFLOW, EvalMode.OPTIMIZED_WORKFLOW]:
            from src.core.rule_engine import RuleEngine
            self.rule_engine = RuleEngine()
        
        # Judge LLM (ç”¨äºè¯„ä¼°è®ºè¯è´¨é‡)
        self.judge_llm = self.llm
    
    def get_prompt(self, clause: str, reference_info: str = "") -> str:
        """æ ¹æ®æ¨¡å¼è·å–Prompt"""
        if self.mode == EvalMode.RAW_LLM:
            return RAW_LLM_PROMPT.format(clause=clause)
        elif self.mode == EvalMode.BASIC_PROMPT:
            return BASIC_PROMPT.format(clause=clause)
        elif self.mode == EvalMode.CURRENT_WORKFLOW:
            return CURRENT_WORKFLOW_PROMPT.format(clause=clause, reference_info=reference_info or "æ— ")
        elif self.mode == EvalMode.OPTIMIZED_WORKFLOW:
            return OPTIMIZED_WORKFLOW_PROMPT.format(clause=clause, reference_info=reference_info or "æ— ")
        else:
            return RAW_LLM_PROMPT.format(clause=clause)
    
    def get_reference_info(self, clause: str) -> str:
        """è·å–è§„åˆ™å¼•æ“çš„å‚è€ƒä¿¡æ¯"""
        if self.rule_engine is None:
            return ""
        
        try:
            matched_rule, confidence, match_source = self.rule_engine.match_risk(clause)
            if matched_rule:
                return f"**{matched_rule.get('risk_name', '')}**\n{matched_rule.get('analysis_logic', '')}\n"
            elif match_source.startswith("keyword_fallback:"):
                keyword = match_source.split(":", 1)[1]
                return f"ã€å…³é”®è¯é¢„è­¦ã€‘æ£€æµ‹åˆ°é«˜å±å…³é”®è¯ï¼š\"{keyword}\"ï¼Œå»ºè®®è°¨æ…åˆ¤æ–­ã€‚\n"
        except Exception as e:
            print(f"Rule engine error: {e}")
        
        return ""
    
    async def analyze_clause(self, clause: str) -> ParsedResult:
        """åˆ†æå•ä¸ªæ¡æ¬¾"""
        # è·å–å‚è€ƒä¿¡æ¯ï¼ˆæ¨¡å¼3å’Œ4ï¼‰
        reference_info = self.get_reference_info(clause)
        
        # æ„å»ºPrompt
        prompt = self.get_prompt(clause, reference_info)
        
        try:
            # ä½¿ç”¨ OllamaClient çš„ achat æ–¹æ³•
            content = await self.llm.achat(prompt)
            
            # è§£æè¾“å‡º
            result = parse_markdown_output(content)
            return result
            
        except Exception as e:
            print(f"LLM error: {e}")
            return ParsedResult()
    
    async def evaluate_reason(self, clause: str, gt_keywords: List[str], ai_reason: str) -> int:
        """ä½¿ç”¨LLM-as-a-Judgeè¯„ä¼°è®ºè¯è´¨é‡"""
        if not ai_reason or not gt_keywords:
            return 0
        
        judge_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå…¬æ­£çš„æ³•å¾‹è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼° AI ç”Ÿæˆçš„é£é™©åˆ†æç†ç”±æ˜¯å¦å‡†ç¡®ã€‚

### è¯„ä¼°è¾“å…¥
- **æ¡æ¬¾åŸæ–‡**: {clause}
- **æ ‡å‡†ç­”æ¡ˆå…³é”®è¯**: {", ".join(gt_keywords)}
- **AI ç”Ÿæˆç†ç”±**: {ai_reason}

### è¯„åˆ†æ ‡å‡†
- **1åˆ† (å‡†ç¡®)**: AI çš„ç†ç”±åŒ…å«äº†æ ‡å‡†ç­”æ¡ˆä¸­çš„æ ¸å¿ƒå…³é”®è¯æˆ–é€»è¾‘ã€‚
- **0åˆ† (é”™è¯¯)**: AI çš„ç†ç”±å®Œå…¨åç¦»ï¼Œæˆ–æœªè¯†åˆ«å‡ºæ ¸å¿ƒé£é™©ã€‚

### è¾“å‡ºæ ¼å¼
ä»…è¾“å‡ºä¸€ä¸ªæ•°å­—ï¼š1 æˆ– 0
"""
        
        try:
            content = await self.judge_llm.achat(judge_prompt)
            return 1 if "1" in content.strip() else 0
        except Exception as e:
            print(f"Judge error: {e}")
            return 0
    
    async def evaluate_single(self, item: Dict[str, Any], metrics: EvalMetrics) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªæ ·æœ¬"""
        import time
        
        text = item.get("text", "")
        gt = item.get("ground_truth", {})
        original_data = item.get("original_data", {})  # LLM æ•°æ®é›†çš„åŸå§‹æ•°æ®
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # åˆ†ææ¡æ¬¾
        result = await self.analyze_clause(text)
        
        # è®°å½•å“åº”æ—¶é—´
        latency = time.time() - start_time
        metrics.total_latency += latency
        
        metrics.total += 1
        
        # è§£ææˆåŠŸç‡
        if result.parse_success:
            metrics.parse_success += 1
        
        # é£é™©ç­‰çº§è¯„ä¼°
        gt_risk = gt.get("risk_level", "")
        if gt_risk == "ä¸­":
            gt_risk = "ä½"  # å½“å‰ç³»ç»Ÿä¸æ”¯æŒä¸­é£é™©
        
        pred_risk = result.risk_level
        is_risk_correct = (gt_risk == pred_risk) or (gt_risk in pred_risk)
        
        if is_risk_correct:
            metrics.correct_risk += 1
        
        # æ··æ·†çŸ©é˜µæ›´æ–°
        if pred_risk == "é«˜" and gt_risk == "é«˜":
            metrics.true_positive += 1
        elif pred_risk == "é«˜" and gt_risk != "é«˜":
            metrics.false_positive += 1
        elif pred_risk != "é«˜" and gt_risk == "é«˜":
            metrics.false_negative += 1
        else:
            metrics.true_negative += 1
        
        # risk_id åŒ¹é…è¯„ä¼°ï¼ˆé’ˆå¯¹ LLM ç”Ÿæˆçš„æ•°æ®é›†ï¼‰
        expected_risks = original_data.get("expected_risks", [])
        if expected_risks:
            # æœ‰é¢„æœŸé£é™©ï¼Œæ£€æŸ¥æ˜¯å¦æ­£ç¡®è¯†åˆ«
            metrics.risk_id_total += 1
            # å¦‚æœé¢„æµ‹ä¸ºé«˜é£é™©ä¸”æ ·æœ¬ç¡®å®åŒ…å«é£é™©ï¼Œç®—åŒ¹é…æˆåŠŸ
            if pred_risk == "é«˜" and gt_risk == "é«˜":
                metrics.risk_id_match += 1
        
        # è¯æ®éªŒè¯ï¼ˆå¹»è§‰æ£€æµ‹ï¼‰
        if verify_evidence(result.evidence, text):
            metrics.evidence_valid += 1
        else:
            metrics.evidence_invalid += 1
        
        # è®ºè¯è´¨é‡è¯„ä¼°ï¼ˆå¦‚æœæœ‰ reason_keywordsï¼‰
        reason_keywords = gt.get("reason_keywords", [])
        if reason_keywords:
            reason_score = await self.evaluate_reason(text, reason_keywords, result.analysis)
            if reason_score:
                metrics.correct_reason += 1
        
        return {
            "id": item.get("id"),
            "prediction": {
                "risk_level": result.risk_level,
                "evidence": result.evidence,
                "analysis": result.analysis,
                "parse_success": result.parse_success,
                "latency": round(latency, 3),
            },
            "ground_truth": gt,
            "correct_risk": is_risk_correct,
            "evidence_valid": verify_evidence(result.evidence, text),
        }


def convert_llm_dataset_item(item: Dict[str, Any]) -> Dict[str, Any]:
    """
    å°† LLM ç”Ÿæˆçš„æ•°æ®é›†æ ¼å¼è½¬æ¢ä¸º benchmark æœŸæœ›çš„æ ¼å¼
    
    LLM æ ¼å¼:
    {
        "id": "GENERAL_001_pos_1",
        "contract_text": "...",
        "expected_risks": [{"risk_id": "...", "risk_name": "...", ...}],
        "case_type": "positive/negative/boundary"
    }
    
    Benchmark æ ¼å¼:
    {
        "id": "...",
        "text": "...",
        "ground_truth": {"risk_level": "é«˜/ä½", "reason_keywords": [...]},
        "original_data": {...}  # ä¿ç•™åŸå§‹æ•°æ®ç”¨äº risk_id åŒ¹é…
    }
    """
    # æ£€æµ‹æ˜¯å¦ä¸ºæ–°æ ¼å¼
    if "contract_text" in item:
        expected_risks = item.get("expected_risks", [])
        case_type = item.get("case_type", "")
        
        # ç¡®å®šé£é™©ç­‰çº§ï¼špositive ä¸”æœ‰ expected_risks ä¸ºé«˜é£é™©
        if case_type == "positive" and expected_risks:
            risk_level = "é«˜"
        else:
            risk_level = "ä½"
        
        # æå–å…³é”®è¯ä½œä¸º reason_keywords
        reason_keywords = []
        for risk in expected_risks:
            if risk.get("risk_name"):
                reason_keywords.append(risk["risk_name"])
        
        return {
            "id": item.get("id", ""),
            "text": item.get("contract_text", ""),
            "ground_truth": {
                "risk_level": risk_level,
                "reason_keywords": reason_keywords,
            },
            "original_data": item,  # ä¿ç•™åŸå§‹æ•°æ®ç”¨äº risk_id åŒ¹é…
        }
    
    # å·²ç»æ˜¯æ—§æ ¼å¼ï¼Œç›´æ¥è¿”å›
    return item


async def run_ablation_benchmark(
    data_path: str = None,
    mode: int = 2,
    limit: int = None,
    source: str = "local",
    log_callback=None,
    dataset: List[Dict[str, Any]] = None  # æ”¯æŒä¼ å…¥é¢„åŠ è½½çš„æ•°æ®é›†
) -> Dict[str, Any]:
    """
    è¿è¡Œæ¶ˆèå®éªŒè¯„æµ‹
    
    Args:
        data_path: æ•°æ®é›†è·¯å¾„ (å½“ dataset ä¸º None æ—¶å¿…é¡»æä¾›)
        mode: è¯„æµ‹æ¨¡å¼ (1-4)
        limit: æ ·æœ¬æ•°é‡é™åˆ¶ (ä»…å½“ dataset ä¸º None æ—¶ç”Ÿæ•ˆ)
        source: LLM æ¥æº (local/cloud)
        log_callback: æ—¥å¿—å›è°ƒå‡½æ•°
        dataset: é¢„åŠ è½½çš„æ•°æ®é›† (ç”¨äºæ¶ˆèå®éªŒä¸­æ§åˆ¶å˜é‡)
    """
    
    def log(msg):
        print(msg)
        if log_callback:
            log_callback(msg)
    
    log(f"\n{'='*60}")
    log(f"ğŸ§ª æ¶ˆèå®éªŒè¯„æµ‹ - æ¨¡å¼ {mode}: {EvalMode.name(mode)}")
    log(f"{'='*60}")
    
    # å¦‚æœæ²¡æœ‰ä¼ å…¥æ•°æ®é›†ï¼Œåˆ™ä»æ–‡ä»¶åŠ è½½
    if dataset is None:
        if not data_path or not os.path.exists(data_path):
            log(f"Error: Data file not found at {data_path}")
            return None
        
        dataset = []
        with open(data_path, "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    raw_item = json.loads(line)
                    converted_item = convert_llm_dataset_item(raw_item)
                    dataset.append(converted_item)
        
        # ä»…åœ¨ç‹¬ç«‹è¿è¡Œæ—¶è¿›è¡Œéšæœºé‡‡æ ·
        total_samples = len(dataset)
        if limit and limit < total_samples:
            import random
            dataset = random.sample(dataset, limit)
            log(f"ğŸ“Š éšæœºé‡‡æ · {limit} æ¡ï¼ˆå…± {total_samples} æ¡å¯ç”¨ï¼‰")
    
    log(f"ğŸ“Š å®é™…è¯„æµ‹æ ·æœ¬æ•°: {len(dataset)}")
    
    # ç»Ÿè®¡æ•°æ®é›†ä¿¡æ¯
    positive_count = sum(1 for d in dataset if d.get("ground_truth", {}).get("risk_level") == "é«˜")
    negative_count = len(dataset) - positive_count
    log(f"ğŸ“Š é«˜é£é™©æ ·æœ¬: {positive_count}, ä½é£é™©æ ·æœ¬: {negative_count}")
    
    # åˆå§‹åŒ–è¯„æµ‹å™¨
    benchmark = AblationBenchmark(mode=mode, source=source)
    metrics = EvalMetrics()
    results = []
    
    # é€ä¸ªè¯„æµ‹
    for i, item in enumerate(dataset):
        log(f"è¯„æµ‹è¿›åº¦: {i+1}/{len(dataset)} - {item.get('id', 'unknown')}")
        result = await benchmark.evaluate_single(item, metrics)
        results.append(result)
    
    # è¾“å‡ºæŠ¥å‘Š
    log(f"\n{'='*60}")
    log(f"ğŸ“ˆ è¯„æµ‹ç»“æœ - æ¨¡å¼ {mode}: {EvalMode.name(mode)}")
    log(f"{'='*60}")
    
    metrics_dict = metrics.to_dict()
    for key, value in metrics_dict.items():
        if isinstance(value, float):
            # latency ä½¿ç”¨ç§’æ•°æ ¼å¼ï¼Œå…¶ä»–ä½¿ç”¨ç™¾åˆ†æ¯”æ ¼å¼
            if "latency" in key:
                log(f"  {key}: {value:.3f}s")
            else:
                log(f"  {key}: {value:.2%}")
        else:
            log(f"  {key}: {value}")
    
    return {
        "mode": mode,
        "mode_name": EvalMode.name(mode),
        "metrics": metrics_dict,
        "results": results,
    }


async def run_full_ablation_study(
    data_path: str,
    modes: List[int] = None,
    limit: int = None,
    source: str = "local"
) -> Dict[str, Any]:
    """è¿è¡Œå®Œæ•´æ¶ˆèå®éªŒï¼ˆæ‰€æœ‰æ¨¡å¼å¯¹æ¯”ï¼Œä½¿ç”¨ç›¸åŒæ ·æœ¬æ§åˆ¶å˜é‡ï¼‰"""
    
    if modes is None:
        modes = [1, 2, 3, 4]  # é»˜è®¤è¿è¡Œæ‰€æœ‰æ¨¡å¼
    
    print("\n" + "="*70)
    print("ğŸ”¬ æ¶ˆèå®éªŒ (Ablation Study) - å¤šæ¨¡å¼å¯¹æ¯”è¯„æµ‹")
    print("="*70)
    
    # ========== ç»Ÿä¸€åŠ è½½å’Œé‡‡æ ·æ•°æ®ï¼ˆæ§åˆ¶å˜é‡ï¼‰ ==========
    if not os.path.exists(data_path):
        print(f"Error: Data file not found at {data_path}")
        return {}
    
    dataset = []
    boundary_count = 0
    with open(data_path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                raw_item = json.loads(line)
                # è¿‡æ»¤æ‰ boundary ç±»å‹çš„æµ‹è¯•ç”¨ä¾‹ï¼ˆæ²¡æœ‰æ˜ç¡®çš„é¢„æœŸç»“æœï¼‰
                case_type = raw_item.get("case_type", "")
                if case_type == "boundary":
                    boundary_count += 1
                    continue
                converted_item = convert_llm_dataset_item(raw_item)
                dataset.append(converted_item)
    
    if boundary_count > 0:
        print(f"ğŸ“Š å·²è¿‡æ»¤ {boundary_count} æ¡ boundary ç”¨ä¾‹ï¼ˆæ— æ˜ç¡®é¢„æœŸç»“æœï¼‰")
    
    total_samples = len(dataset)
    if limit and limit < total_samples:
        import random
        dataset = random.sample(dataset, limit)
        print(f"ğŸ“Š ç»Ÿä¸€éšæœºé‡‡æ · {limit} æ¡ï¼ˆå…± {total_samples} æ¡å¯ç”¨ï¼‰")
    
    print(f"ğŸ“Š æ‰€æœ‰æ¨¡å¼å°†ä½¿ç”¨ç›¸åŒçš„ {len(dataset)} æ¡æ ·æœ¬")
    
    # æ‰“å°æ ·æœ¬ ID ä»¥ä¾¿éªŒè¯
    sample_ids = [item.get("id", "unknown") for item in dataset[:5]]
    print(f"ğŸ“Š æ ·æœ¬ ID é¢„è§ˆ: {sample_ids}{'...' if len(dataset) > 5 else ''}")
    
    all_results = {}
    
    # ========== å¯¹æ¯ä¸ªæ¨¡å¼è¿è¡Œè¯„æµ‹ï¼ˆä¼ å…¥ç›¸åŒæ•°æ®é›†ï¼‰ ==========
    for mode in modes:
        result = await run_ablation_benchmark(
            mode=mode,
            source=source,
            dataset=dataset  # ä¼ å…¥é¢„åŠ è½½çš„æ•°æ®é›†
        )
        if result:
            all_results[f"mode_{mode}"] = result
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print("\n" + "="*70)
    print("ğŸ“Š æ¨¡å¼å¯¹æ¯”æ±‡æ€»")
    print("="*70)
    
    # è¡¨å¤´
    print(f"{'æŒ‡æ ‡':<20}", end="")
    for mode in modes:
        print(f"{EvalMode.name(mode):<18}", end="")
    print()
    print("-" * (20 + 18 * len(modes)))
    
    # æŒ‡æ ‡å¯¹æ¯” (ç§»é™¤ç»´åº¦ç›¸å…³æŒ‡æ ‡)
    metric_keys = ["accuracy", "f1", "precision", "recall", "parse_rate", "hallucination_rate", "risk_id_accuracy", "avg_latency_sec"]
    for key in metric_keys:
        print(f"{key:<20}", end="")
        for mode in modes:
            mode_key = f"mode_{mode}"
            if mode_key in all_results:
                value = all_results[mode_key]["metrics"].get(key, 0)
                # latency ä½¿ç”¨ç§’æ•°æ ¼å¼
                if "latency" in key:
                    print(f"{value:.3f}s".ljust(18), end="")
                else:
                    print(f"{value:.2%}".ljust(18), end="")
            else:
                print(f"{'N/A':<18}", end="")
        print()
    
    # ä¿å­˜ç»“æœ (ä½¿ç”¨è„šæœ¬ç›®å½•çš„ç»å¯¹è·¯å¾„)
    script_dir = Path(__file__).parent
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = script_dir / f"ablation_results_{timestamp}.json"
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {output_path}")
    
    # ========== ç”Ÿæˆå›¾è¡¨ ==========
    try:
        from evaluation.chart_generator import generate_ablation_charts, generate_combined_chart
        
        print("\nğŸ“Š æ­£åœ¨ç”Ÿæˆè¯„æµ‹å›¾è¡¨...")
        chart_paths = generate_ablation_charts(all_results, timestamp=timestamp)
        combined_path = generate_combined_chart(all_results, timestamp=timestamp)
        
        # å°†å›¾è¡¨è·¯å¾„æ·»åŠ åˆ°ç»“æœä¸­
        all_results["chart_paths"] = chart_paths
        all_results["combined_chart"] = combined_path
        all_results["timestamp"] = timestamp
        
        # æ›´æ–°ä¿å­˜çš„ JSONï¼ˆåŒ…å«å›¾è¡¨è·¯å¾„ï¼‰
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š å·²ç”Ÿæˆ {len(chart_paths)} å¼ å›¾è¡¨ + 1 å¼ ç»¼åˆå›¾")
    except ImportError as e:
        print(f"âš ï¸ å›¾è¡¨ç”Ÿæˆå¤±è´¥ï¼ˆè¯·ç¡®ä¿å®‰è£… matplotlibï¼‰: {e}")
    except Exception as e:
        print(f"âš ï¸ å›¾è¡¨ç”Ÿæˆå¼‚å¸¸: {e}")
    
    return all_results


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="æ¶ˆèå®éªŒè¯„æµ‹è„šæœ¬")
    parser.add_argument("--data", type=str, default="evaluation/llm_benchmark_dataset.jsonl",
                        help="è¯„æµ‹æ•°æ®é›†è·¯å¾„ (æ”¯æŒæ–°æ—§ä¸¤ç§æ ¼å¼)")
    parser.add_argument("--mode", type=int, choices=[1, 2, 3, 4], default=None,
                        help="è¯„æµ‹æ¨¡å¼ (1-4)ï¼Œä¸æŒ‡å®šåˆ™è¿è¡Œæ‰€æœ‰æ¨¡å¼")
    parser.add_argument("--limit", type=int, default=None,
                        help="é™åˆ¶è¯„æµ‹æ ·æœ¬æ•°é‡")
    parser.add_argument("--source", type=str, choices=["local", "cloud"], default="local",
                        help="LLMæ¥æº (local/cloud)")
    
    args = parser.parse_args()
    
    if args.mode:
        # è¿è¡Œå•ä¸ªæ¨¡å¼
        asyncio.run(run_ablation_benchmark(
            data_path=args.data,
            mode=args.mode,
            limit=args.limit,
            source=args.source
        ))
    else:
        # è¿è¡Œå®Œæ•´æ¶ˆèå®éªŒ
        asyncio.run(run_full_ablation_study(
            data_path=args.data,
            limit=args.limit,
            source=args.source
        ))
