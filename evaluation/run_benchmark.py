import argparse
import asyncio
import json
import os
import sys
from pathlib import Path
from typing import List, Dict, Any
from langchain_core.messages import HumanMessage, SystemMessage

# Add project root to sys.path
sys.path.append(str(Path(__file__).resolve().parents[1]))

from src.core.engine import ContractAnalyzer
from src.core.llm import LLMClient
from src.utils.parser import chunk_contract

JUDGE_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªå…¬æ­£çš„æ³•å¾‹è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼° AI ç”Ÿæˆçš„é£é™©åˆ†æç†ç”±æ˜¯å¦å‡†ç¡®ã€‚

### è¯„ä¼°è¾“å…¥
- **æ¡æ¬¾åŸæ–‡**: {clause}
- **æ ‡å‡†ç­”æ¡ˆ (Ground Truth)**: {ground_truth_keywords}
- **AI ç”Ÿæˆç†ç”±**: {ai_reason}

### è¯„åˆ†æ ‡å‡†
- **1åˆ† (å‡†ç¡®)**: AI çš„ç†ç”±åŒ…å«äº†æ ‡å‡†ç­”æ¡ˆä¸­çš„æ ¸å¿ƒå…³é”®è¯æˆ–é€»è¾‘ã€‚
- **0åˆ† (é”™è¯¯)**: AI çš„ç†ç”±å®Œå…¨åç¦»ï¼Œæˆ–æœªè¯†åˆ«å‡ºæ ¸å¿ƒé£é™©ã€‚

### è¾“å‡ºæ ¼å¼
ä»…è¾“å‡ºä¸€ä¸ªæ•°å­—ï¼š1 æˆ– 0
"""

async def evaluate_reasoning(llm, clause, ground_truth_keywords, ai_reason):
    if not ai_reason or not ground_truth_keywords:
        return 0
    
    prompt = JUDGE_PROMPT.format(
        clause=clause,
        ground_truth_keywords=", ".join(ground_truth_keywords),
        ai_reason=ai_reason
    )
    
    try:
        response = await llm.ainvoke([HumanMessage(content=prompt)])
        content = getattr(response, "content", str(response)).strip()
        if "1" in content:
            return 1
        return 0
    except Exception as e:
        print(f"Judge error: {e}")
        return 0

async def evaluate_single_contract(analyzer, judge_llm, item: Dict[str, Any]) -> Dict[str, Any]:
    text = item.get("text", "")
    ground_truth = item.get("ground_truth", {})
    
    # Run Agent
    # We need to simulate the analyzer's process but just for one chunk/clause
    # Since ContractAnalyzer.analyze is designed for full documents and updates a tracker,
    # we might want to use the LLMClient directly for single clause evaluation to match the old graph behavior,
    # OR we can use the analyzer but capture the results differently.
    
    # The old graph.ainvoke(state) returned "scan_results".
    # The new analyzer.analyze returns nothing but updates the tracker.
    # However, for evaluation, we usually want to test the *scanning* capability primarily.
    
    # Let's use LLMClient directly to scan the text, similar to how engine.py does it.
    # This avoids the overhead of the full analyzer workflow (tracker, deduplication, etc.) which might be overkill for single-clause eval.
    
    llm_client = LLMClient(source="local") # Use local for eval by default
    
    try:
        # Analyze the text directly using analyze_clause
        # Note: analyze_clause returns a single ClauseAnalysis object or None
        from src.core.rule_engine import RuleEngine
        rule_engine = RuleEngine()
        
        # Get reference info from rule engine (simplified for eval)
        matched_rule, confidence, match_source = rule_engine.match_risk(text)
        reference_info = ""
        if matched_rule:
            reference_info = f"**{matched_rule.get('risk_name', '')}**\n{matched_rule.get('analysis_logic', '')}\n"
        elif match_source.startswith("keyword_fallback:"):
            keyword = match_source.split(":", 1)[1]
            reference_info = f"ã€å…³é”®è¯é¢„è­¦ã€‘æ£€æµ‹åˆ°é«˜å±å…³é”®è¯ï¼š\"{keyword}\"ï¼Œå»ºè®®è°¨æ…åˆ¤æ–­ã€‚\n"
        
        result = await asyncio.to_thread(llm_client.analyze_clause, text, reference_info)
        
        # Convert to list format to match old format
        scan_results = []
        if result:
            scan_results.append({
                "clause": result.clause_text,
                "risk": result.risk_level,
                "dimension": result.dimension,
                "reason": result.risk_reason,
            })
            
    except Exception as e:
        print(f"Agent error: {e}")
        scan_results = []
        
    # Evaluate
    # Assuming one clause per text in golden dataset
    if not scan_results:
        return {
            "id": item.get("id"),
            "correct_risk": False,
            "correct_dimension": False,
            "correct_reason": False,
            "prediction": {},
            "ground_truth": ground_truth
        }
        
    pred = scan_results[0] # Take the first one
    
    # 1. Risk Level Match
    # å½“å‰ç³»ç»Ÿä½¿ç”¨"é«˜/ä½"äºŒå…ƒåˆ†ç±»ï¼ŒGround Truth ä¸­çš„"ä¸­"è§†ä¸º"ä½"
    pred_risk = pred.get("risk", "æœªçŸ¥")
    gt_risk = ground_truth.get("risk_level", "")
    
    # å°† Ground Truth ä¸­çš„"ä¸­"æ˜ å°„ä¸º"ä½"ï¼ˆå½“å‰ç³»ç»Ÿä¸æ”¯æŒä¸­é£é™©ï¼‰
    if gt_risk == "ä¸­":
        gt_risk = "ä½"
    
    # ç›´æ¥æ¯”è¾ƒé£é™©ç­‰çº§
    is_risk_correct = gt_risk in pred_risk or pred_risk == gt_risk
    
    # 2. Dimension Match
    pred_dim = str(pred.get("dimension", "0"))
    gt_dim = str(ground_truth.get("dimension_id", "0"))
    is_dim_correct = pred_dim == gt_dim
    
    # 3. Reasoning Match (LLM Judge)
    is_reason_correct = await evaluate_reasoning(
        judge_llm, 
        text, 
        ground_truth.get("reason_keywords", []), 
        pred.get("reason", "")
    )
    
    return {
        "id": item.get("id"),
        "correct_risk": is_risk_correct,
        "correct_dimension": is_dim_correct,
        "correct_reason": bool(is_reason_correct),
        "prediction": pred,
        "ground_truth": ground_truth
    }

async def run_benchmark(data_path: str, limit: int = None, log_callback=None):
    def log(msg):
        print(msg)
        if log_callback:
            log_callback(msg)

    log(f"Loading data from {data_path}...")
    
    if not os.path.exists(data_path):
        log(f"Error: Data file not found at {data_path}")
        return None

    # Load Data
    dataset = []
    with open(data_path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                dataset.append(json.loads(line))
                
    if limit:
        dataset = dataset[:limit]
        
    log(f"Loaded {len(dataset)} samples.")
    
    # Init Judge
    # We can use LLMClient's text_llm for judging
    client = LLMClient(source="local")
    judge_llm = client.text_llm
    
    # Analyzer is not strictly needed as an object if we use LLMClient directly in evaluate_single_contract
    # but we can pass None or keep the signature
    analyzer = None 
    
    results = []
    
    for i, item in enumerate(dataset):
        log(f"Evaluating sample {i+1}/{len(dataset)}: {item.get('id')}...")
        res = await evaluate_single_contract(analyzer, judge_llm, item)
        results.append(res)
        
    # Statistics
    total = len(results)
    risk_acc = sum(1 for r in results if r["correct_risk"]) / total if total else 0
    dim_acc = sum(1 for r in results if r["correct_dimension"]) / total if total else 0
    reason_acc = sum(1 for r in results if r["correct_reason"]) / total if total else 0
    
    log("\n" + "="*30)
    log("ğŸ“Š Evaluation Report")
    log("="*30)
    log(f"Total Samples: {total}")
    log(f"Risk Level Accuracy: {risk_acc:.2%}")
    log(f"Dimension ID Accuracy: {dim_acc:.2%}")
    log(f"Reasoning Quality Score: {reason_acc:.2%}")
    
    # Save Results with timestamp
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    os.makedirs("evaluation", exist_ok=True)
    result_file = f"evaluation/results_{timestamp}.json"
    with open(result_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    log(f"Results saved to {result_file}")
        
    # Export Bad Cases
    bad_cases = [r for r in results if not (r["correct_risk"] and r["correct_dimension"] and r["correct_reason"])]
    
    with open("evaluation/bad_cases.md", "w", encoding="utf-8") as f:
        f.write("# ğŸš¨ Bad Cases Analysis\n\n")
        for case in bad_cases:
            f.write(f"### Case ID: {case['id']}\n")
            f.write(f"- **Clause (Pred)**: {case['prediction'].get('clause', 'N/A')}\n")
            f.write(f"- **Ground Truth**: Risk={case['ground_truth'].get('risk_level')}, Dim={case['ground_truth'].get('dimension_id')}, Keywords={case['ground_truth'].get('reason_keywords')}\n")
            f.write(f"- **Prediction**: Risk={case['prediction'].get('risk')}, Dim={case['prediction'].get('dimension')}\n")
            f.write(f"- **Reasoning**: {case['prediction'].get('reason')}\n")
            f.write(f"- **Errors**: ")
            errors = []
            if not case["correct_risk"]: errors.append("Risk Mismatch")
            if not case["correct_dimension"]: errors.append("Dimension Mismatch")
            if not case["correct_reason"]: errors.append("Reasoning Poor")
            f.write(", ".join(errors) + "\n\n")
            f.write("---\n")
            
    log(f"\nBad cases exported to evaluation/bad_cases.md ({len(bad_cases)} cases)")
    
    return {
        "metrics": {
            "total": total,
            "risk_acc": risk_acc,
            "dim_acc": dim_acc,
            "reason_acc": reason_acc
        },
        "results": results,
        "bad_cases": bad_cases
    }

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--data", type=str, default="evaluation/golden_dataset.jsonl")
    parser.add_argument("--limit", type=int, default=None)
    args = parser.parse_args()
    
    asyncio.run(run_benchmark(args.data, args.limit))
