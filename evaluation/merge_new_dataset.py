"""
æ•°æ®åˆå¹¶è„šæœ¬ï¼šå°† GENERAL/LABOR/LEASE/SALES.json åˆå¹¶ä¸º llm_benchmark_dataset.json

åŠŸèƒ½ï¼š
1. åˆå¹¶å››ä¸ª JSON æ–‡ä»¶
2. æ·»åŠ  source_domain å­—æ®µ
3. ID å»é‡æ£€æŸ¥
4. æ•°æ®è´¨é‡éªŒè¯
"""

import json
from pathlib import Path
from typing import List, Dict, Any


def load_json(path: Path) -> List[Dict[str, Any]]:
    """åŠ è½½ JSON æ–‡ä»¶"""
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)


def validate_item(item: Dict[str, Any], idx: int, source: str) -> List[str]:
    """éªŒè¯å•ä¸ªæ ·æœ¬çš„æ•°æ®è´¨é‡
    
    Returns:
        List[str]: é”™è¯¯ä¿¡æ¯åˆ—è¡¨ï¼Œç©ºè¡¨ç¤ºæ— é”™è¯¯
    """
    errors = []
    
    # å¿…å¡«å­—æ®µæ£€æŸ¥
    required_fields = ['id', 'contract_text', 'case_type']
    for field in required_fields:
        if not item.get(field):
            errors.append(f"[{source}] æ ·æœ¬ {idx}: ç¼ºå°‘å¿…å¡«å­—æ®µ '{field}'")
    
    # case_type å€¼æ£€æŸ¥
    valid_case_types = {'high_positive', 'medium_positive', 'negative'}
    case_type = item.get('case_type', '')
    if case_type and case_type not in valid_case_types:
        errors.append(f"[{source}] æ ·æœ¬ {idx}: case_type å€¼æ— æ•ˆ '{case_type}'ï¼Œåº”ä¸º {valid_case_types}")
    
    # expected_risks ç»“æ„æ£€æŸ¥ï¼ˆå¯¹äºæ­£ä¾‹ï¼‰
    if case_type in {'high_positive', 'medium_positive'}:
        expected_risks = item.get('expected_risks', [])
        if not expected_risks:
            errors.append(f"[{source}] æ ·æœ¬ {idx}: æ­£ä¾‹æ ·æœ¬ç¼ºå°‘ expected_risks")
        else:
            for i, risk in enumerate(expected_risks):
                if not isinstance(risk, dict):
                    errors.append(f"[{source}] æ ·æœ¬ {idx}: expected_risks[{i}] åº”ä¸ºå­—å…¸")
                elif not risk.get('risk_id'):
                    errors.append(f"[{source}] æ ·æœ¬ {idx}: expected_risks[{i}] ç¼ºå°‘ risk_id")
    
    return errors


def merge_datasets(root_dir: Path, output_path: Path) -> Dict[str, Any]:
    """åˆå¹¶æ•°æ®é›†
    
    Returns:
        Dict with 'success', 'total', 'errors', 'duplicates'
    """
    source_files = ['GENERAL.json', 'LABOR.json', 'LEASE.json', 'SALES.json']
    domain_map = {
        'GENERAL.json': 'GENERAL',
        'LABOR.json': 'LABOR',
        'LEASE.json': 'LEASE',
        'SALES.json': 'SALES',
    }
    
    merged = []
    seen_ids = set()
    duplicates = []
    all_errors = []
    
    for filename in source_files:
        filepath = root_dir / filename
        if not filepath.exists():
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            continue
        
        print(f"ğŸ“‚ åŠ è½½ {filename}...")
        items = load_json(filepath)
        domain = domain_map[filename]
        
        for idx, item in enumerate(items):
            # æ·»åŠ  source_domain
            item['source_domain'] = domain
            
            # æ•°æ®è´¨é‡éªŒè¯
            errors = validate_item(item, idx, filename)
            all_errors.extend(errors)
            
            # ID å»é‡æ£€æŸ¥
            item_id = item.get('id', '')
            if item_id in seen_ids:
                duplicates.append(item_id)
                print(f"âš ï¸ é‡å¤ ID: {item_id}")
            else:
                seen_ids.add(item_id)
                merged.append(item)
        
        print(f"   âœ… åŠ è½½ {len(items)} æ¡æ ·æœ¬")
    
    # è¾“å‡ºç»Ÿè®¡
    print(f"\nğŸ“Š åˆå¹¶ç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(merged)}")
    print(f"   é‡å¤ ID: {len(duplicates)}")
    print(f"   æ•°æ®è´¨é‡é”™è¯¯: {len(all_errors)}")
    
    if all_errors:
        print("\nâš ï¸ æ•°æ®è´¨é‡é—®é¢˜:")
        for err in all_errors[:10]:  # åªæ˜¾ç¤ºå‰ 10 æ¡
            print(f"   {err}")
        if len(all_errors) > 10:
            print(f"   ... è¿˜æœ‰ {len(all_errors) - 10} æ¡é”™è¯¯")
    
    # å†™å…¥è¾“å‡ºæ–‡ä»¶
    if not all_errors:  # ä»…æ— é”™è¯¯æ—¶å†™å…¥
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(merged, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… å·²ä¿å­˜åˆ°: {output_path}")
    else:
        print(f"\nâŒ å­˜åœ¨æ•°æ®è´¨é‡é—®é¢˜ï¼Œæœªä¿å­˜æ–‡ä»¶")
    
    return {
        'success': len(all_errors) == 0,
        'total': len(merged),
        'errors': all_errors,
        'duplicates': duplicates,
    }


if __name__ == '__main__':
    # é¡¹ç›®æ ¹ç›®å½•
    root_dir = Path(__file__).resolve().parents[1]
    output_path = Path(__file__).resolve().parent / 'llm_benchmark_dataset.json'
    
    print("=" * 60)
    print("ğŸ“¦ æ¶ˆèå®éªŒæ•°æ®é›†åˆå¹¶è„šæœ¬")
    print("=" * 60)
    
    result = merge_datasets(root_dir, output_path)
    
    if result['success']:
        print(f"\nğŸ‰ åˆå¹¶æˆåŠŸï¼å…± {result['total']} æ¡æ ·æœ¬")
    else:
        print(f"\nğŸ’¥ åˆå¹¶å¤±è´¥ï¼Œè¯·ä¿®å¤æ•°æ®è´¨é‡é—®é¢˜")
